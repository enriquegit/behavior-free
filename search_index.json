[
["index.html", "Behavior Analysis with Machine Learning and R A Sensors and Data Driven Approach Welcome", " Behavior Analysis with Machine Learning and R A Sensors and Data Driven Approach Enrique Garcia Ceja 2020-10-10 Welcome &gt; This is a complete and free html version of the book. The e-book version can be purchased at Leanpub. This book aims to provide an introduction to machine learning concepts and algorithms applied to a diverse set of behavior analysis problems. It focuses on the practical aspects of solving such problems based on data collected from sensors or stored in databases. The included examples demonstrate how to perform several of the tasks involved during a data analysis pipeline such as: data exploration, visualization, preprocessing, representation, model training/validation, and so on. All of this, using the R programming language and real-life datasets. The accompanying source code for all examples is available at https://github.com/enriquegit/behavior-code. The book itself was written in R with the bookdown package1 developed by Yihui Xie. This is a work in progress, and I will keep updating and adding new content. Some of the content that you will find here includes, How To: Build supervised machine learning models to predict indoor locations based on WiFi signals, recognize physical activities from smartphone sensors and 3D skeleton data, detect hand gestures from accelerometer signals, and so on. Use unsupervised learning algorithms to discover criminal behavioral patterns. Program your own ensemble learning methods and use multi-view stacking to fuse signals from heterogeneous data sources. Train deep learning models such as neural networks to classify muscle activity from electromyography signals and CNNs to detect smiles in images. Evaluate the performance of your models in traditional and multi-user settings. About me: My name is Enrique and I am a researcher at SINTEF. Please feel free to e-mail me with any questions/comments/feedback, etc. e-mail: twitter: e_g_mx website: http://www.enriquegc.com The html version of this book is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License https://CRAN.R-project.org/package=bookdown↩︎ "],
["preface.html", "Preface", " Preface Automatic behavior monitoring technologies are becoming part of our everyday lives thanks to advances in sensors and machine learning. The automatic analysis and understanding of behavior are being applied to solve problems in several fields, including health care, sports, marketing, ecology, security, and psychology, to name a few. This book provides a practical introduction to machine learning methods applied to behavior analysis with the R programming language. The book does not assume any previous knowledge in machine learning. You should be familiar with the basics of R and some knowledge in basic statistics and high school-level mathematics would be beneficial. Supplemental Material Supplemental material consists of the examples’ code and datasets. The source code for the examples can be downloaded from https://github.com/enriquegit/behavior-code. Instructions on how to set up the code and get the datasets are in Appendix A. A reference for all the utilized datasets is in Appendix B. Conventions DATASET names are written in uppercase italics. Functions are referred to by their name followed by parenthesis and omitting their arguments, for example: myFunction(). Class labels are written in italics and between single quotes: ‘label1’. The following icons are used to provide additional contextual information: Provides additional information and notes. Important information to consider. Provides tips and good practice recommendations. Lists the R scripts and files used in the corresponding section. The folder icon will appear at the beginning of a section (if applicable) to indicate which scripts were used for the corresponding examples. Acknowledgments I want to thank Michael Riegler, Jaime Mondragon y Ariana, Viviana M., Linda Sicilia, Anton Aguilar, Aleksander Karlsen, my former master’s and PhD. advisor Ramon F. Brena, and my colleagues at SINTEF. The examples in this book rely heavily on datasets. I want to thank all the people that made all their datasets used here publicly available. I want to thank Vance Capley who brought to life the front cover and comic illustrations. I want to thank all the bands whose music helped me to keep up: Hatebreed, Sworn Enemy, Killswitch Engage, As I Lay Dying, Lamb of God, Himsa, Slipknot, Madball, Fleshgod Apocalypse, Bleeding Through, Caliban, Heaven Shall Burn, Darkest Hour, Demon Hunter, Frente de Ira, Desarmador, ill niño, Lionheart, Neaera, Soulfly, Walls of Jericho, Arrecife, Corcholata, Amon Amarth, Fit for a King, Annisokay. "],
["intro.html", "Chapter 1 Introduction 1.1 What is Machine Learning? 1.2 Types of Machine Learning 1.3 Terminology 1.4 Data Analysis Pipeline 1.5 Evaluating Predictive Models 1.6 Simple Classification Example 1.7 Simple Regression Example 1.8 Underfitting and Overfitting 1.9 Bias and Variance 1.10 Summary", " Chapter 1 Introduction Living organisms are constantly sensing and analyzing their surrounding environment. This includes inanimate objects but also other living entities. All of this is with the objective of making decisions and taking actions, either consciously or unconsciously. If we see someone running, we will react differently depending on whether we are at a stadium or in a bank. At the same time, we may also analyze other cues such as the runner’s facial expressions, clothes, items, and the reactions of the other people around us. Based on this aggregated information, we can decide how to react and behave. All this is supported by the organisms’ sensing capabilities and decision-making processes (the brain and/or chemical reactions). Understanding our environment and how others behave is crucial for conducting our everyday life activities and provides support for other tasks. But, what is behavior? The Cambridge dictionary defines behavior as: “the way that a person, an animal, a substance, etc. behaves in a particular situation or under particular conditions”. Another definition by dictionary.com is: “observable activity in a human or animal.” Both definitions are very similar and include humans and animals. Following those definitions, this book will focus on the automatic analysis of human and animal behaviors. There are three main reasons why one may want to analyze behaviors in an automatic manner: React. A biological or an artificial agent (or a combination of both) can take actions based on what is happening in the surrounding environment. For example, if suspicious behavior is detected in an airport, preventive actions can be triggered by security systems and the corresponding authorities. Without the possibility to automate such a detection system, it would be infeasible to implement it in practice. Just imagine trying to analyze airport traffic by hand. Understand. Analyzing the behavior of an organism can help us to understand other associated behaviors and processes and to answer research questions. For example, Williams et al. (2020) found that Andean condors (the heaviest soaring bird) only flap their wings for about \\(1\\%\\) of their total flight time. In one of the cases, a condor flew \\(\\approx 172\\) km without flapping. Those findings were the result of analyzing the birds’ behavior from data recorded by bio-logging devices. In this book, several examples that make use of inertial devices will be studied. Figure 1.1: Andean condor. source: wikipedia. Document/Archive. Finally, we may want to document certain behaviors for future use. It could be for evidence purposes or maybe it is not clear how the information can be used now but may come in handy later. Based on the archived information, one could gain new knowledge in the future and use it to react (take decisions/actions). For example, we could document our nutritional habits (what do we eat, how often, etc.). If there is a health issue, a specialist could use this historical information to make a more precise diagnosis and propose actions. Some behaviors can be used as a proxy to understand other behaviors, states, and/or processes. For example, detecting body movement behaviors during a job interview could serve as the basis to understand stress levels. Behaviors can also be modeled as a composition of lower-level behaviors. In chapter 7, a method called Bag of Words that can be used to decompose complex behaviors into a set of simpler ones will be presented. In order to analyze and monitor behaviors, we need a way to observe them. Living organisms use their available senses such as eyesight, hearing, smell, echolocation (bats, dolphins), thermal senses (snakes, mosquitoes), etc. In the case of machines, they need sensors to accomplish or approximate those tasks. For example, RGB and thermal cameras, microphones, temperature sensors, and so on. The reduction in the size of sensors has allowed the development of more powerful wearable devices. Wearable devices are electronic devices that are worn by a user, usually as accessories or embedded in clothes. Examples of wearable devices are smartphones, smartwatches, fitness bracelets, actigraphy watches, etc. These devices have embedded sensors that allow them to monitor different aspects of a user such as activity levels, blood pressure, temperature, and location, to name a few. Examples of sensors that can be found in those devices are accelerometers, magnetometers, gyroscopes, heart rate, microphones, Wi-Fi, Bluetooth, Galvanic Skin Response (GSR), etc. Several of those sensors were initially used for some specific purposes. For example, accelerometers in smartphones were intended to be used for gaming or detecting the device’s orientation. Later, some people started to propose and implement new use cases such as activity recognition (Shoaib et al. 2015) and fall detection. The magnetometer, which measures the earth’s magnetic field, was mainly used with map applications to determine the orientation of the device, but later, it was found that it can also be used for indoor location purposes (Brena et al. 2017). In general, wearable devices have proven success in tracking different types of behaviors such as physical activity, sports activities, location, and even mental health states (Garcia-Ceja, Riegler, Nordgreen, et al. 2018). These sensors generate a lot of raw data, but it will be our task to process and analyze it. Doing it by hand becomes impossible given the large amounts and available formats in which data is available. Thus, in this book, several machine learning methods will be introduced to extract and analyze different types of behaviors from data. The next section begins with an introduction to machine learning. The rest of this chapter will introduce the required machine learning concepts before we start analyzing behaviors in chapter 2. 1.1 What is Machine Learning? You can think of Machine Learning as a set of computational algorithms that automatically find useful patterns and relationships from data. Here, the keyword is automatic. When trying to solve a problem, one can hard-code a predefined set of rules, for example, chained if-else conditions. For instance, if we want to detect if the object in a picture is an orange or a pear, we can do something like: if(number_green_pixels &gt; 90%) return &quot;pear&quot; else return &quot;orange&quot; This simple rule should work well and will do the job. Imagine that now your boss tells you that the system needs to recognize green apples as well. Our previous rule will no longer work, and we will need to include additional rules and thresholds. On the other hand, a machine learning algorithm will automatically learn such rules based on the updated data. So, you only need to update your data with examples of green apples and “click” the re-train button! The result of learning is knowledge that the system can use to solve new instances of a problem. In this case, when you show a new image to the system it should be able to recognize the type of fruit. Figure 1.2 shows this general idea. Figure 1.2: Overall Machine Learning phases. For more formal definitions of machine learning, I recommend you check (Kononenko and Kukar 2007). Machine learning methods rely on three main building blocks: Data Algorithms Models Every machine learning method needs data to learn from. For the example of the fruits, we need examples of images for each type of fruit we want to recognize. Additionally, we need their corresponding output types (labels) so the algorithm can learn how to associate each image with its corresponding label. Not every machine learning method needs the expected output or labels (more on this in the Taxonomy section 1.2). Typically, an algorithm will use the data to learn a model. This is called the learning or training phase. The learned model can then be used to generate predictions when presented with new data. The data used to train the models is called the train set. Since we need a way to test how the model will perform once it is deployed in a real setting (in production), we also need what is known as the test set. The test set is used to estimate the model’s performance on data it has never seen before (more on this will be presented in section 1.5). 1.2 Types of Machine Learning Machine learning methods can be grouped into different types. Figure 1.3 depicts a categorization of machine learning ‘types’. This figure is based on (Biecek et al. 2012). The \\(x\\) axis represents the certainty of the labels and the \\(y\\) axis the percent of training data that is labeled. In our previous example, the labels are the fruits’ names associated with each image. Figure 1.3: Machine learning taxonomy. From the figure, four main types of machine learning methods can be observed: Supervised learning. In this case, \\(100\\%\\) of the training data is labeled and the certainty of those labels is \\(100\\%\\). This is like the fruits example. For every image used to train the system, the respective label is also known and there is no uncertainty about the label. When the expected output is a category (the type of fruit), this is called classification. Examples of classification models (a.k.a classifiers) are decision trees, k-nearest neighbors, random forest, neural networks, etc. When the output is a real number (e.g., predict temperature) it is called regression. Examples of regression models are linear regression, regression trees, neural networks, random forest, k-nearest neighbors, etc. Note that some models can be used for both classification and regression. A supervised learning problem can be formalized as follows: \\[\\begin{equation} f\\left(x\\right) = y \\tag{1.1} \\end{equation}\\] where \\(f\\) is a function that maps some input data \\(x\\) (for example images) to an output \\(y\\) (types of fruits). Usually, an algorithm will try to learn the best model \\(f\\) given some data consisting of \\(n\\) pairs \\((x,y)\\) of examples. During learning, the algorithm has access to the expected output/label \\(y\\) for each input \\(x\\). At inference time, that is, when we want to make predictions for new examples, we can use the learned model \\(f\\) and feed it with a new input \\(x\\) to obtain the corresponding predicted value \\(y\\). Semi-supervised learning. This is the case when the certainty of the labels is \\(100\\%\\) but not all training data are labeled. Usually, this scenario considers the case when only a very small proportion of the data is labeled. That is, the dataset contains pairs of examples of the form \\((x,y)\\) but also examples where \\(y\\) is missing \\((x,?)\\). In supervised learning, both \\(x\\) and \\(y\\) must be present. On the other hand, semi-supervised algorithms can learn even if some examples are missing the expected output \\(y\\). This is a common situation in real life since labeling data can be expensive and time-consuming. In the fruits example, someone needs to tag every training image manually before training a model. Semi-supervised learning methods try to extract information also from the unlabeled data to improve the models. Examples of semi-supervised learning methods are self-learning, co-training, tri-training, etc. (Triguero, García, and Herrera 2013). Partially-supervised learning. This is a generalization that encompasses supervised and semi-supervised learning. Here, the examples have uncertain (soft) labels. For example, the label of a fruit image instead of being an “orange” or “pear” could be a vector \\([0.7, 0.3]\\) where the first element is the probability that the image corresponds to an orange and the second element is the probability that it’s a pear. Maybe the image was not very clear, and these are the beliefs of the person tagging the images encoded as probabilities. Examples of models that can be used for partially-supervised learning are mixture models with belief functions (Côme et al. 2009) and neural networks. Unsupervised learning. This is the extreme case when none of the training examples have a label. That is, the dataset only has pairs \\((x,?)\\). Now, you may be wondering: If there are no labels, is it possible to extract information from these data? The answer is yes. Imagine you have fruit images with no labels. What you could try to do is to automatically group them into meaningful categories/groups. The categories could be the types of fruits themselves, i.e., trying to form groups in which images within the same category belong to the same type. In the fruits example, we could infer the true types by visually inspecting the images, but in many cases, visual inspection is difficult and the formed groups may not have an easy interpretation, but still, they can be very useful and can be used as a preprocessing step (like in vector quantization). These types of algorithms that find groups (hierarchical groups in some cases) are called clustering methods. Examples of clustering methods are k-means, k-medoids, and hierarchical clustering. Clustering algorithms are not the only unsupervised learning methods. Association rules, word embeddings, and autoencoders are examples of other unsupervised learning methods. Some people may disagree that word embeddings and autoencoders are not fully unsupervised methods but for practical purposes, this categorization is not relevant. Additionally, there is another type of machine learning called Reinforcement Learning (RL) which has substantial differences from the previous ones. This type of learning does not rely on example data as the previous ones but on stimuli from the agent’s environment. At any given point in time, an agent can perform an action which will lead it to a new state where a reward is collected. The aim is to learn the sequence of actions that maximize the reward. This type of learning is not covered in this book. A good introduction to the topic can be consulted here2. This book will mainly cover supervised learning problems and more specifically, classification problems. For example, given a set of wearable sensor readings, we want to predict contextual information about a given user such as location, current activity, mood, and so on. Unsupervised learning methods (clustering and association rules) will be covered as well in chapter 6. 1.3 Terminology This section introduces some basic terminology that will be helpful for the rest of the book. 1.3.1 Tables Since data is the most important ingredient in machine learning, let’s start with some related terms. First, data needs to be stored/structured so it can be easily manipulated and processed. Most of the time, datasets will be stored as tables or in R terminology, data frames. Figure 1.4 shows the mtcars dataset stored in a data frame. Figure 1.4: Table/Data frame components. Columns represent variables and rows represent examples also known as instances or data points. In this table, there are \\(5\\) variables mpg, cyl, disp, hp and the model (the first column). In this example, the first column does not have a name, but it is still a variable. Each row represents a specific car model with its values per variable. In machine learning terminology, rows are more commonly called instances whereas in statistics they are often called data points or observations. Here, those terms will be used interchangeably. Figure 1.5 shows a data frame for the iris dataset which consists of different kinds of plants. Suppose that we are interested in predicting the Species based on the other variables. In machine learning terminology, the variable of interest (the one that depends on the others) is called the class or label for classification problems. For regression, it is often referred to as y. In statistics, it is more commonly known as the response, dependent, or y variable, for both classification and regression. In machine learning terminology, the rest of the variables are called features or attributes. In statistics, they are called predictors, independent variables, or just X. From the context, most of the time it should be easy to identify dependent from independent variables regardless of the used terminology. Figure 1.5: Table/Data frame components (cont.). 1.3.2 Variable Types When working with machine learning algorithms, the following are the most commonly used variable types. Here, when I talk about variable types, I do not refer to programming-language-specific data types (int, boolean, string, etc.) but to more general types regardless of the underlying implementation for each specific programming language. Categorical/Nominal: These variables take values from a discrete set of possible values (categories). For example, the categorical variable color can take the values “red”, “green”, “black”, and so on. Categorical variables do not have an ordering. Numeric: Real values such as height, weight, price, etc. Integer: Integer values such as number of rooms, age, number of floors, etc. Ordinal: Similar to categorical variables, these take their values from a set of discrete values, but they do have an ordering. For example, low &lt; medium &lt; high. 1.3.3 Predictive Models In machine learning terminology, a predictive model is a model that takes some input and produces an output. Classifiers and Regressors are predictive models. I will use the terms classifier/model and regressor/model interchangeably. 1.4 Data Analysis Pipeline Usually, the data analysis pipeline consists of several steps which are depicted in Figure 1.6. This is not a complete list but includes the most common steps. It all starts with the data collection. Then the data exploration and so on, until the results are presented. These steps can be followed in sequence, but you can always jump from one step to another one. In fact, most of the time you will end up using an iterative approach by going from one step to the other (forward or backward) as needed. Figure 1.6: Data analysis pipeline. The big gray box at the bottom means that machine learning methods can be used in all those steps and not just during training or evaluation. For example, one may use dimensionality reduction methods in the data exploration phase to plot the data or classification or regression methods in the cleaning phase to impute missing values. Now, let’s give a brief description of each of those phases: Data exploration. This step aims to familiarize yourself and understand the data so you can make informed decisions during the following steps. Some of the tasks involved in this phase include summarizing your data, generating plots, validating assumptions, and so on. During this phase you can, for example, identify outliers, missing values, or noisy data points that can be cleaned in the next phase. Chapter 4 will introduce some data exploration techniques. Throughout the book, we will also use some other data exploratory methods but if you are interested in diving deeper into this topic, I recommend you check out the “Exploratory Data Analysis with R” book by Peng (2016). Data cleaning. After the data exploration phase here, we can remove the identified outliers, remove noisy data points, remove variables that are not needed for further computation, and so on. Preprocessing. Predictive models expect the data to be in some structured format and satisfying some constraints. For example, several models are sensitive to class imbalances, i.e., the presence of many instances with a given class but a small number of instances with other classes. In fraud detection scenarios, most of the instances will belong to the normal class but just a small proportion will be of type ‘illegal transaction’. In this case, we may want to do some preprocessing to try to balance the dataset. Some models are also sensitive to feature scale differences. For example, a variable weight could be in kilograms but another variable height in centimeters. Before training a predictive model, the data needs to be prepared in such a way that the models can get the most out of it. Chapter 5 will present some common preprocessing steps. Training and evaluation. Once the data is preprocessed, we can then proceed to train the models. Furthermore, we also need ways to evaluate their generalization performance on new unseen instances. The purpose of this phase is to try, and fine-tune different models to find the one that performs the best. Later in this chapter, some model evaluation techniques will be introduced. Interpretation and presentation of results. The purpose of this phase is to analyze and interpret the models’ results. We can use performance metrics derived from the evaluation phase to make informed decisions. We may also want to understand how the models work internally and how the predictions are derived. 1.5 Evaluating Predictive Models Before showing how to train a machine learning model, in this section, I would like to introduce the process of evaluating a predictive model, which is part of the data analysis pipeline. This applies to both classification and regression problems. I’m starting with this topic because it will be a recurring one every time you work with machine learning. You will also be training a lot of models, but you will need ways to validate them as well. Once you have trained a model (with a training set), that is, finding the best function \\(f\\) that maps inputs to their corresponding outputs, you may want to estimate how good the model is at solving a particular problem when presented with examples it has never seen before (that were not part of the training set). This estimate of how good the model is at predicting the output of new examples is called the generalization performance. To estimate the generalization performance of a model, a dataset is usually divided into a train set and a test set. As the name implies, the train set is used to train the model (learn its parameters) and the test set is used to evaluate/test its generalization performance. We need independent sets because when deploying models in the wild, they will be presented with new instances never seen before. By dividing the dataset into two subsets, we are simulating this scenario where the test set instances were never seen by the model at training time so the performance estimate will be more accurate rather than if we used the same set to train and evaluate the performance. There are two main validation methods that differ in the way the dataset is divided: hold-out validation and k-fold cross validation. 1) Hold-out validation. This method randomly splits the dataset into train and test sets based on some predefined percentages. For example, randomly select \\(70\\%\\) of the instances and use them as the train set and use the remaining \\(30\\%\\) of the examples for the test set. This will vary depending on the application and the amount of data, but typical splits are \\(50/50\\) and \\(70/30\\) percent for the train and test sets, respectively. Figure 1.7 shows an example of a dataset divided into \\(70/30\\). Figure 1.7: Hold-out validation. Then, the train set is used to train (fit) a model, and the test set to evaluate how well that model performs on new data. The performance can be measured using performance metrics such as the accuracy for classification problems. The accuracy is the percent of correctly classified instances. It is a good practice to estimate the performance on both, the train and test sets. Usually, the performance on the train set will be higher since the model was trained with that very same data. It is also common to measure the performance computing the error instead of accuracy. For example, the percent of misclassified instances. These are called the train error and test error (also known as the generalization error), for both the train and test sets respectively. Estimating these two errors will allow you to ‘debug’ your models and understand if they are underfitting or overfitting (more on this in the following sections). 2) K-fold cross-validation. Hold-out validation is a good way to evaluate your models when you have a lot of data. However, in many cases, your data will be limited. In those cases, you want to make efficient use of the data. With hold-out validation, each instance is included either in the train or test set. K-fold cross-validation provides a way in which instances take part in both, the test and train set, thus making more efficient use of the data. This method consists of randomly assigning each instance into one of \\(k\\) folds (subsets) with approximately the same size. Then, \\(k\\) iterations are performed. In each iteration, one of the folds is used to test the model while the remaining ones are used to train it. Each fold is used once as the test set and \\(k-1\\) times it’s used as part of the train set. Typical values for \\(k\\) are \\(3\\), \\(5\\), and \\(10\\). In the extreme case where \\(k\\) is equal to the total number of instances in the dataset, it is called leave-one-out cross-validation (LOOCV). Figure 1.7 shows an example of cross-validation with \\(k=5\\). Figure 1.8: k-fold cross validation with k=5 and 5 iterations. The generalization performance is then computed by taking the average accuracy/error from each iteration. Hold-out validation is typically used when there is a lot of available data and models take significant time to be trained. On the other hand, k-fold cross-validation is used when data is limited. However, it is more computational intensive since it requires training \\(k\\) models. Validation set. Most predictive models require some hyperparameter tuning. For example, a k-NN model requires to set \\(k\\), the number of neighbors. For decision trees, one can specify the maximum allowed tree depth, among other hyperparameters. Neural networks require even more hyperparameter tuning to work properly. Also, one may try different preprocessing techniques and features. All those changes affect the final performance. If all those hyperparameter changes are evaluated using the test set, there is a risk of overfitting the model. That is, making the model very specific to this particular data. Instead of using the test set to fine-tune parameters, a validation set needs to be used instead. Thus, the dataset is randomly partitioned into three subsets: train/validation/test sets. The train set is used to train the model. The validation set is used to estimate the model’s performance while trying different hyperparameters and preprocessing methods. Once you are happy with your final model, you use the test set to assess the final generalization performance and this is what you report. The test set is used only once. Remember that we want to assess performance on unseen instances. When using k-fold cross validation, first, an independent test set needs to be put aside. Hyperparameters are tuned using cross-validation and the test set is used at the very end and just once to estimate the final performance. When working with multi-user systems, we need to additionally take into account between-user differences. In those situations, it is advised to perform extra validations. Those multi-user validation techniques will be covered in chapter 9. 1.6 Simple Classification Example simple_model.R So far, a lot of terminology and concepts have been introduced. In this section, we will work through a practical example that will demonstrate how most of those concepts fit together. Here you will build (from scratch) your first classification and regression models! Furthermore, you will learn how to evaluate their generalization performance. Suppose you have a dataset that contains information about felines including their maximum speed in km/hr and their specific type. For the sake of the example, suppose that these two variables are the only ones that we can observe. As for the types, consider that there are two possibilities: ‘tiger’ and ‘leopard’. Figure 1.9 shows the first \\(10\\) instances (rows) of the dataset. Figure 1.9: First 10 instances of felines dataset. This table has \\(2\\) variables: speed and class. The first one is a numeric variable. The second one is a categorical variable. In this case, it can take two possible values: ‘tiger’ or ‘leopard’. This dataset was synthetically created for illustration purposes, but I promise that after this, we will mostly use real datasets. The code to reproduce this example is contained in the Introduction folder in the script file simple_model.R. The script contains the code used to generate the dataset. The dataset is stored in a data frame named dataset. Let’s start by doing a simple exploratory analysis of the dataset. More detailed exploratory analysis methods will be presented in chapter 4. First, we can print the data frame dimensions with the dim() function. # Print number of rows and columns. dim(dataset) #&gt; [1] 100 2 The output tells us that the data frame has \\(100\\) rows and \\(2\\) columns. Now we may be interested to know from those \\(100\\) instances, how many correspond to tigers. We can use the table() function to get that information. # Count instances in each class. table(dataset$class) #&gt; leopard tiger #&gt; 50 50 Here we can see that \\(50\\) instances are of type ‘leopard’ and also \\(50\\) instances are of type ‘tiger’. In fact, this is how the dataset was intentionally generated. The next thing we can do is compute some summary statistics for each column. R already provides a very convenient function for that purpose. Yes, it is the summary() function. # Compute some summary statistics. summary(dataset) #&gt; speed class #&gt; Min. :42.96 leopard:50 #&gt; 1st Qu.:48.41 tiger :50 #&gt; Median :51.12 #&gt; Mean :51.53 #&gt; 3rd Qu.:53.99 #&gt; Max. :61.65 Since speed is a numeric variable, summary() computes some statistics like the mean, min, max, etc. The class variable is a factor. Thus, it returns row counts instead. In R, categorical variables are usually encoded as factors. It is similar to a string, but R treats factors in a special way. We can already appreciate that with the previous code snippet when the summary function returned class counts. There are many other ways in which you can explore a dataset, but for now, let’s assume we already feel comfortable and that we have a good understanding of the data. Since this dataset is very simple, we won’t need to do any further data cleaning or preprocessing. Now, imagine that you are asked to build a model that is able to predict the type of feline based on observed attributes. In this case, the only thing we can observe is the speed. Our task is to build a function that maps speed measurements to classes. That is, we want to be able to predict what type of feline it is based on how fast it runs. Based on the terminology presented in section 1.3, speed would be a feature variable and class would be the class variable. Based on the types of machine learning presented in section 1.2, this one corresponds to a supervised learning problem because, for each instance, we have its respective label or class which we can use to train a model. And, specifically, since we want to predict a category, this is a classification problem. Before building our classification model, it would be worth plotting the data. Let’s plot the speeds for both tigers and leopards. Figure 1.10: Feline speeds with vertical dashed lines at the means. Here, I omitted the code for building the plot, but it is included in the script. I have also added vertical dashed lines at the mean speeds for the two classes. From this plot, it seems that leopards are faster than tigers (with some exceptions). One thing we can note is that the data points are grouped around the mean values of their corresponding classes. That is, most tiger data points are closer to the mean speed for tigers and the same can be observed for leopards. Of course, there are some exceptions in which an instance is closer to the mean of the opposite class. This could be because some tigers may be as fast as leopards. Some leopards may also be slower than the average, maybe because they are newborns or they are old. Unfortunately, we do not have more information so the best we can do is use our single feature speed. We can use these insights to come up with a simple model that discriminates between the two classes based on this single feature variable. One thing we can do for any new instance we want to classify is to compute its distance to the ‘center’ of each class and predict the class that is the closest one. In this case, the center is the mean value. We can formally define our model as the set of \\(n\\) centrality measures where \\(n\\) is the number of classes (\\(2\\) in our example). \\[\\begin{equation} M = \\{\\mu_1,\\dots ,\\mu_n\\} \\tag{1.2} \\end{equation}\\] Those centrality measures (the class means in this particular case) are called the parameters of the model. Training a model consists of finding those optimal parameters that will allow us to achieve the best performance on new instances that were not part of the training data. In most cases, we will need an algorithm to find those parameters. In our example, the algorithm consists of simply computing the mean speed for each class. That is, for each class, sum all the speeds belonging to that class and divide them by the number of data points in that class. Once those parameters are found, we can start making predictions on new data points. This is called inference or prediction time. In this case, when a new data point arrives, we can predict its class by computing its distance to each of the \\(n\\) centrality measures in \\(M\\) and returning the class of the closest one. The following function implements the training part of our model. # Define a simple classifier that learns # a centrality measure for each class. simple.model.train &lt;- function(data, centrality=mean){ # Store unique classes. classes &lt;- unique(data$class) # Define an array to store the learned parameters. params &lt;- numeric(length(classes)) # Make this a named array. names(params) &lt;- classes # Iterate through each class and compute its centrality measure. for(c in classes){ # Filter instances by class. tmp &lt;- data[which(data$class == c),] # Compute the centrality measure. centrality.measure &lt;- centrality(tmp$speed) # Store the centrality measure for this class. params[c] &lt;- centrality.measure } return(params) } The first argument is the training data and the second argument is the centrality function we want to use (the mean, by default). This function iterates each class, computes the centrality measure based on the speed, and stores the results in a named array called params which is then returned at the end. Most of the time, training a model involves passing the training data and any additional hyperparameters specific to each model. In this case, the centrality measure is a hyperparameter and here we want to use the mean. The difference between parameters and hyperparameters is that the former are learned during training. The hyperparameters are settings specific to each model that we can define before the actual training starts. Now that we have a function that performs the training, we need another one that performs the actual inference or prediction on new data points. Let’s call this one simple.classifier.predict(). Its first argument is a data frame with the instances we want to get predictions for. The second argument is the named vector of parameters learned during training. This function will return an array with the predicted class for each instance in newdata. # Define a function that predicts a class # based on the learned parameters. simple.classifier.predict &lt;- function(newdata, params){ # Variable to store the predictions of # each instance in newdata. predictions &lt;- NULL # Iterate instances in newdata for(i in 1:nrow(newdata)){ instance &lt;- newdata[i,] # Predict the name of the class which # centrality measure is closest. pred &lt;- names(which.min(abs(instance$speed - params))) predictions &lt;- c(predictions, pred) } return(predictions) } This function iterates through each row and computes the distance to each centrality measure and returns the name of the class that was the closest one. The distance computation is done with the following line of code: pred &lt;- names(which.min(abs(instance$speed - params))) First, it computes the absolute difference between the speed and each centrality measure stored in params and then, it returns the name of the one that was the minimum. Now that we have defined the training and prediction procedures, we are ready to test our classifier! In section 1.5, two evaluation methods were presented. Hold-out and k-fold cross-validation. These methods allow you to estimate how your model will perform on new data. Let’s first start with hold-out validation. First, we need to split the data into two independent sets. We will use \\(70\\%\\) of the data to train our classifier and the remaining \\(30\\%\\) to test it. The following code splits dataset into a trainset and testset. # Percent to be used as training data. pctTrain &lt;- 0.7 # Set seed for reproducibility. set.seed(123) idxs &lt;- sample(nrow(dataset), size = nrow(dataset) * pctTrain, replace = FALSE) trainset &lt;- dataset[idxs,] testset &lt;- dataset[-idxs,] The sample() function was used to select integer numbers at random from \\(1\\) to \\(n\\), where \\(n\\) is the total number of data points in dataset. These randomly selected data points are the ones that will go to the train set. Thus, with the size argument we tell the function to return \\(70\\) numbers which correspond to \\(70\\%\\) of the total since dataset has \\(100\\) instances. The last argument replace is set to FALSE because we do not want repeated numbers. This ensures that any instance only belongs to either the train or the test set. We don’t want an instance to be copied into both sets. Now it’s time to test our functions. We can train our model using the trainset by calling our previously defined function simple.model.train(). # Train the model using the trainset. params &lt;- simple.model.train(trainset, mean) # Print the learned parameters. print(params) #&gt; tiger leopard #&gt; 48.88246 54.58369 After training the model, we print the learned parameters. In this case, the mean for tiger is \\(48.88\\) and for leopard, it is \\(54.58\\). With these parameters, we can start making predictions on our test set! We pass the test set and the newly-learned parameters to our function simple.classifier.predict(). # Predict classes on the test set. test.predictions &lt;- simple.classifier.predict(testset, params) # Display first predictions. head(test.predictions) #&gt; [1] &quot;tiger&quot; &quot;tiger&quot; &quot;leopard&quot; &quot;tiger&quot; &quot;tiger&quot; &quot;leopard&quot; Our predict function returns predictions for each instance in the test set. We can use the head() function to print the first predictions. The first two instances were classified as tigers, the third one as leopard, and so on. But how good are those predictions? Since we know what the true classes are (also known as ground truth) in our test set, we can compute the performance. In this case, we will compute the accuracy, which is the percentage of correct classifications. Note that we did not use the class information when making predictions, we only used the speed. We pretended that we didn’t have the true class. We will use the true class only to evaluate the model’s performance. # Compute test accuracy. sum(test.predictions == as.character(testset$class)) / nrow(testset) #&gt; [1] 0.8333333 We can compute the accuracy by counting how many predictions were equal to the true classes and divide them by the total number of points in the test set. In this case, the test accuracy was \\(83.0\\%\\). Congratulations! you have trained and evaluated your first classifier. It is also a good idea to compute the performance on the same train set that was used to train the model. # Compute train accuracy. train.predictions &lt;- simple.classifier.predict(trainset, params) sum(train.predictions == as.character(trainset$class)) / nrow(trainset) #&gt; [1] 0.8571429 The train accuracy was \\(85.7\\%\\). As expected, this was higher than the test accuracy. Typically, what you report is the performance on the test set, but we can use the performance on the train set to look for signs of over/under-fitting which will be covered in the following sections. 1.6.1 K-fold Cross-Validation Example Now, let’s see how k-fold cross-validation can be implemented to test our classifier. I will choose a \\(k=5\\). This means that \\(5\\) independent sets are going to be generated and \\(5\\) iterations will be run. # Number of folds. k &lt;- 5 set.seed(123) # Generate random folds. folds &lt;- sample(k, size = nrow(dataset), replace = TRUE) # Print how many instances ended up in each fold. table(folds) #&gt; folds #&gt; 1 2 3 4 5 #&gt; 21 20 23 17 19 Again, we can use the sample() function. This time we want to select random integers between \\(1\\) and \\(k\\). The total number of integers will be equal to the total number of instances \\(n\\) in the entire dataset. Note that this time we set replace = TRUE since \\(k &lt; n\\) we need to pick repeated numbers. Each number will represent the fold to which each instance belongs to. As before, we need to make sure that each instance belongs only to one of the sets. Here, we are guaranteeing that by assigning each instance a single fold number. We can use the table() function to print how many instances ended up in each fold. Here, we see that the folds will contain between \\(17\\) and \\(23\\) instances. K-fold cross-validation consists of iterating \\(k\\) times. In each iteration, we select one of the folds to function as the test set and the remaining folds are used as the train set. We can then train a model with the train set and evaluate it with the test set. In the end, we report the average accuracy across folds. # Variable to store accuracies on each fold. test.accuracies &lt;- NULL train.accuracies &lt;- NULL for(i in 1:k){ testset &lt;- dataset[which(folds == i),] trainset &lt;- dataset[which(folds != i),] params &lt;- simple.model.train(trainset, mean) test.predictions &lt;- simple.classifier.predict(testset, params) train.predictions &lt;- simple.classifier.predict(trainset, params) # Accuracy on test set. acc &lt;- sum(test.predictions == as.character(testset$class)) / nrow(testset) test.accuracies &lt;- c(test.accuracies, acc) # Accuracy on train set. acc &lt;- sum(train.predictions == as.character(trainset$class)) / nrow(trainset) train.accuracies &lt;- c(train.accuracies, acc) } # Print mean accuracy across folds on the test set. mean(test.accuracies) #&gt; [1] 0.829823 # Print mean accuracy across folds on the train set. mean(train.accuracies) #&gt; [1] 0.8422414 The test mean accuracy across the \\(5\\) folds was \\(\\approx 83\\%\\) which is very similar to the accuracy estimated by hold-out validation. Note that in section 1.5 a validation set was also mentioned. This one is useful when you want to fine-tune a model and/or try different preprocessing methods on your data. In case you are using hold-out validation, you may want to split your data into three sets: train/validation/test sets. So, you train your model using the train set and estimate its performance using the validation set. Then you can fine-tune your model. For example, here, instead of the mean as centrality measure, you can try to use the median and measure the performance again with the validation set. When you are pleased with your settings, you estimate the final performance of the model with the test set only once. In the case of k-fold cross-validation, you can set aside a test set at the beginning. Then you use the remaining data to perform cross-validation and fine-tune your model. Within each iteration, you test the performance with the validation data. Once you are sure you are not going to do any parameter tuning, you can train a model with the train and validation sets and test the generalization performance using the test set. One of the benefits of machine learning is that it allows us to find patterns based on data freeing us from having to program hard-coded rules. This means a more scalable and flexible code. If for some reason, now, instead of \\(2\\) classes we needed to add another class, for example, a jaguar, the only thing we need to do is update our database and retrain our model. We don’t need to modify the internals of the algorithms. They will update themselves based on the data. We can try this by adding a third class to the dataset. The simple_model.R script shows how to add a new class, ‘jaguar’, to the dataset. It then trains the model as usual and performs predictions. 1.7 Simple Regression Example simple_model.R As opposed to classification models where the aim is to predict a category, regression models predict numeric values. To exemplify this, we can use our felines dataset but this time we can try to predict speed based on the type of feline. The class column will be treated as a feature variable and speed will be the response variable. Since there is only one predictor, and it is categorical, the best thing we can do to implement our regression model is to predict the mean speed depending on the class. Recall that for classification, our learned parameters consisted of the means for each class. Thus, we can reuse our training function simple.model.train(). All we need to do is define a new predict function that returns the speed based on the class. This is the opposite of what we did in classification (return the class based on the speed). # Define a function that predicts speed # based on the type of feline. simple.regression.predict &lt;- function(newdata, params){ # Variable to store the predictions of # each instance in newdata. predictions &lt;- NULL # Iterate instances in newdata for(i in 1:nrow(newdata)){ instance &lt;- newdata[i,] # Return the mean value of the corresponding class stored in params. pred &lt;- params[which(names(params) == instance$class)] predictions &lt;- c(predictions, pred) } return(predictions) } The simple.regression.predict() function iterates through each instance in newdata and returns the mean speed from params for the corresponding class. Again, we can validate our model using hold-out validation. The train set will have \\(70\\%\\) of the instances and the remaining will be used as the test set. pctTrain &lt;- 0.7 set.seed(123) idxs &lt;- sample(nrow(dataset), size = nrow(dataset) * pctTrain, replace = FALSE) trainset &lt;- dataset[idxs,] testset &lt;- dataset[-idxs,] # Reuse our train function. params &lt;- simple.model.train(trainset, mean) print(params) #&gt; tiger leopard #&gt; 48.88246 54.5836 Here, we reused our previous function simple.model.train() to learn the parameters and then print them. Then we can use those parameters to infer the speed. If a test instance belongs to the class ‘tiger’ then return \\(48.88\\). If it is of class ‘leopard’ then return \\(54.58\\). # Predict speeds on the test set. test.predictions &lt;- simple.regression.predict(testset, params) # Print first predictions. head(test.predictions) #&gt; 48.88246 54.58369 54.58369 48.88246 48.88246 54.58369 Since these are numeric predictions, we cannot use accuracy as with classification to evaluate the performance. One way to evaluate how well these predictions are is by computing the mean absolute error (MAE). This measure tells you, on average, how much each prediction deviates from its true value. It is computed by subtracting each prediction from its real value and taking the absolute value: \\(|predicted - realValue|\\). This can be visualized in Figure 1.11. The distances between the true and predicted values are the errors and the MAE is the average of all those errors. Figure 1.11: Prediction errors. We can use the following code to compute the MAE: # Compute mean absolute error (MAE) on the test set. mean(abs(test.predictions - testset$speed)) #&gt; [1] 2.562598 The MAE on the test set was \\(2.56\\). That is, on average, our simple model had a deviation of \\(2.56\\) km/hr. with respect to the true values, which is not bad. We can also compute the MAE on the train set. # Predict speeds on the train set. train.predictions &lt;- simple.regression.predict(trainset, params) # Compute mean absolute error (MAE) on the train set. mean(abs(train.predictions - trainset$speed)) #&gt; [1] 2.16097 The MAE on the train set was \\(2.16\\), which is better than the test set MAE (small MAE values are preferred). Now, you have built, trained, and evaluated a regression model! This was a simple example, but it illustrates the basic idea of regression and how it differs from classification. It also shows how the performance of regression models is typically evaluated with the MAE as opposed to the accuracy used in classification. In chapter 8, more advanced methods such as neural networks will be introduced, which can be used to solve regression problems. In this section, we have gone through several of the data analysis pipeline phases. We did a simple exploratory analysis of the data and then we built, trained, and validated the models to perform both classification and regression. Finally, we estimated the overall performance of the models and presented the results. Here, we coded our models from scratch, but in practice, you typically use models that have already been implemented and tested. All in all, I hope these examples have given you the feeling of how it is to work with machine learning. 1.8 Underfitting and Overfitting From the felines classification example, we saw how we can separate two classes by computing the mean for each class. For the two-class problem, this is equivalent to having a decision line between the two means (Figure 1.12). Everything to the right of this decision line will be closer to the mean that corresponds to ‘leopard’ and everything to the left to ‘tiger’. In this case, the classification function is a vertical line, and during learning the position of the line that reduces the classification error is searched for. We implicitly estimated the position of that line by finding the mean values for each of the classes. Figure 1.12: Decision line between the two classes. Now, imagine that we do not only have access to the speed but also to the felines’ age. This extra information could help us reduce the prediction error since age plays an important role in how fast a feline is. Figure 1.13 (left) shows how it will look like if we plot age in the x-axis and speed in the y-axis. Here, we can see that for both, tigers and leopards, the speed seems to increase as age increases. Then, at some point, as age increases the speed begins to decrease. Constructing a classifier with a single vertical line as we did before will not work in this \\(2\\)-dimensional case where we have \\(2\\) predictors. We will need a more complex decision boundary (function) to separate the two classes. One approach would be to use a line as before but this time we allow the line to have an angle. Everything below the line is classified as ‘tiger’ and everything else as ‘leopard’. Thus, the learning phase involves finding the line’s position and its angle that achieves the smallest error. Figure 1.13 (left) shows a possible decision line. Even though this function is more complex than a vertical line, it will still produce a lot of misclassifications (it does not clearly separate both classes). This is called underfitting, that is, the model is so simple that it is not able to capture the underlying data patterns. Figure 1.13: Underfitting and overfitting. Let’s try a more complex function, for example, a curve. Figure 1.13 (middle) shows that a curve does a better job at separating the two classes with fewer misclassifications but still, \\(3\\) leopards were misclassified as tigers and \\(1\\) tiger was misclassified as a leopard. Can we do better than that? Yes, just keep increasing the complexity of the decision function. Figure 1.13 (right) shows a more complex function that was able to separate the two classes with \\(100\\%\\) accuracy or equivalently, with a \\(0\\%\\) error. However, there is a problem. This function learned how to accurately separate the training data, but it is likely that it will not do as well with a new test set. This function became so specialized in this data that it failed to capture the overall pattern. This is called overfitting. In this case, the model starts to ‘memorize’ the train set instead of finding general patterns applicable to new unseen instances. If we were to choose a model, the best one would be the one in the middle. Even if it was not perfect on the train data, it will do better than the other models when evaluating it on new test data. Overfitting is a common problem in machine learning. One way to know if a model is overfitting is if the error in the train set is low, but it is high on a new set (can be a test or validation set). Figure 1.14 illustrates this idea. Too-simple models will have a high error in both, the train and validation sets. As the complexity of the model increases, the error on both sets is reduced. Then, at some point, the complexity of a model is too high so that it gets too specific on the train set and fails to perform well on a new independent set. Figure 1.14: Model complexity v.s. train and validation error. In this example, we saw how underfitting and overfitting can affect the generalization performance of a model in a classification setting but the same can occur in regression problems. There are several methods that aim to reduce overfitting but many of them are specific to each type of model. For example, with decision trees (covered in chapter 2), one way to reduce overfitting is to limit their depth or build ensembles of trees (chapter 3). Neural networks are also highly prone to overfitting since they can be very complex with millions of parameters and there are also several techniques to reduce the effect of overfitting (chapter 8). 1.9 Bias and Variance So far, we have seen how we can train predictive models and evaluate how well they do on new data (test/validation sets). The main goal is to have predictive models that have a low error rate with new data. Understanding the source of the error can help us make more informed decisions when building predictive models. The test error, also known as the generalization error of a predictive model can be decomposed into three components: a bias, a variance, and noise. Noise. This component is related to the data itself and there is nothing we can do about it. For example, two instances having the same values in their features but with a different label. Bias. The bias is how much the average prediction differs from the true value. Note the average keyword. This means that we make the assumption that an infinite (or very large) number of train sets can be generated, and for each a predictive model is trained. Then we average the predictions of all those models and see how much that average differs from the true value. Variance. The variance relates to how much the predictions change for a given data point when training a model using a different train set each time. The following picture graphically illustrates different scenarios for high/low bias and variance. The center of the target represents the true value and the small red dots the predictions on a hypothetical test set. Figure 1.15: Error due to bias and variance. Bias and variance are closely related to underfitting and overfitting. High variance is a sign of overfitting. That is, a model is so complex that it will fit a particular train set very well. Every time it is trained with a different train set, the train error will be low, but it will likely generate very different predictions for the same test points and a much higher test error. Figure 1.16 illustrates the relation between overfitting and high variance with a regression problem. Given a feature \\(x\\), two models are trained to predict \\(y\\): i) a complex model (top row), and ii) a simpler model (bottom row). Both models are fitted with two training sets (\\(a\\) and \\(b\\)) sampled from the same distribution. The complex model fits the train data perfectly but makes very different predictions (big \\(\\Delta\\)) for the same test point when using a different train set. The simpler model does not fit the train data so well but has a smaller \\(\\Delta\\) and a lower error on the test point as well. Visually, the function (red curve) of the complex model also varies a lot across train sets whereas the shapes of the simpler model functions look very similar. Figure 1.16: High variance and overfitting. On the other hand, if a model is too simple, it will underfit causing highly biased results and not being able to capture the input-output relationships. This results in a high train error and in consequence a high test error as well. A formal formulation of the error decomposition can be consulted in the book “The elements of statistical learning: data mining, inference, and prediction” (Hastie, Tibshirani, and Friedman 2009). 1.10 Summary In this chapter, several introductory machine learning concepts and terminology were introduced. These concepts are the basis for the methods that will be covered in the following chapters. Behavior can be defined as “an observable activity in a human or animal”. Three main reasons of why we may want to analyze behavior automatically were discussed: react, understand, and document/archive. One way to observe behavior automatically is through the use of sensors and/or data. Machine Learning consists of a set of computational algorithms that automatically find useful patterns and relationships from data. The three main building blocks of machine learning are: data, algorithms, and models. The main types of machine learning are supervised learning, semi-supervised learning, partially-supervised learning, and unsupervised learning. In R, data is usually stored in data frames. Data frames have variables (columns) and instances (rows). Depending on the task, variables can be independent or dependent. A predictive model is a model that takes some input and produces an output. Classifiers and regressors are predictive models. A data analysis pipeline consists of several tasks including data collection, cleaning, preprocessing, training/evaluation, and presentation of results. Model evaluation can be performed with hold-out validation or k-fold cross-validation. Overfitting occurs when a model ‘memorizes’ the training data instead of finding useful underlying patterns. The test error can be decomposed into noise, bias, and variance. References "],
["classification.html", "Chapter 2 Predicting Behavior with Classification Models 2.1 k-nearest Neighbors 2.2 Performance Metrics 2.3 Decision Trees 2.4 Naive Bayes 2.5 Dynamic Time Warping 2.6 Summary", " Chapter 2 Predicting Behavior with Classification Models In the previous chapter, the concept of classification was introduced along with a simple example (feline type classification). This chapter will cover more in depth concepts on classification methods and their application to behavior analysis tasks. Moreover, additional performance metrics will be introduced. This chapter begins with an introduction to \\(k\\)-nearest neighbors (\\(k\\)-NN) which is one of the simplest classification algorithms. Then, an example of \\(k\\)-NN applied to indoor location using Wi-Fi signals is presented. This chapter also covers Decision Trees and Naive Bayes classifiers and how they can be used for activity recognition based on smartphone accelerometer data. After that, Dynamic Time Warping (DTW) (a method for aligning time series) is introduced, and an example of how it can be used for hand gesture recognition is presented. 2.1 k-nearest Neighbors \\(k\\)-nearest neighbors (\\(k\\)-NN) is one of the simplest classification algorithms. The predicted class for a given query instance is the most common class of its k nearest neighbors. A query instance is just the instance we want to make predictions on. In its most basic form, the algorithm consists of two steps: Compute the distance between the query instance and all training instances. Return the most common class among the k nearest training instances. This is a type of lazy-learning algorithm because all the computations take place at prediction time. There are no parameters to learn at training time! The training phase consists only of storing the training instances so they can be compared to the query instance at prediction time. The hyper-parameter k is usually specified by the user and will depend on each application. We also need to specify a distance function such that similar instances should have smaller distances between them in the feature space and dissimilar instances should have longer distances. For numeric features, the Euclidean distance is one of the most commonly used distance metrics. The Euclidean distance between two points can be computed as follows: \\[\\begin{equation} d\\left(p,q\\right) = \\sqrt{\\sum_{i=1}^n{\\left(p_i-q_i\\right)^2}} \\tag{2.1} \\end{equation}\\] where \\(p\\) and \\(q\\) are \\(n\\)-dimensional feature vectors and \\(i\\) is the index to the vectors’ elements. Figure 2.1 shows the idea graphically. (Figure inspired from the k-nn article3 in Wikipedia). The query instance is depicted with the ‘?’ symbol. If we choose \\(k=3\\) (represented by the inner dashed circle) the predicted class is ‘square’ because there are two squares but only one circle. If \\(k=5\\) (outer dotted circle), the predicted class is ‘circle’. Figure 2.1: k-NN example for k=3 (inner dashed circle) and k=5 (dotted outer circle). Typical values for \\(k\\) are small odd numbers like \\(1,2,3,5\\). The \\(k\\)-nn algorithm can also be used for regression with a small modification: Instead of returning the majority class of the nearest neighbors, return the mean value of their response variable. Despite its simplicity, \\(k\\)-nn has proved to perform really well in many tasks including time series classification (Xi et al. 2006). 2.1.1 Indoor Location with Wi-Fi Signals indoor_classification.R indoor_auxiliary.R It is possible that you might already have experienced some troubles with geolocation services when you are inside a building. Part of this is because GPS technologies do not provide good indoors accuracy due to several sources of interference. For some applications, it would be beneficial to have accurate location estimations inside buildings even at room-level. For example, in domotics and localization services in big public places like airports or shopping malls. Having good indoor location estimates can also be used in behavior analysis such as extracting trajectory patterns. In this section, we will implement \\(k\\)-NN to perform indoor location in a building based on Wi-Fi signals. For instance, we can use a smartphone to scan the nearby Wi-Fi access points and based on this information, determine our location at room-level. This can be formulated as a classification problem: Given a set of Wi-Fi signals as input, predict the location where the device is located. For this classification problem, we will use the INDOOR LOCATION dataset (see Appendix B for more info.) which was collected with an Android smartphone. The smartphone application scans the nearby access points and stores their information and label. The label is provided by the user and represents the room where the device is located. Several instances for every location were recorded. To generate each instance, the device scans and records the MAC address and signal strength of the nearby access points. A delay of \\(500\\) ms is set between scans. For each location, approximately \\(3\\) minutes of data were collected while the user walked around the specific room. Figure 2.2 depicts the layout of the building where the data was collected. The data has four different locations: ‘bedroomA’, ‘beadroomB’, ‘tvroom’, and the ‘lobby’. The lobby (not shown in the layout) is at the same level as bedroom A but on the first floor. Figure 2.2: Layout of the apartments building. Table 2.1 shows the first rows of the dataset. The first column is the class. scanid column is a unique identifier for the given Wi-Fi scan (instance). To preserve privacy, MAC addresses were converted into integer values. Every instance is composed of several rows. For example, the first instance with scanid=1 has two rows (one row per mac address). Intuitively, same locations should have similar MAC addresses. From the table, we can see that at bedroomA access points with MAC address \\(1\\) and \\(2\\) are usually found by the device. Table 2.1: First rows of wifi scans. locationid scanid mac signalstrength bedroomA 1 1 -88.500 bedroomA 1 2 -91.000 bedroomA 2 1 -88.000 bedroomA 2 2 -90.000 bedroomA 3 1 -87.625 bedroomA 3 2 -90.000 bedroomA 4 2 -90.250 bedroomA 4 1 -90.000 bedroomA 4 3 -91.000 Since each instance is composed of several rows, we will convert our data frame into a list of lists where each inner list represents a single instance with the class (locationId), a unique id, and a data frame with the corresponding access points. The example code can be found in the script indoor_classification.R. # Read Wi-Fi data df &lt;- read.csv(datapath, stringsAsFactors = F) # Convert data frame into a list of lists. # Each inner list represents one instance. dataset &lt;- wifiScansToList(df) # Print number of instances in the dataset. length(dataset) #&gt; [1] 365 # Print the first instance. dataset[[1]] #&gt; $locationId #&gt; [1] &quot;bedroomA&quot; #&gt; #&gt; $scanId #&gt; [1] 1 #&gt; #&gt; $accessPoints #&gt; mac signalstrength #&gt; 1 1 -88.5 #&gt; 2 2 -91.0 First, we read the dataset from the csv file and store it in the data frame df. To make things easier, the data frame is converted into a list of lists using the auxiliary function wifiScansToList() which is defined in the script indoor_auxiliary.R. Next we print the number of instances in the dataset, that is, the number of lists in the dataset list. The dataset list contains \\(365\\) instances. The \\(365\\) was just a coincidence, the data was not collected every day during one year but in the same day. Next, we extract the first instance with dataset[[1]]. Here, we can see that each instance has three pieces of information. The class (locationId), a unique id (scanId), and a set of access points stored in a data frame. The first instance has two access points with MAC addresses \\(1\\) and \\(2\\). There is also information about the signal strength, though, this one will not be used. Since we would expect that similar locations have similar MAC addresses and locations that are far away from each other have different MAC addresses, we need a distance measure that captures this notion of similarity. In this case, we cannot use the Euclidean distance on MAC addresses. Even though they were encoded as integer values, they do not represent magnitudes but unique identifiers. Each instance is composed of a set of \\(n\\) MAC addresses stored in the accessPoints data frame. To compute the distance between two instances (two sets) we can use the Jaccard distance. This distance is based on element sets: \\[\\begin{equation} j\\left(A,B\\right)=\\frac{\\left|A\\cup B\\right|-\\left|A\\cap B\\right|}{\\left|A\\cup B\\right|} \\tag{2.2} \\end{equation}\\] where \\(A\\) and \\(B\\) are sets of MAC addresses. A set is an unordered collection of elements. As an example, let’s say we have two sets, \\(S_1\\) and \\(S_2\\): \\[\\begin{align*} S_1&amp;=\\{a,b,c,d,e\\}\\\\ S_2&amp;=\\{e,f,g,a\\} \\end{align*}\\] The set \\(S_1\\) has \\(5\\) elements (letters) and \\(S_2\\) has \\(4\\) elements. \\(A \\cup B\\) means the union of the two sets \\(A\\) and \\(B\\) and its result is the set of all elements that are either in \\(A\\) or \\(B\\). For instance, the union of \\(S_1\\) and \\(S_2\\) is \\(S_1 \\cup S_2 = \\{a,b,c,d,e,f,g\\}\\). The \\(A \\cap B\\) denotes the intersection between \\(A\\) and \\(B\\) which is the set of elements that are in both \\(A\\) and \\(B\\). In our example, \\(S_1 \\cap S_2 = \\{a,e\\}\\). Finally the vertical bars \\(||\\) mean the cardinality of the set, that is, the number of elements. The cardinality of \\(S_1\\) is \\(|S_1|=5\\) because it has \\(5\\) elements. The cardinality of the union of the two sets \\(|S_1 \\cup S_2|=7\\) because the set that results from the union of \\(S_1\\) and \\(S_2\\) has \\(7\\) elements. In R, we can implement the Jaccard distance as follows: jaccardDistance &lt;- function(set1, set2){ lengthUnion &lt;- length(union(set1, set2)) lengthIntersectoin &lt;- length(intersect(set1, set2)) d &lt;- (lengthUnion - lengthIntersectoin) / lengthUnion return(d) } The implementation is in the script indoor_auxiliary.R. Now, we can try our function! Let’s compute the distance between two instances of the same class (‘bedroomA’). # Compute jaccard distance between instances with same class: # (bedroomA) jaccardDistance(dataset[[1]]$accessPoints$mac, dataset[[4]]$accessPoints$mac) #&gt; [1] 0.3333333 Now let’s try to compute the distance between instances with different classes. # Jaccard distance of instances with different class: # (bedroomA and bedroomB) jaccardDistance(dataset[[1]]$accessPoints$mac, dataset[[210]]$accessPoints$mac) #&gt; [1] 0.6666667 The distance between instances of the same class was \\(0.33\\) whereas the distance between instances of different class was \\(0.66\\). So, our function is working as expected. In the extreme case when sets \\(A\\) and \\(B\\) are the same, the distance will be \\(0\\). When there are no common elements in the sets, the distance will be \\(1\\). Armed with this distance metric, we can now implement the \\(k\\)-nn function in R. The knn_classifier() implementation is in the script indoor_auxiliary.R. Its first argument is the dataset (the list of instances). The second argument k, is the number of nearest neighbors to use, and the last two arguments are the indices of the train and test instances, respectively. This indices are pointers to the elements in the dataset variable. knn_classifier &lt;- function(dataset, k, trainSetIndices, testSetIndices){ groundTruth &lt;- NULL predictions &lt;- NULL for(queryInstance in testSetIndices){ distancesToQuery &lt;- NULL for(trainInstance in trainSetIndices){ jd &lt;- jaccardDistance(dataset[[queryInstance]]$accessPoints$mac, dataset[[trainInstance]]$accessPoints$mac) distancesToQuery &lt;- c(distancesToQuery, jd) } indices &lt;- sort(distancesToQuery, index.return = TRUE)$ix indices &lt;- indices[1:k] # Indices of the k nearest neighbors nnIndices &lt;- trainSetIndices[indices] # Get the actual instances nnInstances &lt;- dataset[nnIndices] # Get their respective classes nnClasses &lt;- sapply(nnInstances, function(e){e[[1]]}) prediction &lt;- Mode(nnClasses) predictions &lt;- c(predictions, prediction) groundTruth &lt;- c(groundTruth, dataset[[queryInstance]]$locationId) } return(list(predictions = predictions, groundTruth = groundTruth)) } For each instance queryInstance in the test set, the knn_classifier() computes its jaccard distance to every other instance in the train set and stores those distances in distancesToQuery. Then, those distances are sorted in ascending order and the most common class among the first \\(k\\) elements is returned as the predicted class. The function Mode() was used to return the most common element. Finally, knn_classifier() returns a list with the predictions for every instance in the test set and their respective ground truth class for evaluation. Now, we can try our classifier. We will use \\(70\\%\\) of the dataset as train set and the remaining as the test set. # Total number of instances numberInstances &lt;- length(dataset) # Set seed for reproducibility set.seed(12345) # Split into train and test sets. trainSetIndices &lt;- sample(1:numberInstances, size = round(numberInstances * 0.7), replace = F) testSetIndices &lt;- (1:numberInstances)[-trainSetIndices] The function knn_classifier() will predict the class for each test set instance and will return a list with their predictions and their ground truth classes. With this information, we can compute the accuracy on the test set which is the percentage of correctly classified instances. For this example, I will set \\(k=3\\). # Obtain predictions on the test set. result &lt;- knn_classifier(dataset, k = 3, trainSetIndices, testSetIndices) # Calculate and print accuracy. sum(result$predictions == result$groundTruth) / length(result$predictions) #&gt; [1] 0.9454545 Not bad! Our simple \\(k\\)-nn algorithm achieved an accuracy of \\(94.5\\%\\). Usually, it is a good idea to visualize the predictions to have a better understanding of the classifier’s behavior. Confusion matrices allow us to precisely do that. We can use the confusionMatrix() function from the caret package to generate a confusion matrix. Its first argument is a factor with the predictions and the second one is a factor with the corresponding true values. This function returns an object with several performance metrics (see next section) and the confusion matrix. library(caret) cm &lt;- confusionMatrix(factor(result$predictions), factor(result$groundTruth)) cm$table # Access the confusion matrix. #&gt; Reference #&gt; Prediction bedroomA bedroomB lobby tvroom #&gt; bedroomA 26 0 3 1 #&gt; bedroomB 0 17 0 1 #&gt; lobby 0 1 28 0 #&gt; tvroom 0 0 0 33 The columns represent the true classes and rows the predictions. For example, from the total \\(31\\) instances of type ‘lobby’, \\(28\\) of them were correctly classified as ‘lobby’ but \\(3\\) of them were misclassified as ‘bedroomA’. Something I find useful is to plot the confusion matrix as proportions instead of counts (Figure 2.3). From this confusion matrix we can see that for the class ‘bedroomB’, \\(94\\%\\) of the instances were correctly classified and \\(6\\%\\) were mislabeled as ‘lobby’. On the other hand, instances of type ‘bedroomA’ were always classified correctly. Figure 2.3: Confusion matrix for location predictions. A confusion matrix is a good way to analyze classification results per class and spot weaknesses which we can use to improve the model, for example, by extracting additional features. 2.2 Performance Metrics Performance metrics allow us to measure the generalization performance of a model from several angles. The most common performance metric for classification is the accuracy: \\[\\begin{equation} accuracy = \\frac{\\# \\textrm{ correctly classified instances}}{\\textrm{total } \\# \\textrm{ instances}} \\tag{2.3} \\end{equation}\\] In order to have a better understanding of the generalization performance of a model, it is a good practice to compute several performance metrics in addition to the accuracy. Accuracy also has some limitations, especially in highly imbalanced datasets. The following metrics provide different views of a model’s performance for the binary case (when there are only two classes). These metrics can be extended to the multi-class setting using a one v.s. all approach. That is, compare each class to the remaining classes. Before introducing the other metrics, it is convenient to define some terms: True positives (TP): Positive examples classified as positives. True negatives (TN): Negative examples classified as negatives. False positives (FP): Negative examples misclassified as positives. False negatives (FN): Positive examples misclassified as negatives. For binary classification, it is you who decides which is the positive class. For example, if your problem is about detecting falls and you have two classes: ‘fall’ and ‘nofall’, then, considering ‘fall’ as the positive class makes sense since that is the one you are most interested in detecting. The following, is a list of commonly used metrics in classification: Recall: The proportion of positives that are classified as such. Alternative names for recall are: true positive rate, sensitivity, and hit rate. In fact, the diagonal of the confusion matrix with proportions of the indoor location example shows the recall for each class (Figure 2.3). \\[\\begin{equation} recall = \\frac{\\textrm{TP}}{\\textrm{P}} \\tag{2.4} \\end{equation}\\] Specificity: The proportion of negatives classified as such. It is also called the true negative rate. \\[\\begin{equation} specificity = \\frac{\\textrm{TN}}{\\textrm{N}} \\tag{2.5} \\end{equation}\\] Precision: The fraction of true positives among those classified as positives. Also known as the positive predictive value. \\[\\begin{equation} precision = \\frac{\\textrm{TP}}{\\textrm{TP + FP}} \\tag{2.6} \\end{equation}\\] F1-score: This is the harmonic mean of precision and recall. \\[\\begin{equation} \\textit{F1-score} = 2 \\cdot \\frac{\\textrm{precision} \\cdot \\textrm{recall}}{\\textrm{precision + recall}} \\tag{2.7} \\end{equation}\\] The confusionMatrix() function from the caret package computes several of those metrics. From our previous confusion matrix object, we can see those metrics by class. cm$byClass[,c(&quot;Recall&quot;, &quot;Specificity&quot;, &quot;Precision&quot;, &quot;F1&quot;)] #&gt; Recall Specificity Precision F1 #&gt; Class: bedroomA 1.0000000 0.9523810 0.8666667 0.9285714 #&gt; Class: bedroomB 0.9444444 0.9891304 0.9444444 0.9444444 #&gt; Class: lobby 0.9032258 0.9873418 0.9655172 0.9333333 #&gt; Class: tvroom 0.9428571 1.0000000 1.0000000 0.9705882 The mean of the metrics across all classes can be computed by taking the mean for each column of the returned object: colMeans(cm$byClass[,c(&quot;Recall&quot;, &quot;Specificity&quot;, &quot;Precision&quot;, &quot;F1&quot;)]) #&gt; Recall Specificity Precision F1 #&gt; 0.9476318 0.9822133 0.9441571 0.9442344 2.3 Decision Trees Decision trees are powerful predictive models (especially when combining several of them, see chapter 3) used for classification and regression tasks. Here, the focus will be on classification. Each node in a tree represents partial or final decisions based on a single feature. If a node is a leaf, then it leads to a final decision. A leaf is simply a terminal node, i.e, it has no children nodes. Given a feature vector representing an instance, the predicted class is obtained by testing the feature values and following the tree path until a leaf is reached. Figure 2.4 shows an example decision tree and a query instance with an unknown class. To get the final class, features are evaluated starting at the root. In this case number_wheels is \\(4\\) in the query instance so we take the left path from the root. Now, we need to evaluate weight. This time the test is false since the weight is \\(2300\\) and we take the right path. Since this is a leaf node the final predicted class is ‘truck’. Usually, small trees are preferred (small depth) because they are easier to visualize and interpret and are less prone to overfitting. The example tree has a depth of 2. Should the number of wheels had been \\(2\\) instead of \\(4\\), then testing the weight feature would not have been necessary. Figure 2.4: Example decision tree. The query instance is classified as truck by this tree. As shown in the example, decision trees are easy to interpret and the explanation of a final result can be obtained by just following the path. Now let’s see how these decision trees are learned from data. Consider the following artificial cinema dataset (Figure 2.5). Figure 2.5: Cinema dataset. The first \\(4\\) variables are features and the last column is the class. The class is the decision whether or not we should go to the movies based on the other variables. In this case, all variables are binary except Price which has three possible values: low, medium, and high. Tired: Indicates if the person is tired or not. Rain: Whether it is raining or not. Comedy: Indicates if the genre of the movie is comedy. Price: Ticket price. Go: The decision of whether to go to the movies or not. The main question when building a tree is which feature should be at the root (top). Once you answer this question, you may need to grow the tree by adding another feature (node) as one of the root’s children. To decide which new feature to add you need to answer the same first question: “What feature should be at the root of this subtree?”. This is a recursive definition! The tree keeps growing until you reach a leaf node, there are no more features to select from, or you have reached a predefined maximum depth. For the cinema dataset we need to find which is the best variable to be placed at the root. Let’s suppose we need to choose between Price and Comedy. Figure 2.6 shows these two possibilities. Figure 2.6: Two example trees with one variable split by Price (left) and Comedy (right). If we select Price, there are three possible subnodes, one for each value: low, medium, and high. If Price is low then four instances fall into this subtree (the first four from the table). For all of them, the value of Go is \\(1\\). If Price is high, two instances fall into this category and their Go value is \\(0\\), thus if the price is high then you should not go to the movies according to this data. There are six instances for which the Price value is medium. From those, two of them have Go=1 and the remaining four have Go=0. For cases when the price is low or high we can arrive at a solution. If the price is low then go to the cinema, if the price is high then do not go. However, if the price is medium it is still not clear what to do since this subnode is not pure. That is, the types of the instances are mixed: two with an output of \\(1\\) and four with an output of \\(0\\). In this case we can try to use another feature to decide and grow the tree but first, let’s look at what happens if we decide to use Comedy as the first feature at the root. In this case, we end up with two subsets with six instances each. And for each subnode, what decision should we take is still not clear because the output is ‘mixed’ (Go: 3, NotGo: 3). At this point we would need to continue growing the tree below each subnode. Intuitively, it seems like Price is a better feature since its subnodes are more pure. Then we can use another feature to split the instances whose Price is medium. For example, using the Comedy variable. Figure 2.7 shows how this would look like. Since one of the subnodes of Comedy is still not pure we can further split it using the Rain variable, for example. At this point, we can not split any further. Note that the Tired variable was never used. Figure 2.7: Tree splitting example. Left: tree splits. Right: Highlighted instances when splitting Comedy. So far, we have chosen the root variable based on which one looks more pure but to automate the process, we need a way to measure this purity in a quantitative manner. One way to do that is by using the entropy. Entropy is a measure of uncertainty from information theory. It is zero when there is no uncertainty and one when there is complete uncertainty. The entropy of a discrete variable \\(X\\) with values \\(x_1\\dots x_n\\) and probability mass function \\(P(X)\\) is: \\[\\begin{equation} H(X) = -\\sum_{i=1}^n{P(x_i)log P(x_i)} \\tag{2.8} \\end{equation}\\] Take for example a fair coin with probability of heads and tails = \\(0.5\\) each. The entropy for that coin is: \\[\\begin{equation*} H(X) = - (0.5)log(0.5) + (0.5)log(0.5) = 1 \\end{equation*}\\] Since we do not know what will be the result when we drop the coin, the entropy is maximum. Now consider the extreme case when the coin is biased such that the probability of heads is \\(1\\) and the probability of tails is \\(0\\). The entropy in this case is zero: \\[\\begin{equation*} H(X) = - (1)log(1) + (0)log(0) = 0 \\end{equation*}\\] If we know that the result is always going to be heads, then there is no uncertainty when the coin is dropped. The entropy of \\(p\\) positive examples and \\(n\\) negative examples is: \\[\\begin{equation} H(p, n) = - (\\frac{p}{p+n})log(\\frac{p}{p+n}) + (\\frac{n}{p+n})log(\\frac{n}{p+n}) \\tag{2.9} \\end{equation}\\] Thus, the entropies for the three possible values of Price are: \\[\\begin{equation*} H_{price=low}(4, 0) = - (\\frac{4}{4+0})log(\\frac{4}{4+0}) + (\\frac{0}{4+0})log(\\frac{0}{4+0}) = 0 \\end{equation*}\\] \\[\\begin{equation*} H_{price=medium}(2, 4) = - (\\frac{2}{2+4})log(\\frac{2}{2+4}) + (\\frac{4}{2+4})log(\\frac{4}{2+4}) = 0.918 \\end{equation*}\\] \\[\\begin{equation*} H_{price=high}(0, 2) = - (\\frac{0}{0+2})log(\\frac{0}{0+2}) + (\\frac{2}{0+2})log(\\frac{2}{0+2}) = 0 \\end{equation*}\\] The average of those three can be calculated by taking into account the number of corresponding instances for each value and the total number of instances (\\(12\\)): \\[\\begin{equation*} meanH(price) = (4/12)(0) + (6/12)(0.918) + (2/12)(0) = 0.459 \\end{equation*}\\] Before deciding to split on Price the entropy of the entire dataset is \\(1\\) since there are six positive and negative examples: \\[\\begin{equation*} H(6,6) = 1 \\end{equation*}\\] thus, the information gain for Price is: \\[\\begin{equation*} infoGain(Price) = 1 - meanH(Price) = 1 - 0.459 = 0.541 \\end{equation*}\\] For the rest of the variables the information gain is: \\(infoGain(Tired) = 0\\) \\(infoGain(Rain) = 0.020\\) \\(infoGain(Comedy) = 0\\) The highest information gain is produced by Price, thus, it is selected as the root node. Then, the process continues recursively for each branch but excluding Price. Since branches with values low and high are already done, we only need to further split medium. Sometimes it is not possible to have completely pure nodes like low and high. This can happen for example, when there are no more attributes left or when two or more instances have the same feature values but different labels. In those situations the final prediction is the most common label (majority vote). There exist many implementations of decision trees. Some implementations compute variable importance using the entropy (as shown here) but others use the Gini index, for example. Each implementation also treats numeric variables in different ways. Pruning the tree using different techniques is also common in order to reduce its size. Some of the most common implementations are C4.5 trees (Quinlan 2014) and CART (Steinberg and Colla 2009). The later is implemented in the rpart R package (Therneau and Atkinson 2019) which will be used in the following section to build a model that predicts physical activities from smartphones sensor data. 2.3.1 Activity Recognition with Smartphones smartphone_activities.R As mentioned in the introduction, behavior can be an observable activity in a human. We can easily infer what physical activity someone is doing by looking at her/his body movements. Observing physical activities can provide useful behavioral and contextual information about someone. This can also be used as a proxy to, for example, infer someone’s health condition by detecting deviations in activity patterns. Nowadays, most smartphones come with a tri-axial accelerometer sensor. This sensor measures gravitational forces from the \\(x\\), \\(y\\), and \\(z\\) axes. This information can be used to capture movement patterns from the user and automate the process of monitoring the type of physical activity being performed. In this section, we will use decision trees to automatically classify physical activities from acceleration data. We will use the SMARTPHONE ACTIVITIES dataset that contains acceleration recordings that were collected with a smartphone. This dataset is called WISDM4 and was made available by Kwapisz, Weiss, and Moore (2010). The dataset has \\(6\\) different activities: ‘walking’, ‘jogging’, ‘walking upstairs’, ‘walking downstairs’, ‘sitting’ and ‘standing’. The data was collected by \\(36\\) volunteers with an Android phone located in their pant’s pocket and with a sampling rate of \\(20Hz\\) (\\(1\\) sample every \\(50\\) milliseconds). The dataset contains two types of files. One with the raw accelerometer data and the other one after feature extraction. Figure 2.8 shows the first \\(10\\) lines of the raw accelerometer values of the first file. The first column is the id of the user that collected the data and the second column is the class. The third column is the timestamp and the remaining columns are the \\(x\\), \\(y\\), and \\(z\\) accelerometer values, respectively. Figure 2.8: First 10 lines of raw accelerometer data. Usually, classification models are not trained with the raw data but with feature vectors extracted from the raw data. Feature vectors have the advantage of being more compact, thus, making the learning phase more efficient. For activity recognition, the feature extraction process consists of defining a moving window of size \\(w\\) that starts at position \\(i\\). At the beginning, \\(i\\) is the index pointing to the first accelerometer readings. Then, \\(n\\) statistical features are computed on the elements covered by the window such as mean, standard deviation, \\(0\\)-crossings, etc. This will produce a \\(n\\)-dimensional feature vector and the process is repeated by moving the window \\(s\\) steps forward. Typical values of \\(s\\) are such that the overlap between the previous window position and the next one is about \\(30\\%\\) to \\(50\\%\\). An overlap of \\(0\\) is also typical, that is, \\(s = w\\). Figure 2.9 depicts the process. Figure 2.9: Moving window for feature extraction. Once we have the set of feature vectors and their associated class labels, we can use them to train a classifier and make predictions on new data. For this example, we will use the file with features already extracted. The authors used windows of \\(10\\) seconds which is equivalent to \\(200\\) observations given the \\(20Hz\\) sampling rate and they used \\(0\\%\\) overlap. From each window, they extracted \\(43\\) features such as the mean, standard deviation, absolute deviations, etc. Let’s read and print the first rows of the dataset. The script for this section is smartphone_activities.R. The data frame has several columns, but we only print the first five features and the class which is stored in the last column. # Read data. df &lt;- read.csv(datapath,stringsAsFactors = F) # Some code to clean the dataset. # (cleaning code not shown here). # Print first rows of the dataset. head(df[,c(1:5,40)]) Table 2.2: First rows of activities dataset. X0 X1 X2 X3 X4 class 0.04 0.09 0.14 0.12 0.11 Jogging 0.12 0.12 0.06 0.07 0.11 Jogging 0.14 0.09 0.11 0.09 0.09 Jogging 0.06 0.10 0.09 0.09 0.11 Walking 0.12 0.11 0.10 0.08 0.10 Walking 0.09 0.09 0.10 0.12 0.08 Walking 0.12 0.12 0.12 0.13 0.15 Upstairs 0.10 0.10 0.10 0.10 0.11 Upstairs 0.08 0.07 0.08 0.08 0.05 Upstairs Our aim is to predict the class based on all the numeric features. We will use the rpart package (Therneau and Atkinson 2019) which implements classification and regression trees. We will assess the performance of the decision tree with \\(10\\)-fold cross-validation. We can use the sample() function to generate the folds. This function will sample \\(n\\) integers from \\(1\\) to \\(k\\) where \\(n\\) is the number of rows in the data frame. # Package with implementations of decision trees. library(rpart) # Set seed for reproducibility. set.seed(1234) # Define the number of folds. k &lt;- 10 # Generate folds. folds &lt;- sample(k, size = nrow(df), replace = TRUE) # Print first 10 values. head(folds) #&gt; [1] 10 6 5 9 5 6 The folds variable stores the fold each instance belongs to. For example, the first instance belongs to fold \\(1\\), the second instance belongs to fold \\(6\\), and so on. We can now generate our test and train sets. We will iterate \\(k=10\\) times. For each iteration \\(i\\), the test set is built using the instances that belong to fold \\(i\\) and the train set will be composed of the remaining instances (those that do not belong to fold \\(i\\)). Next, the rpart() function is used to train the decision tree with the train set. By default, rpart() performs \\(10\\)-fold cross-validation internally. To avoid this, we set the parameter xval = 0. Then, we can use the trained model to obtain the predictions on the test set with the generic predict() function. The ground truth classes and the predictions are stored so the performance metrics can be computed. # Variable to store ground truth classes. groundTruth &lt;- NULL # Variable to store the classifier&#39;s predictions. predictions &lt;- NULL for(i in 1:k){ trainSet &lt;- df[which(folds != i), ] testSet &lt;- df[which(folds == i), ] # Train the decision tree treeClassifier &lt;- rpart(class ~ ., trainSet, xval=0) # Get predictions on the test set. foldPredictions &lt;- predict(treeClassifier, testSet, type = &quot;class&quot;) predictions &lt;- c(predictions, as.character(foldPredictions)) groundTruth &lt;- c(groundTruth, as.character(testSet$class)) } Now, we use the confusionMatrix() function to compute the performance metrics and the confusion matrix. cm &lt;- confusionMatrix(as.factor(predictions), as.factor(groundTruth)) # Print accuracy cm$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.7895903 # Print performance metrics per class. cm$byClass[,c(&quot;Recall&quot;, &quot;Specificity&quot;, &quot;Precision&quot;, &quot;F1&quot;)] #&gt; Recall Specificity Precision F1 #&gt; Class: Downstairs 0.2821970 0.9617587 0.4434524 0.3449074 #&gt; Class: Jogging 0.9612308 0.9601898 0.9118506 0.9358898 #&gt; Class: Sitting 0.8366013 0.9984351 0.9696970 0.8982456 #&gt; Class: Standing 0.8983740 0.9932328 0.8632812 0.8804781 #&gt; Class: Upstairs 0.2246835 0.9669870 0.4733333 0.3047210 #&gt; Class: Walking 0.9360884 0.8198981 0.7642213 0.8414687 # Print overall metrics across classes. colMeans(cm$byClass[,c(&quot;Recall&quot;, &quot;Specificity&quot;, &quot;Precision&quot;, &quot;F1&quot;)]) #&gt; Recall Specificity Precision F1 #&gt; 0.6898625 0.9500836 0.7376393 0.7009518 Figure 2.10: Confusion matrix for activities’ predictions. The overall accuracy was \\(78\\%\\) and by looking at the individual performance metrics, some classes had low scores like ‘walking downstairs’ and ‘walking upstairs’. From the confusion matrix, it can be seen that those two activities were often confused with each other but also with the ‘walking’ activity. Package rpart.plot (Milborrow 2019) can be used to plot the resulting tree. library(rpart.plot) # Plot the tree from the last fold. rpart.plot(treeClassifier, fallen.leaves = F, shadow.col = &quot;gray&quot;, legend.y = 1) Figure 2.11: Resulting decision tree. The fallen.leaves = F argument prevents the leaves to be plotted at the bottom. This is useful if the tree has many nodes. Each node shows the predicted class, the predicted probability of each class and the percentage of observations in the node. The plot also shows the feature used for each split. We can see that the YABSOLDEV variable is at the root thus, it had the highest information gain with the initial set of instances. At the root of the tree, before looking at any of the features, the predicted class is ‘Walking’. This is because its prior probability is the highest one (\\(\\approx 0.39\\)), that is, it’s the most common activity present in the dataset. So, if we didn’t have any other information, our best bet would be to predict the most frequent activity. # Prior probabilities. table(trainSet$class) / nrow(trainSet) #&gt; Downstairs Jogging Sitting Standing Upstairs Walking #&gt; 0.09882885 0.29607561 0.05506472 0.04705157 0.11793713 0.38504212 These results look promising, but they can still be improved. In the next chapter, I will show you how to improve these results with Ensemble Learning which is a method that is used to aggregate many models. 2.4 Naive Bayes Naive Bayes is yet another type of classifier. This one is based on Bayes’ rule. The name Naive is because this method assumes that the features are independent. In the previous section we learned that decision trees are built recursively. Trees are built by first selecting a feature to be at the root and then, the root is split into subnodes and so on. How those subnodes are chosen depends on their parent node. With Naive Bayes, features don’t need information about other features, thus, the parameters for each feature can be learned in parallel. To demonstrate how Naive Bayes works I will use the SMARTPHONE ACTIVITIES dataset as in the previous section. For any given query instance, the aim is to predict its most likely class based on the accelerometer features. For a new query instance, we want to estimate its class based on the features that we have observed. Let’s say we want to know what is the probability that the query instance belongs to the class ‘Walking’. This can be formulated as follows: \\[\\begin{equation*} P(C=\\textit{Walking} | f_1,\\dots ,f_n). \\end{equation*}\\] This reads as the conditional probability that the class is ‘Walking’ given the observed evidence. For each instance, the evidence that we can observe are its features \\(f_1, \\dots ,f_n\\). In this dataset, each instance has \\(39\\) features. If we want to estimate what is the most likely class all we need to do is to compute the conditional probability for each class and return the highest one: \\[\\begin{equation} y = \\operatorname*{arg max}_{k \\in \\{1, \\dots ,K\\}} P(C_k | f_1,\\dots ,f_n) \\tag{2.10} \\end{equation}\\] where \\(K\\) is the total number of possible classes. The \\(\\text{arg max}\\) notation means: Evaluate the right hand expression for every class \\(k\\) and return the \\(k\\) that resulted with the maximum probability. If instead of \\(\\text{arg max}\\) we had \\(\\text{max}\\) (without the \\(\\text{arg}\\)) that would mean to return the actual maximum probability instead of the class \\(k\\). Now let’s see how we can compute \\(P(C_k | f_1,\\dots ,f_n)\\). To compute a conditional probability we can use Bayes’ rule: \\[\\begin{equation} P(H|E) = \\frac{P(H)P(E|H)}{P(E)} \\tag{2.11} \\end{equation}\\] Let’s dissect that formula: \\(P(H|E)\\) is called the posterior and it is the probability of the hypothesis \\(H\\) given the observed evidence \\(E\\). In our example, the hypothesis can be that \\(C=Walking\\) and the evidence consists of the measured features. This is the probability that ultimately we want to estimate for each class and pick the class with the highest probability. \\(P(H)\\) is called the prior. This is the probability of a hypothesis happening without having any evidence. In our example, this translates into the probability that an instance belongs to a particular class without looking at its features. In practice, this is estimated from the class counts in the training set. Suppose the training set consists of \\(100\\) instances and from those, \\(80\\) are of type ‘Walking’ and \\(20\\) are of type ‘Jogging’. Then, the prior probability for ‘Walking’ is \\(P(C=Walking)=80/100=0.8\\) and the prior for ‘Jogging’ is \\(P(C=Jogging)=20/100=0.2\\). \\(P(E)\\) is the probability of the evidence. Since this one doesn’t depend on the class we don’t need to compute it. This can be thought of as a normalization factor. When choosing the final class we only need to select the one with the highest score, so there is no need to normalize them into proper probabilities between \\(0\\) and \\(1\\). \\(P(E|H)\\) is called the likelihood. For numerical variables we can estimate this using a Gaussian probability density function. This sounds intimidating! but all we need to do is to compute the mean and standard deviation for each feature-class pair and plug them in the probability density function (pdf). The formula for a Gaussian (also called normal) pdf is: \\[\\begin{equation} f(x) = \\frac{1}{{\\sigma \\sqrt {2\\pi } }}e^{ - \\left( {x - \\mu } \\right)^2 / 2 \\sigma ^2 } \\tag{2.12} \\end{equation}\\] where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation. Suppose that for some feature \\(f1\\) and when the class is ‘Walking’, its mean is \\(5\\) and its standard deviation is \\(3\\). That is, we filter the train set and only select those instances with class ‘Walking’ and compute the mean and standard deviation for feature \\(f1\\). Figure 2.12 shows how its pdf looks like. Figure 2.12: Gaussian probability density function with mean 5 and standard deviation 3. If we have a query instance with a feature \\(f_1 = 1.7\\) we can compute its likelihood given the ‘Walking’ class \\(P(f_1=1.7|C=Walking)\\) with equation (2.12) by plugging \\(x=1.7\\), \\(\\mu=5\\) and \\(\\sigma=3\\). In R, the function dnorm() implements the normal pdf. dnorm(x=1.7, mean = 5, sd = 3) #&gt; [1] 0.07261739 Figure 2.13 shows the likelihood with a solid circle when \\(x=1.7\\). Figure 2.13: Likelihood (0.072) when x=1.7. If we have more than one feature we need to compute the likelihood for each and take their product: \\(P(f_1|C=Walking)*P(f_2|C=Walking)*\\dots*P(f_n|C=Walking)\\). Each feature and class pair has its own \\(\\mu\\) and \\(\\sigma\\) parameters. Thus, Naive Bayes requires to learn \\(K*F*2\\) parameters for the \\(P(E|H)\\) part plus \\(K\\) parameters for the priors \\(P(H)\\). \\(K\\) is the number of classes, \\(F\\) is the number of features and the \\(2\\) stands for the mean and standard deviation. We have seen how we can compute \\(P(C_k|f_1, \\dots ,f_n)\\) using Baye’s rule by calculating the prior \\(P(H)\\) and \\(P(E|H)\\) which is the product of the likelihoods for each feature. If we substitute Bayes’s rule (omitting the denominator) in equation (2.10) we get our Naive Bayes classifier: \\[\\begin{equation} y = \\operatorname*{arg max}_{k \\in \\{1, \\dots ,K\\}} P(C_k) \\prod_{i=1}^{F} P(f_i | C_k) \\tag{2.13} \\end{equation}\\] In the following section we will implement our own Naive Bayes algorithm in R and test it on the SMARTPHONE ACTIVITIES dataset. Then, we will compare our implementation with that of the well known e1071 package (Meyer et al. 2019). Naive Bayes works well with missing values since the features are independent. At prediction time, if an instance has one or more missing values then, those features are just ignored and the posterior probability is computed based only on the available variabels. Another advantage of the feature independence assumption is that feature selection algorithms run very fast with Naive Bayes. When building a predictive model, not all features may provide useful information and some features may even degrade the performance. Feature selection algorithms aim to find the best set of features and some of them need to try a huge number of feature combinations. With Naive Bayes, the parameters only need to be learned once and then different combinations of features can be evaluated by ommiting the ones that are not used. With decision trees, for example, we would need to build entire new trees every time we want to try different input features. Here, we have shown how we can use a Gaussian pdf to compute the likelihood \\(P(E|H)\\) when the features are numeric. This assumes that the features have a normal distribution however, this is not always the case. In practice, Naive Bayes can work really well even if that assumption is not met. Furthermore, nothing prevents us from using another distribution to estimate the likelihood or even defining a specific distribution for each feature. For categorical variables, \\(P(E|H)\\) is estimated using the feature values frequencies. 2.4.1 Activity Recognition with Naive Bayes naive_bayes.R It’s time to implement Naive Bayes. To keep it simple, first we will go through a step by step example using a single feature. Then, we will implement a function to train a Naive Bayes classifier for the case of multiple features. Let’s assume we have already split the data into train and test sets. The complete code is in the script naive_bayes.R. We will only use the feature RESULTANT which corresponds to the acceleration magnitude of the three axes of the accelerometer sensor. The following code snippet prints the first rows of the train set. The RESULTANT feature is in column \\(39\\) and the class is the last column (\\(40\\)). head(trainset[,c(39:40)]) #&gt; RESULTANT class #&gt; 1004 11.14 Walking #&gt; 623 1.24 Upstairs #&gt; 2693 9.90 Standing #&gt; 934 10.44 Upstairs #&gt; 4496 10.43 Walking #&gt; 2948 15.28 Jogging First, we compute the prior probabilities for each class in the train set and store them in the variable priors. This corresponds to the \\(P(C_k)\\) part in equation (2.13). # Compute prior probabilities. priors &lt;- table(trainset$class) / nrow(trainset) # Print the table of priors. priors #&gt; Downstairs Jogging Sitting Standing Upstairs #&gt; 0.09622990 0.30266280 0.05721065 0.04640127 0.11521223 #&gt; Walking #&gt; 0.38228315 We can access each prior by name like this: # Get the prior for &quot;Jogging&quot;. priors[&quot;Jogging&quot;] #&gt; Jogging #&gt; 0.3026628 This means that \\(30\\%\\) of the instances in the train set are of type ‘Jogging’. Now we need to compute the \\(P(f_i|C_k)\\) part from equation (2.13). We can define a method to compute the probability density function in equation (2.12): # Probability density function of normal distribution. f &lt;- function(x, m, s){ (1 / (sqrt(2*pi)*s)) * exp(-((x-m)^2) / (2 * s^2)) } It’s first argument x is the input value. The second argument m is the mean and the last argument s is the standard deviation. For illustration purposes we are defining this function manually but remember that this pdf is already implemented with the base dnorm() function. According to equation (2.13) we need to compute \\(P(f_i|C_k)\\) for each feature \\(i\\) and class \\(k\\). Let’s assume there are only two classes, ‘Walking’ and ‘Jogging’ thus, we need the mean and standard deviation for each and for the feature RESULTANT (column \\(39\\)). # Compute the mean and sd of # the feature RESULTANT (column 39) # when the class = &quot;Standing&quot;. mean.standing &lt;- mean(trainset[which(trainset$class == &quot;Standing&quot;), 39]) sd.standing &lt;- sd(trainset[which(trainset$class == &quot;Standing&quot;), 39]) # Compute mean and sd when # the class = &quot;Jogging&quot;. mean.jogging &lt;- mean(trainset[which(trainset$class == &quot;Jogging&quot;), 39]) sd.jogging &lt;- sd(trainset[which(trainset$class == &quot;Jogging&quot;), 39]) We can print the means: mean.standing #&gt; [1] 9.405795 mean.jogging #&gt; [1] 13.70145 We can see that the mean value for ‘Jogging’ is higher for this feature. This was expected since this feature captures the overall movement across all axes. Now we have everything we need to start making predictions on new instances. We have the priors and we have the means and standard deviations for each feature-class pair. Let’s select the first instance from the test set and try to predict its class. # Select a query instance from the test set. query &lt;- testset[1,] # Select the first one. Now we compute the posterior probability for each class using the learned means and standard deviations: # Compute P(Standing)P(RESULTANT|Standing) priors[&quot;Standing&quot;] * f(query$RESULTANT, mean.standing, sd.standing) #&gt; 0.003169748 # Compute P(Jogging)P(RESULTANT|Jogging) priors[&quot;Jogging&quot;] * f(query$RESULTANT, mean.jogging, sd.jogging) #&gt; 0.03884481 The posterior for ‘Jogging’ was higher (\\(0.038\\)) so we classify the query instance as ‘Jogging’. If we check the true class we see that it was correctly classified! # Inspect the true class of the query instance. query$class #&gt; [1] &quot;Jogging&quot; In this example we assumed that there was only one feature and we computed each step manually. However, this can easily be extended to deal with more features. So let’s just do that. We can write two functions, one for training the classifier and the other one for making predictions. The following function will be used to train the classifier. It takes as input a data frame with \\(n\\) features. This function assumes that the class is the last column. The function returns a list with the learned priors, means, and standard deviations. # Function to learn the parameters of # a Naive Bayes classifier. # Assumes that the last column of data is the class. naive.bayes.train &lt;- function(data){ # Unique classes. classes &lt;- unique(data$class) # Number of features. nfeatures &lt;- ncol(data) - 1 # List to store the learned means and sds. list.means.sds &lt;- list() for(c in classes){ # Matrix to store the mean and sd for each feature. # First column stores the mean and second column # stores the sd. M &lt;- matrix(0, nrow = nfeatures, ncol = 2) # Populate matrix. for(i in 1:nfeatures){ feature.values &lt;- data[which(data$class == c),i] M[i,1] &lt;- mean(feature.values) M[i,2] &lt;- sd(feature.values) } list.means.sds[c] &lt;- list(M) } # Compute prior probabilities. priors &lt;- table(data$class) / nrow(data) return(list(list.means.sds=list.means.sds, priors=priors)) } The function iterates through each class and for each, it creates a matrix M with \\(F\\) rows and \\(2\\) columns where \\(F\\) is the number of features. The first column stores the means and the second the standard deviations. Those matrices are saved in a list indexed by the class name so at prediction time we can retrieve each matrix individually. At the end, the prior probabilities are computed. Finally, a list is returned. The first element of the list is the list of matrices and the second element are the priors. The next function will make predictions based on the learned parameters. Its first argument is the learned parameters and the second parameter is a data frame with the instances we want to make predictions for. # Function to make predictions using # the learned parameters. naive.bayes.predict &lt;- function(params, data){ # Variable to store the prediction for each instance. predictions &lt;- NULL n &lt;- nrow(data) # Get class names. classes &lt;- names(params$priors) # Get number of features. nfeatures &lt;- nrow(params$list.means.sds[[1]]) # Iterate instances. for(i in 1:n){ query &lt;- data[i,] max.probability &lt;- -Inf predicted.class &lt;- &quot;&quot; # Find the class with highest probability. for(c in classes){ # Get the prior probability for class c. acum.prob &lt;- params$priors[c] # Iterate features. for(j in 1:nfeatures){ # Compute P(feature|class) tmp &lt;- f(query[,j], params$list.means.sds[[c]][j,1], params$list.means.sds[[c]][j,2]) # Accumulate result. acum.prob &lt;- acum.prob * tmp } if(acum.prob &gt; max.probability){ max.probability &lt;- acum.prob predicted.class &lt;- c } } predictions &lt;- c(predictions, predicted.class) } return(predictions) } This function iterates through each instance and computes the posterior for each class and stores the one that achieved the highest value as the prediction. Finally, it returns the list with all predictions. We are ready to train our Naive Bayes classifier. All we need to do is call the function naive.bayes.train() and pass the train set. # Learn Naive Bayes parameters. nb.model &lt;- naive.bayes.train(trainset) The learned parameters are stored in nb.model and we can make predictions with the naive.bayes.predict() function by passing the nb.model and a test set. # Make predictions. predictions &lt;- naive.bayes.predict(nb.model, testset) Then, we assess the performance of the model by computing the confusion matrix. # Compute confusion matrix and other performance metrics. groundTruth &lt;- testset$class cm &lt;- confusionMatrix(as.factor(predictions), as.factor(groundTruth)) # Print accuracy cm$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.7501538 # Print overall metrics across classes. colMeans(cm$byClass[,c(&quot;Recall&quot;, &quot;Specificity&quot;, &quot;Precision&quot;, &quot;F1&quot;)]) #&gt; Recall Specificity Precision F1 #&gt; 0.6621381 0.9423729 0.6468372 0.6433231 The accuracy was \\(75\\%\\). In the previous section we obtained an accuracy of \\(78\\%\\) with decision trees. However, this does not necessarily mean that decision trees are better. Moreover, in the previous section we used cross-validation and here we used hold-out validation. One important thing to note is that computing the posterior may cause a loss of numeric precision specially, when there are many features. This is because we are multiplying the likelihoods for each feature (see equation (2.13) and those likelihoods are small numbers. One way to fix that is to use logarithms. In navie.bayes.predict() we can change acum.prob &lt;- params$priors[c] with acum.prob &lt;- log(params$priors[c]) and acum.prob &lt;- acum.prob * tmp with acum.prob &lt;- acum.prob + log(tmp). If you try those changes you should get the same result as before. There is already a popular R package (e1071) for training Naive Bayes classifiers. The following code trains a classifier using this package. #### Use Naive Bayes implementation from package e1071 #### library(e1071) # We need to convert the class into a factor. trainset$class &lt;- as.factor(trainset$class) nb.model2 &lt;- naiveBayes(class ~., trainset) predictions2 &lt;- predict(nb.model2, testset) cm2 &lt;- confusionMatrix(as.factor(predictions2), as.factor(groundTruth)) # Print accuracy cm2$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.7501538 As you can see, the result was the same as the one obtained with our implementation! We implemented our own for illustrative purposes but it is advisable to use already tested and proven packages. Furthermore, this one also supports categorical variables. 2.5 Dynamic Time Warping dtw_example.R In the previous activity recognition example, we used the extracted features represented as feature vectors to train the classifiers instead of using the raw data. This can lead to temporal information loss. In the previous example, we could classify the activities with reasonable accuracy since the extracted features were able to retain enough information from the raw data. However, in some cases, the need to retain temporal information is crucial. For example, in hand signature recognition, one needs to check if a query signature matches one of the signatures from a database. The signatures need to have an almost exact match to authenticate a user, i.e, they need to look very similar. If we represent each signature as a feature vector, it can turn out that two signatures have very similar feature vectors even though they look completely different. For example, Figure 2.14 shows four datasets. They look very different but they all have the same correlation of \\(0.816\\)5. Figure 2.14: Four datasets with the same correlation of 0.816. To avoid this potential issue, we can also include time-dependent information into our models by keeping the order of the data points. Another issue is that two time series that belong to the same class will still have some differences. Every time the same person signs a document the signature will vary a bit. In the same way, when we pronounce a word, sometimes we emphasize some letters or speak at different speeds. Figure 2.15 shows two versions of the sentence “very good”. In the second one (bottom) the speaker emphasizes the “e” and as a result, the two sentences are not aligned in time anymore but they have the same meaning. Figure 2.15: Time shift example between two sentences. If we wanted to compare two sequences we could use the well known Euclidean distance. However since the two sequences may not be aligned in time, the result could be misleading. To account for this “time-shift” effect in time series data we can use Dynamic Time Warping (DTW) (Sakoe et al. 1990) when comparing two time series. DTW is a method that: Finds an optimal match between two given time-dependent sequences. Computes the dissimilarity between the sequences. Finds the optimal deformation of one of the sequences onto the other. Another advantage of DTW is that the time series do not need to be of the same length. Suppose we have two time series, a query, and a reference we want to compare with: \\[\\begin{align*} query&amp;=(2,2,2,4,4,3)\\\\ ref&amp;=(2,2,3,3,2) \\end{align*}\\] The first thing to note is that both sequences differ in length. Figure 2.16 shows a plot with the two sequences. The query is the solid line. It can be seen that the query seems to be shifted to the right one position with respect to the reference. The plot also shows the resulting alignment after applying the DTW algorithm (dashed lines between the sequences). The resulting distance between the sequences is \\(3\\). In the following, we will see how the problem can be formalized and the steps to compute that final distance of \\(3\\). Don’t worry if you find the math notation a bit difficult to grasp at this pint. A step by step example will follow which should help to explain how the method works. Figure 2.16: DTW alignment between the query and reference sequences (Solid line is the query). The problem of aligning two sequences can be formalized as follows (Rabiner and Juang 1993). Let \\(X\\) and \\(Y\\) be two sequences: \\[\\begin{align*} X&amp;=(x_1,x_2,\\dots,x_{T_x}) \\\\ Y&amp;=(y_1,y_2,\\dots,y_{T_y}) \\end{align*}\\] where \\(x_i\\) and \\(y_i\\) are vectors. In the previous example, the vectors only have one element since the sequences are \\(1\\)-dimensional but DTW also works with multidimensional sequences. \\(T_x\\) and \\(T_y\\) are the sequences’ lengths. Let \\[\\begin{align*} d(i_x,i_y) \\end{align*}\\] be the dissimilarity (distance) between vectors \\(x_i\\) and \\(y_i\\) (e.g., Euclidean distance). Then, \\(\\phi_x\\) and \\(\\phi_y\\) are the warping functions that relate \\(i_x\\) and \\(i_y\\) to a common axis \\(k\\): \\[\\begin{align*} i_x&amp;=\\phi_x (k), k=1,2,\\dots,T \\\\ i_y&amp;=\\phi_y (k), k=1,2,\\dots,T. \\end{align*}\\] The total dissimilarity between the two sequences is: \\[\\begin{equation} d_\\phi (X,Y) = \\sum_{k=1}^T{d\\left(\\phi_x (k), \\phi_y (k)\\right)} \\tag{2.14} \\end{equation}\\] The aim is to find the warping function \\(\\phi\\) that minimizes the total dissimilarity: \\[\\begin{equation} \\operatorname*{min}_{\\phi} d_\\phi (X,Y) \\tag{2.15} \\end{equation}\\] The solution can be efficiently computed using dynamic programming. Usually, when solving this minimization problem, some constraints are applied: Endpoint constraints. This constraint makes sure that the first and last elements of each sequence are connected. \\[\\begin{align*} \\phi_x (1)&amp;=1, \\phi_y (1)=1 \\\\ \\phi_x (T)&amp;=T_x, \\phi_y (T)=T_y \\end{align*}\\] Monotonicity. This constraint allows ‘time to flow’ only from left to right. That is, we can not go back in time. \\[\\begin{align*} \\phi_x (k+1) \\geq \\phi_x(k) \\\\ \\phi_y (k+1) \\geq \\phi_y(k) \\end{align*}\\] Local constraints. For example, allow jumps of at most \\(1\\) step. \\[\\begin{align*} \\phi_x (k+1) - \\phi_x(k) \\leq 1 \\\\ \\phi_y (k+1) - \\phi_y(k) \\leq 1 \\end{align*}\\] Also, it is possible to apply global constraints, other local constraints, and apply different weights to slopes but the three described above are the most common ones. For a comprehensive list of constraints, please see (Rabiner and Juang 1993). Now let’s get back to our example and go through the steps to compute the dissimilarity and warping functions between our query (\\(Q\\)) and reference (\\(R\\)) sequences: \\[\\begin{align*} Q&amp;=(2,2,2,4,4,3) \\\\ R&amp;=(2,2,3,3,2) \\end{align*}\\] The first step is to compute a local cost matrix. This is just a matrix that contains the distance between every pair of points between the two sequences. For this example, we will use the Manhattan distance. Since our sequences are \\(1\\)-dimensional this distance can be computed as the absolute difference \\(|x_i - y_i|\\). Figure 2.17 shows the resulting local cost matrix. Figure 2.17: Local cost matrix between Q and R. For example, position \\((1,1)=0\\) (row,column) because the first element of \\(Q\\) is \\(2\\) and the first element of \\(R\\) is also \\(2\\), thus, \\(|2-2|=0\\). The rest of the matrix is filled in the same way. In dynamic programming, partial results are computed and stored in a table. Figure 2.18 shows the final dynamic programming table computed from the local cost matrix. Initially, this table is empty. We start to fill it from bottom left at position \\((1,1)\\). From the local cost matrix, the cost at position \\((1,1)\\) is \\(0\\) so the cost at that position in the dynamic programming table is \\(0\\). Then we can start filling in the contiguous cells. The only direction from which we can arrive at position \\((1,2)\\) is from the west. The cost at position \\((1,2)\\) from the local cost matrix is \\(0\\) and the cost of the minimum of the cell from the west \\((1,1)\\) is also \\(0\\). So \\(W:0+0=0\\). For each cell we add the current cost plus the minimum cost when coming from the contiguous cell. The minimum costs are marked with red. For some cells it is possible to arrive from three different directions: S, W, and SW, thus we need to compute the cost when coming from each of those. The final minimum cost at position \\((5,6)\\) is \\(3\\). Thus, that is the global DTW distance. In the example, it is possible to get the minimum at \\((5,6)\\) when arriving from the south or southwest. Figure 2.18: Dynamic programming table. Once the table is filled in, we can backtrack starting at \\((5,6)\\) to find the warping functions. Figure 2.19 shows the final warping functions. Because of the endpoint constraints we know that \\(\\phi_Q(1)=1, \\phi_R(1)=1\\) and \\(\\phi_Q(6)=6, \\phi_R(6)=5\\). Then, from \\((5,6)\\) the minimum contiguous value is \\(2\\) coming from SW, thus \\(\\phi_Q(5)=5, \\phi_R(5)=4\\) and so on. Note that we could also had chosen to arrive from the south with the same minimum value of \\(2\\) but still this would have resulted in the same overall distance. The dashed line in figure 2.18 shows the full backtracking. Figure 2.19: Resulting warping functions. The runtime complexity of DTW is \\(O(T_x T_y)\\) which is the required time to compute the local cost matrix and the dynamic programming table. In R, the dtw package (Giorgino 2009) has the function dtw() to compute the DTW distance between two sequences. Let’s use this package to solve the previous example. library(&quot;dtw&quot;) # Sequences from the example query &lt;- c(2,2,2,4,4,3) ref &lt;- c(2,2,3,3,2) # Find dtw distance. alignment &lt;- dtw(query, ref, step = symmetric1, keep.internals = T) The keep.internals = T keeps the input data so it can be accessed later, e.g., for plotting. The cost matrix and final distance can be accessed from the resulting object. The step argument specifies a step pattern. A step pattern describes some of the algorithm constraints such as endpoint and local constraints. In this case, we use symmetric1 which applies the constraints explained before. We can access the cost matrix and the final distance \\(\\phi_x\\) and \\(\\phi_y\\) as follows: alignment$localCostMatrix #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 0 0 1 1 0 #&gt; [2,] 0 0 1 1 0 #&gt; [3,] 0 0 1 1 0 #&gt; [4,] 2 2 1 1 2 #&gt; [5,] 2 2 1 1 2 #&gt; [6,] 1 1 0 0 1 alignment$distance #&gt; [1] 3 alignment$index1 #&gt; [1] 1 2 3 4 5 6 alignment$index2 #&gt; [1] 1 1 2 3 4 5 The local cost matrix is the same one as in Figure 2.17 but in rotated form. The resulting object also has the dynamic programming table which can be plotted along with the resulting backtracking. ccm &lt;- alignment$costMatrix image(x = 1:nrow(ccm), y = 1:ncol(ccm), ccm, xlab = &quot;Q&quot;, ylab = &quot;R&quot;) text(row(ccm), col(ccm), label = ccm) lines(alignment$index1, alignment$index2) Figure 2.20: Dynamic programming table and backtracking. And finally, the aligned sequences can be plotted. The previous Figure 2.16 shows the result of the following command. plot(alignment, type=&quot;two&quot;, off=1.5, match.lty=2, match.indices=10, main=&quot;DTW resulting alignment&quot;, xlab=&quot;time&quot;, ylab=&quot;magnitude&quot;) 2.5.1 Hand Gesture Recognition hand_gestures.R, hand_gestures_auxiliary.R Gestures are a form of communication. They are often accompanied with speech but can also be used to communicate something independently of speech (like in sign language). Gestures allow us to externalize and emphasize emotions and thoughts. They are based on body movements from arms, hands, fingers, face, head, etc. Gestures can be used as a non-verbal way to identify and study behaviors for different purposes such as for emotion (De Gelder 2006) or for the identification of developmental disorders like autism (Anzulewicz, Sobota, and Delafield-Butt 2016). Gestures can also be used to develop user-computer interaction applications. The following video shows an example application of gesture recognition for domotics. The application determines the indoor location using \\(k\\)-nn as it was shown in this chapter. The gestures are classified using DTW (I’ll show how to do it in a moment). Based on the location and type of gesture, an specific home appliance is activated. I programmed that app some time ago using the same algorithms presented here. To demonstrate how DTW can be used for hand gesture recognition, we will examine the HAND GESTURES dataset that was collected with a Smartphone using its accelerometer sensor. The data was collected by \\(10\\) individuals who performed \\(5\\) repetitions of \\(10\\) different gestures (‘triangle’, ‘square’, ‘circle’, ‘a’, ‘b’, ‘c’, ‘1’, ‘2’, ‘3’, ‘4’). The sensor is a tri-axial accelerometer that returns values for the \\(x\\), \\(y\\), and \\(z\\) axes. The participants were not instructed to hold the smartphone in any particular way. The sampling rate was set at \\(50\\) Hz. To record a gesture, the user presses the phone’s screen with her/his thumb, performs the gesture in the air, and stops pressing the screen after the gesture is complete. Figure 2.21 shows the start and end positions of the \\(10\\) gestures. Figure 2.21: Paths for the 10 considered gestures. In order to make the recognition orientation-independent, we can compute the magnitude of the \\(3\\) accelerometer axes. This will provide us with the overall movement patterns regardless of orientation. \\[\\begin{equation} Magnitude(t) = \\sqrt {{a_x}{{(t)}^2} + {a_y}{{(t)}^2} + {a_z}{{(t)}^2}} \\tag{2.16} \\end{equation}\\] where \\({a_x}{{(t)}}\\), \\({a_y}{{(t)}}\\) and \\({a_z}{{(t)}}\\) are the accelerations at time \\(t\\). Figure 2.22 shows the raw accelerometer values (dashed lines) for a triangle gesture. The solid line shows the resulting magnitude. This will also simplify things since we will now work with \\(1\\)-dimensional sequences by just using the magnitude instead of the other \\(3\\) axes. Figure 2.22: Triangle gesture. The gestures are stored in text files that contain the \\(x\\), \\(y\\), and \\(z\\) recordings. The script hand_gestures_auxiliary.R has some auxiliary functions to preprocess the data. Since the sequences of each gesture are of varying length, storing them as a data frame could be problematic since data frames have fixed sizes. Instead, the gen.instances() function processes the files and returns all hand gestures as a list. This function also computes the magnitude (equation (2.16)). The following code (from hand_gestures.R) calls the gen.instances() function and stores the results in the instances variable which is a list. Then, we select the first and second instances to be the query and the reference. # Format instances from files. instances &lt;- gen.instances(&quot;../data/hand_gestures/&quot;) # Use first instance as the query. query &lt;- instances[[1]] # Use second instance as the reference. ref &lt;- instances[[2]] Each element in instances is also a list that stores the type and values (magnitude) of each gesture. # Print their respective classes print(query$type) #&gt; [1] &quot;1&quot; print(ref$type) #&gt; [1] &quot;1&quot; Here, the first two instances are of type ‘1’. We can also print the magnitude values. # Print values. print(query$values) #&gt; [1] 9.167477 9.291464 9.729926 9.901090 .... In this case, both classes are “1”. We can use the dtw() function to compute the similarity between the query and the reference instance and plot the resulting alignment. alignment &lt;- dtw(query$values, ref$values, keep = TRUE) # Print similarity (distance) alignment$distance #&gt; [1] 68.56493 # Plot result. plot(alignment, type=&quot;two&quot;, off=1, match.lty=2, match.indices=40, main=&quot;DTW resulting alignment&quot;, xlab=&quot;time&quot;, ylab=&quot;magnitude&quot;) Figure 2.23: Resulting alignment. To perform the actual classification, we will use our well-known \\(k\\)-nn classifier with \\(k=1\\). To classify a query instance, we need to compute the DTW distance from it to every other instance in the training set and predict the label from the closest one. We will test the performance using \\(10\\)-fold cross-validation. Since computing all DTW distances takes some time, we can precompute all pairs of distances and store them in a matrix. The auxiliary function does just that matrix.distances(). Since this can take some minutes, the results are saved so we don’t need to wait again. D &lt;- matrix.distances(instances) # Save results. save(D, file=&quot;D.RData&quot;) The matrix.distances() returns a list. The first element is an array with the gestures’ classes and the second element is the actual distance matrix. The elements in the diagonal are set to Inf to signal that we don’t want to take into account the dissimilarity between a gesture with itself. For convenience, this matrix is already stored in the file D.RData located this chapter’s code directory. The following code performs the \\(10\\)-fold cross-validation and computes the performance results. # Load the DTW distances matrix. load(&quot;D.RData&quot;) set.seed(1234) k &lt;- 10 # Number of folds. folds &lt;- sample(k, size = length(D[[1]]), replace = T) predictions &lt;- NULL groundTruth &lt;- NULL # Implement k-nn with k=1. for(i in 1:k){ trainSet &lt;- which(folds != i) testSet &lt;- which(folds == i) train.labels &lt;- D[[1]][trainSet] for(query in testSet){ type &lt;- D[[1]][query] distances &lt;- D[[2]][query, ][trainSet] # Return the closest one. nn &lt;- sort(distances, index.return = T)$ix[1] pred &lt;- train.labels[nn] predictions &lt;- c(predictions, pred) groundTruth &lt;- c(groundTruth, type) } } # end of for The line distances &lt;- D[[2]][query, ][trainSet] retrieves the pre-computed distances between the test query and all gestures in the train set. Then, those distances are sorted in ascending order and the class from the closest one is used as the prediction. Finally, we can compute the performance. cm &lt;- confusionMatrix(factor(predictions), factor(groundTruth)) # Compute performance metrics per class. cm$byClass[,c(&quot;Recall&quot;, &quot;Specificity&quot;, &quot;Precision&quot;, &quot;F1&quot;)] #&gt; Recall Specificity Precision F1 #&gt; Class: 1 0.84 0.9911111 0.9130435 0.8750000 #&gt; Class: 2 0.84 0.9866667 0.8750000 0.8571429 #&gt; Class: 3 0.96 0.9911111 0.9230769 0.9411765 #&gt; Class: 4 0.98 0.9933333 0.9423077 0.9607843 #&gt; Class: a 0.78 0.9733333 0.7647059 0.7722772 #&gt; Class: b 0.76 0.9955556 0.9500000 0.8444444 #&gt; Class: c 0.90 1.0000000 1.0000000 0.9473684 #&gt; Class: circleLeft 0.78 0.9622222 0.6964286 0.7358491 #&gt; Class: square 1.00 0.9977778 0.9803922 0.9900990 #&gt; Class: triangle 0.92 0.9711111 0.7796610 0.8440367 # Overall performance metrics colMeans(cm$byClass[,c(&quot;Recall&quot;, &quot;Specificity&quot;, &quot;Precision&quot;, &quot;F1&quot;)]) #&gt; Recall Specificity Precision F1 #&gt; 0.8760000 0.9862222 0.8824616 0.8768178 Figure 2.24: Confusion matrix for hand gestures’ predictions. The overall recall was \\(0.87\\) which is not bad. From the confusion matrix, we can see that the class ‘a’ was often confused with ‘circleLeft’ and vice versa. This makes sense since both have similar motions (see Figure 2.21). Also, ‘b’ was often confused with ‘circleLeft’. The ‘square’ class was always correctly classified. This example demonstrated how DTW can be used with \\(k\\)-nn to recognize hand gestures. 2.6 Summary This chapter focused on classification models. Classifiers predict a category based on the input features. Here, we showed how classifiers can be used to detect indoor locations, classify activities and had gestures. \\(k\\)-nearest neighbors (\\(k\\)-nn) predicts the class of a test point as the majority class of the \\(k\\) nearest neighbors. Some classification performance metrics are recall, specificity, precision, accuracy, F1-score, etc. Decision trees are easy-to-interpret classifiers trained recursively based on feature importance (for example, purity). Naive Bayes is a type of classifier where features are assumed to be independent. Dynamic Time Warping (DTW) computes the similarity between two timeseries after ‘aligning’ them in time. This can be used for classification for example, in combination with \\(k\\)-nn. References "],
["ensemble.html", "Chapter 3 Predicting Behavior with Ensemble Learning 3.1 Bagging 3.2 Random Forest 3.3 Stacked Generalization 3.4 Multi-view Stacking for Home Tasks Recognition 3.5 Summary", " Chapter 3 Predicting Behavior with Ensemble Learning In the previous chapters, we have been building single models, either for classification or regression. With ensemble learning, the idea is to train several models and combine their results to increase the performance. Usually, ensemble methods outperform single models. In the context of ensemble learning, the individual models whose results are to be combined are known as base learners. Base learners can be of the same type (homogeneous) or of different types (heterogeneous). Examples of ensemble methods are Bagging, Random Forest, and Stacked Generalization. In the following sections, the three of them will be described and example applications in behavior analysis will be presented as well. 3.1 Bagging Bagging stands for “bootstrap aggregating” and is an ensemble learning method proposed by Breiman (1996). Ummm…, Bootstrap, aggregating? Let’s start with the aggregating part. As the name implies, this method is based on training several base learners (e.g., decision trees) and combining their outputs to produce a single final prediction. One way to combine the results is by taking the majority vote for classification tasks or the average for regression. In an ideal case, we would have enough data to train each base learner with an independent train set. However, in practice we may only have a single train set of limited size. Training several base learners with the same train set is equivalent to having a single learner, provided that the training procedure of the base learners is deterministic. Even if the training procedure is not deterministic, the resulting models could be very similar. What we would like to have is accurate base learners but at the same time they should be different. Then, how can we train those base learners? Well, this is where the bootstrap part comes into play. Bootstrapping means generating new train sets by sampling instances with replacement from the original train set. If the original train set has \\(N\\) instances, the method selects \\(N\\) instances at random to produce a new train set. With replacement means that repeated instances are allowed. This has the effect of generating a new train set of size \\(N\\) by removing some instances and duplicating other instances. By using this method, \\(n\\) different train sets can be generated and used to train \\(n\\) different learners. It has been shown that having more diverse base learners increases performance. One way of generating diverse learners is by using different train sets as just described. In his original work, Breiman (1996) used decision trees as base learners. Decision trees are considered to be very unstable. This means that small changes in the train set produce very different trees–but this is a good thing for bagging! Most of the time, the aggregated predictions will produce better results than the best individual learner from the ensemble. Figure 3.1 shows bootstrapping in action. The train set is sampled with replacement \\(3\\) times. The numbers represent indices to arbitrary train instances. Here, we can see that in the first sample, the instance number \\(5\\) is missing but instead, instance \\(2\\) is duplicated. All samples have five elements. Then, each sample is used to train individual decision trees. Figure 3.1: Bagging example. One of the disadvantages of ensemble methods is their higher computational time both during training and inference. Another disadvantage of ensemble methods is that they are more difficult to interpret. Still, there exist model agnostic interpretability methods (Molnar 2019) that can help to analyze the results. In the next section, I will show you how to implement your own Bagging model with decision trees in R. 3.1.1 Activity recognition with Bagging bagging_activities.R iterated_bagging_activities.R In this section, we will implement Bagging with decision trees. Then, we will test our implementation on the SMARTPHONE ACTIVITIES dataset. The following code snippet shows the implementation of my_bagging() function. The complete code is in the script bagging_activities.R. The function accepts three arguments. The first one is the formula, the second one is the train set, and the third argument is the number of base learners (\\(10\\) by default). Here, we will use the rpart package to train the decision trees. # Define our bagging classifier. my_bagging &lt;- function(theFormula, data, ntrees = 10){ N &lt;- nrow(data) # A list to store the individual trees models &lt;- list() # Train individual trees and add each to &#39;models&#39; list. for(i in 1:ntrees){ # Bootstrap instances from data. idxs &lt;- sample(1:N, size = N, replace = T) bootstrappedInstances &lt;- data[idxs,] treeModel &lt;- rpart(as.formula(theFormula), bootstrappedInstances, xval = 0, cp = 0) models &lt;- c(models, list(treeModel)) } res &lt;- structure(list(models = models), class = &quot;my_bagging&quot;) return(res) } First, a list that will store each individual learner is defined models &lt;- list(). Then, the function iterates ntrees times. In each iteration, a bootstrapped train set is generated and used to train a rpart model. The xval = 0 parameter tells rpart not to perform cross-validation internally. The cp parameter is also set to \\(0\\). This value controls the amount of pruning. The default is \\(0.01\\) leading to smaller trees. This makes the trees to be more similar but since we want diversity we are setting this to \\(0\\) so bigger trees are generated and as a consequence, more diverse. Finally, an object of class \"my_bagging\" is returned. This is just a list containing the trained base learners. The class = \"my_bagging\" argument is important. This tells R that this object is of type my_bagging. Setting the class will allow us to use the generic predict() function and R will automatically call the corresponding predict.my_bagging() function which we will shortly define. The class name and the function name after predict. need to be the same. # Define the predict function for my_bagging. predict.my_bagging &lt;- function(object, newdata){ ntrees &lt;- length(object$models) N &lt;- nrow(newdata) # Matrix to store predictions for each instance # in newdata and for each tree. M &lt;- matrix(data = rep(&quot;&quot;,N * ntrees), nrow = N) # Populate matrix. # Each column of M contains all predictions for a given tree. # Each row contains the predictions for a given instance. for(i in 1:ntrees){ m &lt;- object$models[[i]] tmp &lt;- as.character(predict(m, newdata, type = &quot;class&quot;)) M[,i] &lt;- tmp } # Final predictions predictions &lt;- character() # Iterate through each row of M. for(i in 1:N){ # Compute class counts classCounts &lt;- table(M[i,]) # Get the class with the most counts. predictions &lt;- c(predictions, names(classCounts)[which.max(classCounts)]) } return(predictions) } Now let’s dissect the predict.my_bagging() function. First, note that the function name starts with predict. followed by the type of object. Following this convention will allow us to call predict() and R will call the corresponding method based on the class of the object. The first argument object is an object of type “my_bagging” as returned by my_bagging(). The second argument newdata is the test set we want to generate predictions for. A matrix M that will store the predictions for each tree is defined. This matrix has \\(N\\) rows and \\(ntrees\\) columns where \\(N\\) is the number of instances in newdata and \\(ntrees\\) is the number of trees. Thus, each column stores the predictions for each of the base learners. This function iterates through each base learner (rpart in this case), and makes a prediction for each instance in newdata. Then, the results are stored in matrix M. Finally, it iterates through each instance and computes the most common predicted class from the base learners. Let’s test our Bagging function! We will test it with the activity recognition dataset introduced in section 2.3.1 and set the number of trees to \\(10\\). The following code shows how we would use our bagging functions to train the model and make predictions on a test set. baggingClassifier &lt;- my_bagging(class ~ ., trainSet, ntree = 10) predictions &lt;- predict(baggingClassifier, testSet) The following will perform \\(5\\)-fold cross-validation and print the results. set.seed(1234) k &lt;- 5 folds &lt;- sample(k, size = nrow(df), replace = TRUE) # Variable to store ground truth classes. groundTruth &lt;- NULL # Variable to store the classifier&#39;s predictions. predictions &lt;- NULL for(i in 1:k){ trainSet &lt;- df[which(folds != i), ] testSet &lt;- df[which(folds == i), ] treeClassifier &lt;- my_bagging(class ~ ., trainSet, ntree = 10) foldPredictions &lt;- predict(treeClassifier, testSet) predictions &lt;- c(predictions, as.character(foldPredictions)) groundTruth &lt;- c(groundTruth, as.character(testSet$class)) } cm &lt;- confusionMatrix(as.factor(predictions), as.factor(groundTruth)) # Print accuracy cm$overall[&quot;Accuracy&quot;] #&gt; Accuracy #&gt; 0.861388 # Print other metrics per class. cm$byClass[,c(&quot;Recall&quot;, &quot;Specificity&quot;, &quot;Precision&quot;, &quot;F1&quot;)] #&gt; Recall Specificity Precision F1 #&gt; Class: Downstairs 0.5378788 0.9588957 0.5855670 0.5607108 #&gt; Class: Jogging 0.9618462 0.9820722 0.9583078 0.9600737 #&gt; Class: Sitting 0.9607843 0.9982394 0.9702970 0.9655172 #&gt; Class: Standing 0.9146341 0.9988399 0.9740260 0.9433962 #&gt; Class: Upstairs 0.5664557 0.9563310 0.6313933 0.5971643 #&gt; Class: Walking 0.9336857 0.9226850 0.8827806 0.9075199 # Print average performance metrics across classes. colMeans(cm$byClass[,c(&quot;Recall&quot;, &quot;Specificity&quot;, &quot;Precision&quot;, &quot;F1&quot;)]) #&gt; Recall Specificity Precision F1 #&gt; 0.8125475 0.9695105 0.8337286 0.8223970 The accuracy was much better now compared to \\(0.789\\) from the previous chapter without using Bagging!. We can also check what is the effect of adding more trees to the ensemble. The script iterated_bagging_activities.R does \\(5\\)-fold cross-validation as we just did but starting with \\(1\\) tree in the ensemble and repeating the process by adding more trees until \\(50\\). Figure 3.2 shows the effect on the train and test accuracy with different number of trees. Here, we can see that \\(3\\) trees already produce a significant performance increase compared to \\(1\\) or \\(2\\) trees. This makes sense since having only \\(2\\) trees does not add additional information. If the two trees produce different predictions then, it becomes a random choice between the two labels. In fact, \\(2\\) trees produced worse results than \\(1\\) tree. But we cannot make strong conclusions since the experiment was run only once. One possibility to break ties when there are only two trees is to use the averaged probabilities of each label. rpart can return those probabilities by setting type = \"prob\" in the predict() function which is the default behavior. This is left as an exercise for the reader. In the following section, Random Forest will be described which is a way of introducing more diversity to the base learners. Figure 3.2: Bagging results for different number of trees. 3.2 Random Forest rf_activities.R iterated_rf_activities.R iterated_bagging_rf.R Random Forest can be thought of as an extension of Bagging. Random Forests were proposed also by Breiman (2001) and as the name implies, they introduce more randomness to the individual trees since one of the aims is to have decorrelated trees. With Bagging, most of the trees are very similar at the root because the most important variables are selected first (see chapter 2). To avoid this happening, a simple modification can be introduced. When building a tree, instead of evaluating all features at each split to find the most important one (based on some purity measure like information gain), a random subset of the features (usually \\(\\sqrt{|features|}\\)) is sampled. This simple modification produces more decorrelated trees and in general, it results in better performance compared to Bagging. In R, the most famous library that implements Random Forest is…, yes you guessed it: randomForest (Liaw and Wiener 2002). The following code snippet shows how to fit a Random Forest with \\(10\\) trees. library(randomForest) rf &lt;- randomForest(class ~ ., trainSet, ntree = 10) By default, ntree = 500. Among other things, you can control how many random features are sampled at each split with the mtry argument. By default, for classification mtry = floor(sqrt(ncol(x))) and for regression mtry = max(floor(ncol(x)/3), 1). The following code will perform \\(5\\)-fold cross-validation with the activities dataset already stored in df and print the results. The complete code can be found in the script randomForest_activities.R. set.seed(1234) k &lt;- 5 folds &lt;- sample(k, size = nrow(df), replace = TRUE) # Variable to store ground truth classes. groundTruth &lt;- NULL # Variable to store the classifier&#39;s predictions. predictions &lt;- NULL for(i in 1:k){ trainSet &lt;- df[which(folds != i), ] testSet &lt;- df[which(folds == i), ] rf &lt;- randomForest(class ~ ., trainSet, ntree = 10) foldPredictions &lt;- predict(rf, testSet) predictions &lt;- c(predictions, as.character(foldPredictions)) groundTruth &lt;- c(groundTruth, as.character(testSet$class)) } cm &lt;- confusionMatrix(as.factor(predictions), as.factor(groundTruth)) # Print accuracy cm$overall[&quot;Accuracy&quot;] #&gt;Accuracy #&gt; 0.870801 # Print other metrics per class. cm$byClass[,c(&quot;Recall&quot;, &quot;Specificity&quot;, &quot;Precision&quot;, &quot;F1&quot;)] #&gt; Recall Specificity Precision F1 #&gt; Class: Downstairs 0.5094697 0.9652352 0.6127563 0.5563599 #&gt; Class: Jogging 0.9784615 0.9831268 0.9613059 0.9698079 #&gt; Class: Sitting 0.9803922 0.9992175 0.9868421 0.9836066 #&gt; Class: Standing 0.9512195 0.9990333 0.9790795 0.9649485 #&gt; Class: Upstairs 0.5363924 0.9636440 0.6608187 0.5921397 #&gt; Class: Walking 0.9543489 0.9151933 0.8752755 0.9131034 # Print other metrics overall. colMeans(cm$byClass[,c(&quot;Recall&quot;, &quot;Specificity&quot;, &quot;Precision&quot;, &quot;F1&quot;)]) #&gt; Recall Specificity Precision F1 #&gt; 0.8183807 0.9709083 0.8460130 0.8299943 Those results are better than the previous ones with Bagging. Figure 3.3 shows the results when doing \\(5\\)-fold cross-validation and increasing the number of trees (the complete script is in iterated_randomForest_activities.R). From these results, we can see a similar behavior than with Bagging. That is, the accuracy increases very quickly and then it stabilizes. Figure 3.3: Random Forest results for different number of trees. If we directly compare Bagging v.s. Random Forest, we can see than Random Forest outperforms Bagging (Figure 3.4). The complete code to generate the plot is in the script iterated_bagging_rf.R. Figure 3.4: Bagging v.s. Random Forest. 3.3 Stacked Generalization Stacked Generalization (a.k.a Stacking) is a powerful ensemble learning method proposed by Wolpert (1992). The method consists of training a set of powerful base learners (first-level learners) and then combining their outputs by stacking them to form a new train set. The base learners’ outputs correspond to their predictions and optionally, the class probabilities of those predictions. The predictions of the base learners are known as the meta-features. Then, those meta-features along with their true labels \\(y\\) are used to build a new train set that is used to train a meta-learner. The rationale behind this is that the predictions themselves contain information that can be used by the meta-learner. The procedure to train a Stacking model is as follows: Define a set of first level-learners \\(\\mathscr{L}\\) and a meta-learner. Train the first-level learners \\(\\mathscr{L}\\) with training data \\(\\textbf{D}\\). Predict the classes of \\(\\textbf{D}\\) with each learner in \\(\\mathscr{L}\\). Each learner produces a predictions vector \\(\\textbf{p}_i\\) with \\(\\lvert\\textbf{D}\\lvert\\) elements. Build a matrix \\(\\textbf{M}_{\\lvert\\textbf{D}\\lvert \\times \\lvert\\mathscr{L}\\lvert}\\) by column binding (stacking) the prediction vectors. Then, add the true labels \\(\\textbf{y}\\) to generate the new train set \\(\\textbf{D}&#39;\\). Train the meta-learner with \\(\\textbf{D}&#39;\\). Output the final stacking model \\(\\mathcal{S}:&lt;\\mathscr{L},\\textit{meta-learner}&gt;\\). Figure 3.5 shows the procedure to generate the new training data \\(\\textbf{D}&#39;\\) used to train the meta-learner. Figure 3.5: Process to generate the new train set D’ for the meta-learner by column-binding the predictions of the first-level learners and adding the true labels. Note that steps \\(2\\) and \\(3\\) can lead to overfitting because the predictions are made with the same data used to train the models. To avoid this, steps \\(2\\) and \\(3\\) are usually performed using \\(k\\)-fold cross-validation. After \\(\\textbf{D}&#39;\\) has been generated, the learners in \\(\\mathscr{L}\\) can be retrained using all data in \\(\\textbf{D}\\). Ting and Witten (1999) showed that the performance can increase by adding confidence information about the predictions. For example, the probabilities produced by the first-level learners. Most classifiers can output probabilities. At prediction time, each first-level learner predicts the class, and optionally, the class probabilities of a given instance. These predictions are used to form a feature vector (meta-features) that is fed to the meta-learner to obtain the final prediction. Usually, first-level learners are high performing classifiers such as Random Forests, Support Vector Machines, Neural Networks, etc. The meta-learner should also be a powerful classifier. In the next section, I will introduce Multi-view Stacking which basically is the same as Generalized Stacking but each first-level learner is trained with features from a different view. 3.4 Multi-view Stacking for Home Tasks Recognition stacking_algorithms.R stacking_activities.R Multi-view learning refers to the case when an instance can be characterized by two or more independent ‘views’. For example, one can extract features for webpage classification from a webpage’s text but also from the links pointing to it. Usually, there is the assumption that the views are independent and each is sufficient to solve the problem. Then, why combine them? In many cases, each different view provides additional and complementary information, thus, allowing to train better models. The simplest thing one can do is to extract features from each view, aggregate them, and train a single model. This approach usually works well but it has some limitations. Each view may have different statistical properties, thus, different types of models may be needed for each view. When aggregating features from all views, new variable correlations may be introduced which could impact the performance. Another limitation is that features need to be in the same format (feature vectors, images, etc.), so they can be aggregated. For video classification, we could have two views. One represented by sequences of images, and the other one by the corresponding audio. For the video part, we could encode the features as the images themselves, i.e., matrices. Then, a Convolutional Neural Network (covered in chapter 8) could be trained directly from those images. For the audio part, statistical features can be extracted and stored into normal feature vectors. In this case, the two representations (views) are different. One is a matrix and the other one a one-dimensional feature vector. Combining them to train a single classifier could be problematic given the nature of the views and their different encoding formats. Instead, we can train two models, one for each view and then combine the results. This is precisely the idea of Multi-view Stacking (Garcia-Ceja, Galván-Tejada, and Brena 2018). Train a different model for each view and combine the outputs like in Stacking. Here, Multi-view Stacking will be demonstrated using the HOME TASKS dataset. This dataset was collected from two sources. Acceleration and audio. The acceleration was recorded with a wrist-band watch and the audio using a cellphone. This dataset consists of \\(7\\) common home tasks: ‘mop floor’, ‘sweep floor’, ‘type on computer keyboard’, ‘brush teeth’, ‘wash hands’, ‘eat chips’, and ‘watch t.v.’. Three volunteers performed each activity for approximately \\(3\\) minutes. The acceleration and audio signals were segmented into \\(3\\) sec. windows. From each window, different features were extracted. From the acceleration, \\(16\\) features were extracted from the \\(3\\) axes (\\(x\\),\\(y\\),\\(z\\)) such as mean, standard deviation, maximum values, mean magnitude, area under the curve, etc. From the audio signals, \\(12\\) features were extracted, namely, Mel Frequency Cepstral Coefficients (MFCCs). To preserve volunteers’ privacy, the original audio was not released. The dataset already contains the extracted features from acceleration and audio. The first column is the label. In order to implement Multi-view Stacking, two Random Forests will be trained, one for each view (acceleration and audio). The predicted outputs will be stacked to form the new training set \\(D&#39;\\) and a Random Forest trained with \\(D&#39;\\) will act as the meta-learner. The next code snippet taken from stacking_algorithms.R shows the multi-view stacking function implemented in R. mvstacking &lt;- function(D, v1cols, v2cols, k = 10){ # Generate folds for internal cross-validation. folds &lt;- sample(1:k, size = nrow(D), replace = T) trueLabels &lt;- NULL predicted.v1 &lt;- NULL # predicted labels with view 1 predicted.v2 &lt;- NULL # predicted labels with view 2 probs.v1 &lt;- NULL # predicted probabilities with view 1 probs.v2 &lt;- NULL # predicted probabilities with view 2 # Perform internal cross-validation. for(i in 1:k){ train &lt;- D[folds != i, ] test &lt;- D[folds == i, ] trueLabels &lt;- c(trueLabels, as.character(test$label)) # Train learner with view 1 and make predictions. m.v1 &lt;- randomForest(label ~., train[,c(&quot;label&quot;,v1cols)], nt = 100) raw.v1 &lt;- predict(m.v1, newdata = test[,v1cols], type = &quot;prob&quot;) probs.v1 &lt;- rbind(probs.v1, raw.v1) pred.v1 &lt;- as.character(predict(m.v1, newdata = test[,v1cols], type = &quot;class&quot;)) predicted.v1 &lt;- c(predicted.v1, pred.v1) # Train learner with view 2 and make predictions. m.v2 &lt;- randomForest(label ~., train[,c(&quot;label&quot;,v2cols)], nt = 100) raw.v2 &lt;- predict(m.v2, newdata = test[,v2cols], type = &quot;prob&quot;) probs.v2 &lt;- rbind(probs.v2, raw.v2) pred.v2 &lt;- as.character(predict(m.v2, newdata = test[,v2cols], type = &quot;class&quot;)) predicted.v2 &lt;- c(predicted.v2, pred.v2) } # Build first-order learners with all data. learnerV1 &lt;- randomForest(label ~., D[,c(&quot;label&quot;,v1cols)], nt = 100) learnerV2 &lt;- randomForest(label ~., D[,c(&quot;label&quot;,v2cols)], nt = 100) # Construct meta-features. metaFeatures &lt;- data.frame(label = trueLabels, ((probs.v1 + probs.v2) / 2), pred1 = predicted.v1, pred2 = predicted.v2) #train meta-learner metalearner &lt;- randomForest(label ~., metaFeatures, nt = 100) res &lt;- structure(list(metalearner=metalearner, learnerV1=learnerV1, learnerV2=learnerV2, v1cols = v1cols, v2cols = v2cols), class = &quot;mvstacking&quot;) return(res) } The first argument D is a data frame containing the training data. v1cols and v2cols are the column names of the two views. Finally, argument k specifies the number of folds for the internal cross-validation to avoid overfitting (Steps \\(2\\) and \\(3\\) as described in the generalized stacking procedure). The function iterates through each fold and trains a Random Forest with the train data for each of the two views. Within each iteration, the trained models are used to predict the labels and probabilities on the internal test set. Predicted labels and probabilities on the internal test sets are concatenated across all folds (predicted.v1, predicted.v2). After cross-validation, the meta-features are generated by creating a data frame with the predictions of each view. Additionally, the average of class probabilities is added as a meta-feature. The true labels are also added. The purpose of cross-validation was to avoid overfitting but at the end, we do not want to waste data so both learners are re-trained with all data D. Finally, the meta-learner which is also a Random Forest is trained with the meta-features data frame. A list with all the required information to make predictions is created. This includes first-level learners, the meta-learner, and the column names for each view so we know how to divide the data frame into two views at prediction time. The following code snippet shows the implementation for making predictions using a trained stacking model. predict.mvstacking &lt;- function(object, newdata){ # Predict probabilities with view 1. raw.v1 &lt;- predict(object$learnerV1, newdata = newdata[,object$v1cols], type = &quot;prob&quot;) # Predict classes with view 1. pred.v1 &lt;- as.character(predict(object$learnerV1, newdata = newdata[,object$v1cols], type = &quot;class&quot;)) # Predict probabilities with view 2. raw.v2 &lt;- predict(object$learnerV2, newdata = newdata[,object$v2cols], type = &quot;prob&quot;) # Predict classes with view 2. pred.v2 &lt;- as.character(predict(object$learnerV2, newdata = newdata[,object$v2cols], type = &quot;class&quot;)) # Build meta-features metaFeatures &lt;- data.frame(((raw.v1 + raw.v2) / 2), pred1 = pred.v1, pred2 = pred.v2) # Set levels on factors to avoid errors in randomForest predict. levels(metaFeatures$pred1) &lt;- object$metalearner$classes levels(metaFeatures$pred2) &lt;- object$metalearner$classes predictions &lt;- as.character(predict(object$metalearner, newdata = metaFeatures), type=&quot;class&quot;) return(predictions) } The object parameter is the trained model and newdata is a data frame from which we want to make the predictions. First, labels and probabilities are predicted using the two views. Then, a data frame with the meta-features is assembled with the predicted label and the averaged probabilities. Finally, the meta-learner is used to predict the final classes using the meta-features. The script stacking_activities.R shows how to use our mvstacking() function. With the following two lines we can train and make predictions. m.stacking &lt;- mvstacking(trainset, v1cols, v2cols, k = 10) pred.stacking &lt;- predict(m.stacking, newdata = testset[,-1]) The script performs \\(10\\)-fold cross-validation and for the sake of comparison, it builds three models. One with only audio features, one with only acceleration features, and the Multi-view Stacking one combining both types of features. Table 3.1 shows the results for each view and with Multi-view Stacking. Clearly, combining both views with Multi-view Stacking achieved the best results compared to using a single view. Table 3.1: Stacking results. Accuracy Recall Specificity Precision F1 Audio 0.8463203 0.8413690 0.9741266 0.8487849 0.8438718 Accelerometer 0.8585859 0.8493635 0.9764988 0.8548481 0.8511572 Multi-view Stacking 0.9393939 0.9349011 0.9899275 0.9371645 0.9358082 Figure 3.6: Confusion matrices. Figure 3.6 shows the resulting confusion matrices for the three cases. By looking at the recall (anti-diagonal) of the individual classes, it seems that audio features are better at recognizing some activities like ‘sweep’ and ‘mop floor’ whereas the accelerometer features are better for classifying ‘eat chips’, ‘wash hands’, ‘type on keyboard’, etc. thus, those two views are somehow complementary. All recall values when using Multi-view Stacking are higher than any of the other views. 3.5 Summary In this chapter, several ensemble learning methods were introduced. In general, ensemble models perform better than single models. The main idea of ensemble learning is to train several models and combine their results. Bagging is an ensemble method consisting of \\(n\\) base learners, each, trained with bootstrapped training samples. Random Forest is an ensemble of trees. It introduces randomness to the trees by selecting random features in each split. Another ensemble method is called stacked generalization. It consists of a set of base learners and a meta-learner. The later is trained using the outputs of the base learners. Multi-view learning can be used when an instance can be represented by two or more views (for example, different sensors). References "],
["edavis.html", "Chapter 4 Exploring and Visualizing Behavioral Data 4.1 Talking with Field Experts 4.2 Summary Statistics 4.3 Class Distributions 4.4 User-Class Sparsity Matrix 4.5 Boxplots 4.6 Correlation Plots 4.7 Timeseries 4.8 Multidimensional Scaling (MDS) 4.9 Heatmaps 4.10 Automated EDA 4.11 Summary", " Chapter 4 Exploring and Visualizing Behavioral Data EDA.R, iterative_mds.R Exploratory data analysis (EDA) refers to the process of understanding your data. There are several available methods and tools for doing so, including summary statistics and visualizations. In this chapter, I will cover some of them. As mentioned in section 1.4, data exploration is one of the first steps of the data analysis pipeline. This step is important because it will help in the decision process during the next steps, for example, the selection of pre-processing tasks and predictive methods. Even though there already exist several EDA techniques, you are not constrained on them but you can always apply any means that you think will allow you to better understand your data and gain new insights. 4.1 Talking with Field Experts Sometimes we will be involved in the whole data analysis process starting with the idea, defining research questions, hypotheses, conducting the data collection, and so on. In those cases, it is easier to understand the initial structure of the data since you might had been the one responsible for designing the data collection protocol. Unfortunately (or fortunately for some), it is often the case that we are already given a dataset. It may have some documentation or not. In those cases, it becomes important to talk with the field experts that designed the study and the data collection protocol to understand what was the purpose and motivation of each piece of data. Again, it is often not easy to be directly in touch with the people that conducted the initial study. One of the reasons may be that you found the dataset online and maybe the project is already over. In those cases, you can try to contact the authors. I have done that several times and they were very responsive. It is also a good idea to try to find experts in the field even if they were not involved in the project. This will allow you to understand things from their perspective and possibly to explain patterns/values that you may find later in the process. 4.2 Summary Statistics Now that you understand how the data was collected and what is the meaning of each variable, it is time to find out how the actual data looks like. It is always a good idea to start looking at some summary statistics. This provides some general insights about the data and will help you in deciding future pre-processing steps. In R, an easy way to do this is with the summary() function. The following code reads the SMARTPHONE ACTIVITIES dataset and due to limited space, only prints a summary of the first \\(5\\) columns, column \\(33\\),\\(35\\), and the last one (the class). # Read activities dataset. dataset &lt;- read.csv(file.path(datasets_path, stringsAsFactors = T) # Print first 5 columns, # column 33, 35 and the last one (the class). summary(dataset[,c(1:5,33,35,ncol(dataset))]) #&gt; UNIQUE_ID user X0 X1 #&gt; Min. : 1.0 Min. : 1.00 Min. :0.00000 Min. :0.00000 #&gt; 1st Qu.:136.0 1st Qu.:10.00 1st Qu.:0.06000 1st Qu.:0.07000 #&gt; Median :271.0 Median :19.00 Median :0.09000 Median :0.10000 #&gt; Mean :284.4 Mean :18.87 Mean :0.09414 Mean :0.09895 #&gt; 3rd Qu.:412.0 3rd Qu.:28.00 3rd Qu.:0.12000 3rd Qu.:0.12000 #&gt; Max. :728.0 Max. :36.00 Max. :1.00000 Max. :0.81000 #&gt; #&gt; X2 XAVG ZAVG class #&gt; Min. :0.00000 Min. :0 ?0.22 : 29 Downstairs: 528 #&gt; 1st Qu.:0.08000 1st Qu.:0 ?0.21 : 27 Jogging :1625 #&gt; Median :0.10000 Median :0 ?0.11 : 26 Sitting : 306 #&gt; Mean :0.09837 Mean :0 ?0.13 : 26 Standing : 246 #&gt; 3rd Qu.:0.12000 3rd Qu.:0 ?0.16 : 26 Upstairs : 632 #&gt; Max. :0.95000 Max. :0 ?0.23 : 26 Walking :2081 #&gt; (Other):5258 For numerical variables, the output includes some summary statistics like the min, max, mean, etc. For factor variables, the output is different. It displays the unique values with their respective counts. If there are more than six unique values, the rest is omitted. For example, the class variable (the last one) has \\(528\\) instances with the value ‘Downstairs’. By looking at the min and max values of the numerical variables, one can see that those are not the same for all variables. For some variables, their maximum value is \\(1\\), for others, it is less than \\(1\\) and for some others, it is greater than \\(1\\). It seems that the variables are not in the same scale. This is important because some algorithms are sensitive to different scales. In chapters 2 and 3 we mainly used decision tree-based algorithms which are not sensitive to different scales, but some others like neural networks are. In chapter 5, a method to transform variables into the same scale will be introduced. It is a good practice to check the min and max values of all variables to see if they have different ranges since some algorithms are sensitive to this. The output of the summary() function also shows some strange things. The statistics of variable XAVG are all \\(0s\\). Some other variables like ZAVG were encoded as characters. Seems that the ‘?’ symbol is appended to the numbers. In summary, the summary() function (I know, too many summaries in this sentence), allowed us to spot some errors in the dataset. What we do next with that information, will depend on the domain and application. 4.3 Class Distributions When it comes to behavior sensing, many of the problems are modeled as classification problems. This means that there are different possible categories. It is often a good idea to plot the class counts (class distribution). The following code shows how to do that for the SMARTPHONE ACTIVITIES dataset. First, the table() method is used to get the actual class counts. Then, the plot is generated with ggplot. t &lt;- table(dataset$class) t &lt;- as.data.frame(t) colnames(t) &lt;- c(&quot;class&quot;,&quot;count&quot;) p &lt;- ggplot(t, aes(x=class, y=count, fill=class)) + geom_bar(stat=&quot;identity&quot;, color=&quot;black&quot;) + theme_minimal() + geom_text(aes(label=count), vjust=-0.3, size=3.5) + scale_fill_brewer(palette=&quot;Set1&quot;) print(p) Figure 4.1: Distribution of classes. The most common activity turned out to be ‘Walking’ with \\(2081\\) instances. It seems that the volunteers were a bit sporty since ‘Jogging’ is the second most frequent activity. One thing to note is that there are some big differences here. For example, ‘Walking’ v.s. ‘Standing’. Those differences in class counts can have an impact when training classification models. This is because classifiers try to minimize the overall error regardless of the performance of individual classes, thus, they tend to prioritize the majority classes. This is called the class imbalance problem. This occurs when there are many instances of some classes but many fewer of some other classes. For some applications this can be a problem. For example, in fraud detection, datasets have many legitimate transactions but just a few of illegal transactions. This will bias a classifier to be good at detecting legitimate transactions but what we are really interested in is in detecting the illegal transactions. This is something very common to find in behavior sensing datasets. For example in the medical domain, it is much easier to collect data from healthy controls than from patients with a given condition. In chapter 5, some of the oversampling techniques that can be used to deal with the class imbalance problem will be presented. When the classes are imbalanced, it is also recommended to validate the generalization performance using stratified subsets. This means that when dividing the dataset into train and test sets, the distribution of classes should be preserved. For example, if the dataset has class ‘A’ and ‘B’ and \\(80\\%\\) of the instances are of type ‘A’ then both, the train set and the test set should have \\(80\\%\\) of their instances of type ‘A’. In cross-validation, this is known as stratified cross-validation. 4.4 User-Class Sparsity Matrix In behavior sensing, usually two things are involved: individuals and behaviors. Individuals will express different behaviors to different extents. For the activity recognition example, some persons may go jogging frequently while others may never go jogging at all. Some behaviors will be present or absent depending on each individual. We can plot this information with what I call a user-class sparsity matrix. Figure 4.2 shows this matrix for the activities dataset. The code to generate this plot is included in the script EDA.R. Figure 4.2: User-class sparsity matrix. The x-axis shows the user ids and the y-axis the classes. A colored entry (gray in this case) means that the corresponding user has at least one associated instance of the corresponding class. For example, user \\(3\\) performed all activities and thus, the dataset contains at least one instance for each of the six activities. On the other hand, user \\(25\\) only has instances for two activities. Users are sorted in descending order (users that have more classes are at the left). At the bottom of the plot, the sparsity is shown (\\(0.18\\)). This is just the percentage of empty cells in the matrix. When all users have at least one instance of every class the sparsity is \\(0\\). When the sparsity is different than \\(0\\), one needs to decide what to do depending on the application. The following cases are possible: Some users did not perform all activities. If the classifier was trained with, for example, \\(6\\) classes and a user never goes ‘jogging’, the classifier may still sometimes predict ‘jogging’ even if a particular user never does that. This can degrade the predictions’ performance for that particular user and can be worse if that user never performs other activities of the possible \\(6\\). A possible solution is to train different classifiers with different class subsets. If you know that some users never go ‘jogging’ then you train a classifier that excludes ‘jogging’ and use that one for that set of users. The disadvantage of this is that there are many possible combinations so you need to train many models. Since several classifiers can generate prediction scores and/or probabilities per class, another solution would be to train a single model with all classes and predict the most probable class excluding those that are not part of a particular user. Some users can have unique classes. For example, suppose there is a new user that has an activity labeled as ‘Eating’ which no one else has, and thus, it was not included during training. In this situation, the classifier will never predict ‘Eating’ since it was not trained for that activity. One solution could be to add the new user’s data with the new labels and retrain the model. But if not too many users have the activity ‘Eating’ then, in the worst case, they will die of starvation. In a less severe case, the overall system performance can degrade because as the number of classes increases it becomes more difficult to find separation boundaries between categories, thus, the models become less accurate. Another possible solution is to build user-dependent models for each user. These, and other types of models in multi-user settings will be covered in chapter 9. 4.5 Boxplots Box plots are a good way to visualize the relationship between variables and classes. R already has the boxplot() function. In the SMARTPHONE ACTIVITIES dataset, the RESULTANT variable represents the ‘total amount of movement’ considering the three axes. It is the average of the square roots of the sum of the values of each axis squared √(xi^2 + yi^2 + zi^2). The following code displays a set of boxplots (one for each class) with respect to the RESULTANT variable. boxplot(RESULTANT ~ class, dataset) Figure 4.3: Boxplot of RESULTANT variable across classes. The solid black line in the middle of each box marks the median. For a more complete explanation about boxplots please see here.6 Overall, we can see that this variable can be good at separating high-intensity activities like jogging, walking, etc. from low-intensity ones like sitting or standing. With boxplots we can inspect one feature at a time. If you want to visualize the relationship between predictors, correlation plots can be used instead. Correlation plots will be presented in the next subsection. 4.6 Correlation Plots This type of plot allows us to look at relationships between pairs of variables. The most common type of relationship is the Pearson correlation. The Pearson correlation measures the degree of linear association between two variables. It takes values between \\(-1\\) and \\(1\\). A correlation of \\(1\\) means that as one of the variables increases, the other one does too. A value of \\(-1\\) means that as one of the variables increases, the other decreases. A value of \\(0\\) means that there is no association between the variables. Figure 4.4 shows several examples of correlation values. Note that the correlations of the examples at the bottom are all \\(0\\)s. Even though there are some noticeable patterns in some of the examples, their correlation is \\(0\\) because those relationships are not linear. Figure 4.4: Pearson correlation examples. Source: wikipedia The Pearson correlation (denoted by \\(r\\)) between two variables \\(x\\) and \\(y\\) can be calculated as follows: \\[\\begin{equation} r = \\frac{ \\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y}) }{ \\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}} \\tag{4.1} \\end{equation}\\] The following code snippet uses the corrplot library to generate a correlation plot for the HOME TASKS dataset. Remember that this dataset contains two sets of features. One set extracted from audio and the other one extracted from the accelerometer sensor. First, the Pearson correlation between each pair of variables is computed with the cor() function and then the corrplot() function is used to generate the actual plot. Here, we specify that we only want to display the upper diagonal with type = \"upper\". The tl.pos argument controls where to print the labels. In this example, at the top and in the diagonal. Setting diag = FALSE instructs the function not to print the principal diagonal which is all ones since it is the correlation between each variable and itself. library(corrplot) # Load home activities dataset. dataset &lt;- read.csv(file.path(datasets_path, &quot;home_tasks&quot;, &quot;sound_acc.csv&quot;)) CORRS &lt;- cor(dataset[,-1]) corrplot(CORRS, diag = FALSE, tl.pos = &quot;td&quot;, tl.cex = 0.5, method = &quot;color&quot;, type = &quot;upper&quot;) Figure 4.5: Correlation plot of the HOME TASKS dataset. It looks like the correlations between sound features (v1_) and acceleration features (v2_) are not too big. In this case, this is good since we want both sources of information to be as independent as possible such that they capture different characteristics and can complement each other as explained in section 3.4. On the other hand, there are high correlations between some acceleration features. For example v2_maxY with v2_sdMagnitude. Please, be aware that the Pearson correlation only captures linear relationships. 4.6.1 Interactive Correlation Plots When plotting correlation plots, it is useful to also visualize the actual correlation values. When there are many variables, it becomes difficult to do that. One way to overcome this limitation is by using interactive plots. The following code snippet uses the function iplotCorr() from the qtlcharts package to generate an interactive correlation plot. The nice thing about it, is that you can actually inspect the cell values by hovering the mouse. If you click on a cell, the corresponding scatter plot is also rendered. This makes these types of plots very convenient tools to explore variable relationships. library(qtlcharts) # Library for interactive plots. # Load home activities dataset. dataset &lt;- read.csv(file.path(datasets_path, &quot;home_tasks&quot;, &quot;sound_acc.csv&quot;)) iplotCorr(dataset[,-1], reorder=F, chartOpts=list(cortitle=&quot;Correlation matrix&quot;, scattitle=&quot;Scatterplot&quot;)) Please note that at the time this book was written, printed paper does not support interactive plots. Please, see the online version instead to see the actual result or run the code on a computer. 4.7 Timeseries Behavior is something that usually depends on time. Thus, being able to visualize time series data becomes essential. To illustrate how timeseries data can be plotted, I will use the ggplot package and the HAND GESTURES dataset. Recall that the data was collected with a tri-axial accelerometer, thus, for each hand gesture we have \\(3\\)-dimensional timeseries. Each dimension represents one of the x, y, and z axes. First, we read one of the text files that stores a hand gesture from user \\(1\\). Each column represents an axis. Then, we need to do some formatting. We will create a data frame with three columns. The first one is a timestep represented as integers from \\(1\\) to the number of points per axis. The second column is a factor that represents the axis x, y, or z. The last column contains the actual values. dataset &lt;- read.csv(&quot;../data/hand_gestures/1/1_20130703-120056.txt&quot;, header = F) # Do some preprocessing. type &lt;- c(rep(&quot;x&quot;, nrow(dataset)), rep(&quot;y&quot;, nrow(dataset)), rep(&quot;z&quot;, nrow(dataset))) type &lt;- as.factor(type) values &lt;- c(dataset$V1, dataset$V2, dataset$V3) t &lt;- rep(1:nrow(dataset), 3) df &lt;- data.frame(timestep = t, type = type, values = values) # Print first rows. head(df) #&gt; timestep type values #&gt; 1 1 x 0.6864655 #&gt; 2 2 x 0.9512450 #&gt; 3 3 x 1.3140911 #&gt; 4 4 x 1.4317709 #&gt; 5 5 x 1.5102241 #&gt; 6 6 x 1.5298374 Note that the last column (values) contains the values of all axes instead of having one column per axis. Then we can use the ggplot() function. The lines are colored by type of axis and this is specified with colour = type. The type column should be a factor. The line type is also dependent on the type of axis and is specified with linetype = type. tsPlot &lt;- ggplot(data = df, aes(x = timestep, y = values, colour = type, linetype = type)) + ggtitle(&quot;Hand gesture &#39;1&#39;, user 1&quot;) + xlab(&quot;Timestep&quot;) + ylab(&quot;Acceleration&quot;) + geom_line(aes(color=type)) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5), legend.position=&quot;right&quot;, legend.key.width = unit(1.0,&quot;cm&quot;), legend.key.size = unit(0.5,&quot;cm&quot;)) print(tsPlot) Figure 4.6: Time series plot for hand gesture ‘1’ user 1. 4.7.1 Interactive Timeseries Sometimes it is useful to be able to interactively zoom, highlight, select, etc. parts of the plot. In R, there is a package called dygraphs (Vanderkam et al. 2018) that allows you to generate fancy interactive plots for timeseries data. The following code snippet reads a hand gesture file and ads a column at the beginning called timestep. library(dygraphs) # Read the hand gesture &#39;1&#39; for user 1. dataset &lt;- read.csv(&quot;../data/hand_gestures/1/1_20130703-120056.txt&quot;, header = F, col.names = c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;)) dataset &lt;- cbind(timestep = 1:nrow(dataset), dataset) Then we can generate a minimal plot with one line of code with: dygraph(dataset) In the online version of this book or if you run the code, you can zoom in by clicking and dragging over a region. A double click will restore the zoom. It is possible to add a lot of customization to the plots. For example, the following code adds a text title, fills the area under the lines, adds a point of interest line, and shades the region between \\(30\\) and \\(40\\). dygraph(dataset, main = &quot;Hand Gesture &#39;1&#39;&quot;) %&gt;% dyOptions(fillGraph = TRUE, fillAlpha = 0.25) %&gt;% dyEvent(&quot;10&quot;, &quot;Point of interest&quot;, labelLoc = &quot;top&quot;) %&gt;% dyShading(from = &quot;30&quot;, to = &quot;40&quot;, color = &quot;#CCCCCC&quot;) For a comprehensive list of available features of the dygraph package, the reader is advised to check its demos website: https://rstudio.github.io/dygraphs/index.html. 4.8 Multidimensional Scaling (MDS) In many situations, our data is comprised of several variables. If the number of variables is more than \\(3\\) (\\(3\\)-dimensional data), it becomes difficult to plot the relationships between data points. Take, for example, the HOME TASKS dataset which has \\(27\\) predictor variables from accelerometer and sound. One thing that we may want to do is to visually inspect the data points and check whether or not points from the same class are closer compared to points from different classes. This can give you an idea of the difficulty of the problem at hand. If points of the same class are very close and grouped together then, it is likely that a classification model will not have trouble separating the data points. But how do we plot such relationships with high dimensional data? One method is by using multidimensional scaling (MDS) which consists of a set of techniques aimed at reducing the dimensionality of data so it can be visualized in \\(2\\)D or \\(3\\)D. The objective is to plot the data such that the original distances between pairs of points are preserved in a given lower dimension \\(d\\). There exist several MDS methods but most of them take a distance matrix as input (for example, Euclidean distance). In R, generating a distance matrix from a set of points is easy. As an example, let’s generate some sample data points. # Generate 3 2D random points. x &lt;- runif(3) y &lt;- runif(3) df &lt;- data.frame(x,y) labels &lt;- c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) print(df) #&gt; x y #&gt; 1 0.4457900 0.5978606 #&gt; 2 0.4740106 0.5019398 #&gt; 3 0.8890085 0.4109234 The dist() function can be used to compute the distance matrix. By default, this function computes the Euclidean distance between rows: dist(df) #&gt; 1 2 #&gt; 2 0.09998603 #&gt; 3 0.48102824 0.42486143 The output is the Euclidean distance between the pairs of rows \\((1,2)\\), \\((1,3)\\) and \\((2,3)\\). One way to obtain cartesian coordinates in a \\(d\\) dimensional space for \\(n\\) points from their distance matrix \\(D\\) is to use an iterative algorithm (Borg, Groenen, and Mair 2012). Such an algorithm consists of the following general steps: Initialize \\(n\\) data points with random coordinates \\(C\\) of dimension \\(d\\). Compute a distance matrix \\(D&#39;\\) from \\(C\\). Move the coordinates \\(C\\) such that the distances of \\(D&#39;\\) get closer to the original ones in \\(D\\). Repeat from step \\(2\\) until the error between \\(D&#39;\\) and \\(D\\) cannot be reduced any further or until some predefined max number of iterations. The script iterative_mds.R implements this algorithm (iterativeMDS() function) which is based on the implementation from (Segaran 2007). Its first argument D is a distance matrix, the second argument maxit is the total number of iterations and the last argument lr controls how fast the points are moved in each iteration. The script also shows how to apply the method to the eurodist dataset which consists of distances between several European cities. Figure 4.7 shows the initial random coordinates of the cities. Then, Figure 4.8 shows the result after \\(30\\) iterations. Finally, Figure 4.9 shows the final result. By only knowing the distance matrix, the algorithm was able to find a visual mapping that closely resembles the real positions. Figure 4.7: MDS initial coordinates. Figure 4.8: MDS coordinates after iteration 30. Figure 4.9: MDS final coordinates. Before continuing, I would like to apologize because the previous example (cities’ distances) has nothing to do with behavior which is supposed to be the topic of this book (and probably the reason you are reading it). R already has more efficient implementations to perform MDS and one of them is via the function cmdscale(). Its first argument is a distance matrix and the second argument \\(k\\) is the target dimension. It also has some other additional parameters that can be tuned. This function implements classical MDS based on Gower (1966). The following code snippet uses the HOME TASKS dataset. It selects the accelerometer-based features (v2_*), uses the cmdscale() function to reduce them into \\(2\\), dimensions and plots the result. dataset &lt;- read.csv(&quot;../data/home_tasks/sound_acc.csv&quot;) colNames &lt;- names(dataset) v2cols &lt;- colNames[grep(colNames, pattern = &quot;v2_&quot;)] cols &lt;- as.integer(dataset$label) labels &lt;- unique(dataset$label) d &lt;- dist(dataset[,v2cols]) fit &lt;- cmdscale(d, k = 2) # k is the number of dim x &lt;- fit[,1]; y &lt;- fit[,2] plot(x, y, xlab=&quot;Coordinate 1&quot;, ylab=&quot;Coordinate 2&quot;, main=&quot;Accelerometer features in 2D&quot;, pch=19, col=cols, cex=0.7) legend(&quot;topleft&quot;, legend = labels, pch=19, col=unique(cols), cex=0.7, horiz = F) We can also reduce the data into \\(3\\) dimensions and use the scatterplot3d package to generate a \\(3\\)D scatter plot: library(scatterplot3d) fit &lt;- cmdscale(d,k = 3) x &lt;- fit[,1]; y &lt;- fit[,2]; z &lt;- fit[,3] scatterplot3d(x, y, z, xlab = &quot;&quot;, ylab = &quot;&quot;, zlab = &quot;&quot;, main=&quot;Accelerometer features in 3D&quot;, pch=19, color=cols, tick.marks = F, cex.symbols = 0.5, cex.lab = 0.7, mar = c(1,0,1,0)) legend(&quot;topleft&quot;,legend = labels, pch=19, col=unique(cols), cex=0.7, horiz = F) From those plots, it can be seen that the different points are more or less grouped together based on the type of activity. Still, there are several points with no clear grouping which would make them difficult to classify. In section 3.4 from chapter 3, we achieved a classification accuracy of \\(0.85\\) when using only the accelerometer data. 4.9 Heatmaps Heatmaps are a good way to visualize the ‘intensity’ of events. For example, a heatmap can be used to depict website interactions by overlapping colored pixels relative to the number of clicks. This visualization eases the process of identifying the most relevant sections of the given website, for example. In this section, we will generate a heatmap of weekly motor activity levels of individuals with and without diagnosed depression. The DEPRESJON dataset will be used for this task. It contains motor activity recordings captured with an actigraphy device which is like a watch but has several sensors including accelerometers. The device registers the amount of movement every minute. The data contains recordings of \\(23\\) patients and \\(32\\) controls (those without depression). The participants wore the device for \\(13\\) days on average. The accompanying script auxiliary_eda.R has the function computeActivityHour() that returns a matrix with the average activity level of the depressed patients or the controls (those without depression). The matrix dimension is \\(24\\times7\\) and it stores the average activity level at each day and hour. The type argument is used to specify if we want to compute this matrix for the depressed or control participants. source(&quot;auxiliary_eda.R&quot;) # Generate matrix with mean activity levels # per hour for the control and condition group. map.control &lt;- computeActivityHour(datapath, type = &quot;control&quot;) map.condition &lt;- computeActivityHour(datapath, type = &quot;condition&quot;) Since we want to compare the heatmaps of both groups we will normalize the matrices such that the values are between \\(0\\) and \\(1\\) in both cases. The script also contains a method normalizeMatrices() to do the normalization. # Normalize matrices. res &lt;- normalizeMatrices(map.control, map.condition) Then, the pheatmap package (Kolde 2019) can be used to create the actual heatmap from the matrices. library(pheatmap) library(gridExtra) # Generate heatmap of the control group. a &lt;- pheatmap(res$M1, main=&quot;control group&quot;, cluster_row = FALSE, cluster_col = FALSE, show_rownames = T, show_colnames = T, legend = T, color = colorRampPalette(c(&quot;white&quot;, &quot;blue&quot;))(50)) # Generate heatmap of the condition group. b &lt;- pheatmap(res$M2, main=&quot;condition group&quot;, cluster_row = FALSE, cluster_col = FALSE, show_rownames = T, show_colnames = T, legend = T, color = colorRampPalette(c(&quot;white&quot;, &quot;blue&quot;))(50)) # Plot both heatmaps together. grid.arrange(a$gtable, b$gtable, nrow=2) Figure 4.10 shows the two heatmaps. Here, we can see that overall, the condition group has lower activity levels. It can also be observed that people in the control group wakes up at around \\(6\\) but in the condition group activity starts to increase until \\(7\\) in the morning. Activity levels around midnight look higher during weekends compared to weekdays. Figure 4.10: Activity level heatmaps for the control and condition group. All in all, heatmaps provide a good way to look at the overall patterns of a dataset and can provide some insights to further explore some aspects of the data. 4.10 Automated EDA Most of the time, doing an EDA involves more or less the same steps: print summary statistics, generate boxplots, visualize variable distributions, look for missing values, etc. If your data is stored as a data frame, all those tasks require almost the same code for any dataset. To speed up this process, some packages have been developed. They provide convenient functions to explore the data and generate automatic reports. The DataExplorer package (Cui 2020) has several interesting functions to explore a dataset. The following code uses the plot_str() function to plot the structure of dataset which is a data frame read from the HOME TASKS dataset. The complete code is available in script EDA.R. The output is shown in figure 4.11. This plot shows the number of observations, the number of variables, the variable names and their types. library(DataExplorer) dataset &lt;- read.csv(&quot;../data/home_tasks/sound_acc.csv&quot;) plot_str(dataset) Figure 4.11: Output of function plotstr(). Another useful function is introduce(). This one prints some statistics like the number of rows, columns, missing values, etc. Table 4.1 shows the output result. introduce(dataset) Table 4.1: Output of the introduce() function. rows 1386 columns 29 discrete_columns 1 continuous_columns 28 all_missing_columns 0 total_missing_values 0 complete_rows 1386 total_observations 40194 memory_usage 328680 The package provides more functions to explore your data. The create_report() function can be used to automatically call several of those functions and generate a report in html. The package also offers functions to do feature engineering such as replacing missing values, create dummy variables (covered in chapter 5), etc. For a more detailed presentation of the package’s capabilities please check its vignette7. There is anohter similar package called inspectdf (Rushworth 2019) which has similar functionality. It also offers some functions to check if the categorical variables are imbalanced. This is handy if one of the categorical variables is the response variable (the one we want to predict) since having imbalanced classes may pose some problems (more on this in chapter 5). The following code generates a plot that represents the counts of categorical variables. This dataset only has one categorical variable: label. library(inspectdf) show_plot(inspect_cat(dataset)) Figure 4.12: Output of function plotstr(). Here, we can see that the most frequent class is ‘eat_chips’ and the less frequent one is ‘sweep’. We can confirm this by printing the actual counts: table(dataset$label) #&gt; brush_teeth eat_chips mop_floor sweep type_on_keyboard #&gt; 180 282 181 178 179 #&gt; wash_hands watch_tv #&gt; 180 206 This chapter provided a brief introduction to some exploratory data analysis tools and methods however, this is only a tiny subset. There is already an entire book about EDA with R which I recommend you to check (Peng 2016). 4.11 Summary One of the first tasks during a data analysis pipeline is to familiarize yourself with the data. There are several techniques and tools that can provide support during this process. Talking with field experts can help you to better understand the data. Generating summary statistics is a good way to gain general insights of a dataset. In R, the summary() function will compute such statistics. For classification problems, one of the first steps is to check the distribution of classes. In multi-user settings, generating a user-class sparsity matrix can be useful to detect missing classes per user. Boxplots and correlation plots are used to understand the behavior of the variables. R, has several packages for creating interactive plots such as dygraphs for timeseries and qtlcharts for correlation plots. Multidimensional scaling (MDS) can be used to project high-dimensional data into \\(2\\) or \\(3\\) dimensions so they can be plotted. R has some packages like DataExplorer that provide some degree of automation for exploring a dataset. References "],
["preprocessing.html", "Chapter 5 Preprocessing Behavioral Data 5.1 Missing Values 5.2 Smoothing 5.3 Normalization 5.4 Imbalanced Classes 5.5 Information Injection 5.6 One-hot Encoding 5.7 Summary", " Chapter 5 Preprocessing Behavioral Data preprocessing.R Sensor-based behavioral data comes in many flavors and forms, but when training predictive models, the data needs to be in a particular format. Some of the sources of variation when collecting data are: Sensors’ format. Each type of sensor and manufacturer stores data in a different format. For example, .csv files, binary files, images, proprietary formats, etc. Sampling rate. The sampling rate indicates how many measurements are taken per unit of time. For example, a heart rate sensor may return a single value every second, thus, the sampling rate is \\(1\\) Hz. An accelerometer that captures \\(50\\) values per second has a sampling rate of \\(50\\) Hz. Scales and ranges. Some sensors may return values in degrees (e.g., a temperature sensor) while others may return values in some other scale, for example, in centimeters for a proximity sensor. Furthermore, ranges can also vary. That is, a sensor may capture values in the range of \\(0\\)-\\(1000\\), for example. During the data exploration step (chapter 4) we may also find that there are missing values, inconsistent values, noise, and so on, thus, we also need to take care of that. This chapter provides an overview of some common methods used to clean and preprocess the data before one can start training reliable models. Several of the methods presented here can lead to information injection which can cause overfitting. That is, transferring information from the train set to the test set. This is something that we do not want because both sets need to be independent to accurately estimate the generalization performance. You can find more details about information injection in section 5.5 of this chapter. 5.1 Missing Values Many datasets will have missing values and we need ways to identify and deal with them. The reasons for having missing data could be due to faulty sensors, processing errors, unavailable information, and so on. In this section, I present some tools that ease the process of missing values identification. Later, some imputation methods that can be used to fill the missing values are listed. To demonstrate some of these concepts, the SHEEP GOATS dataset (Kamminga et al. 2017) will be used. Due to its big size, the files of this dataset are not included with the accompanying book files but they can be downloaded from https://easy.dans.knaw.nl/ui/datasets/id/easy-dataset:76131. The data was released as part of a study about animal behaviors. The researchers placed inertial sensors on sheep and goats and tracked their behavior during one day. They also video-recorded the session and annotated the data with different types of behaviors such as grazing, fighting, scratch-biting, etc. The device was placed on the neck with random orientation and it collects acceleration, orientation, magnetic field, temperature, and barometric pressure. Figure 5.1 shows a schematic view of the setting. Figure 5.1: The device was placed on the neck at a random position. We will start by loading a .csv file that corresponds to one of the sheep and check if there are missing values. The naniar package (Tierney et al. 2019) offers a set of different functions to explore and deal with missing values. The gg_miss_var() function allows you to quickly check which variables have missing values and how many. The following code loads the data and then plots the number of missing values in each variable. library(naniar) # Path to S1.csv file. datapath &lt;- file.path(datasets_path, &quot;sheep_goats&quot;,&quot;S1.csv&quot;) # Can take some seconds to load since the file is big. df &lt;- read.csv(datapath, stringsAsFactors = TRUE) # Plot missing values. gg_miss_var(df) Figure 5.2 shows the resulting output. This plot shows that there are missing values in four variables: pressure, cz, cy and cx. The last three correspond to the compass (magnetometer). For pressure, the number of missing values is more than \\(2\\) million! For the rest, it is a bit less (more than \\(1\\) million). Figure 5.2: Missing values counts. To further explore this issue, we can plot each observation in a row with the function vis_miss(). # Select first 1000 rows. # It can take some time to plot bigger data frames. vis_miss(df[1:1000,]) Figure 5.3 shows every observation per row and missing values are black colored (if any). From this image, it can be seen that missing values seem to be systematic. It looks like there is a clear stripes pattern, especially for the compass variables. Based on these observations, we could guess that this doesn’t look like random sensor failures or random noise. Figure 5.3: Rows with missing values. If we explore the data frame’s values, for example with the RStudio viewer (Figure 5.4), two things can be noted. First, for the compass values, there is a missing value for each present value. Thus, it looks like \\(50\\%\\) of compass values are missing. For pressure, it seems that there are \\(7\\) missing values for each available value. Figure 5.4: Displaying the data frame in RStudio. So, what could be the root cause of those missing values? Remember that at the beginning of this chapter it was mentioned that one of the sources of variation is sampling rate. If we look at the data set documentation all sensors have a sampling rate of \\(200\\) Hz except for the compass and the pressure sensor. The compass has a sampling rate of \\(100\\) Hz. That is half compared to the other sensors! This explains why \\(50\\%\\) of the rows are missing. Similarly, the pressure sensor has a sampling rate of \\(25\\) Hz. By visualizing and then inspecting the missing data, we have just found out that the missing values are not caused by random noise or sensor failures but because the sensors are not as fast as the others! Now that we know there are missing values we need to decide what to do with them. The following subsection lists some ways to deal with missing values. 5.1.1 Imputation Imputation is the process of filling in missing values. One of the reasons for imputing missing values is that some predictive models cannot deal with missing data. Another reason is that it may help in increasing the predictions’ performance, for example, if we are trying to predict the sheep behavior from a discrete set of categories based on the inertial data. There are different ways to handle missing values: Discard rows. If the rows with missing values are not too many, they can simply be discarded. Mean value. Fill the missing values with the mean value of the corresponding variable. This method is simple and can be effective. One of the problems with this method is that it is sensitive to outliers (as it is the arithmetic mean). Median value. The median is robust against outliers, thus, it can be used instead of the arithmetic mean to fill the gaps. Replace with the closest value. For timeseries data, as is the case of the sheep readings, one could also replace missing values with the closest known value. Predict the missing values. Use the other variables to predict the missing one. This can be done by training a predictive model. A regressor if the variable is numeric or a classifier if the variable is categorical. Another problem with the mean and median values is that they can be correlated with other variables, for example, with the class that we want to predict. One way to avoid this, is to compute the mean (or median) for each class but still, some hidden correlations may bias the estimates. In R, the simputation package (van der Loo 2019) has implemented various imputation techniques including: group-wise median imputation, model-based with linear regression, random forests, etc. The following code snippet (complete code is in preprocessing.R) uses the impute_lm() method to impute the missing values in the sheep data using linear regression. library(simputation) # Replace NaN with NAs. # Since missing values are represented as NaN, # first we need to replace them with NAs. # Code to replace NaN with NA was taken from Hong Ooi: # https://stackoverflow.com/questions/18142117/# # how-to-replace-nan-value-with-zero-in-a-huge-data-frame/18143097 is.nan.data.frame &lt;- function(x)do.call(cbind, lapply(x, is.nan)) df[is.nan(df)] &lt;- NA # Use simputation package to impute values. # The first 4 columns are removed since we # do not want to use them as predictor variables. imp_df &lt;- impute_lm(df[,-c(1:4)], cx + cy + cz + pressure ~ . - cx - cy - cz - pressure) # Print summary. summary(imp_df) Originally, the missing values are encoded as NaN but in order to use the simputation package functions, we need them as NA. First, NaNs are replaced with NA. The first argument of impute_lm() is a data frame and the second argument is a formula. We discard the first \\(4\\) variables of the data frame since we do not want to use them as predictors. The left-hand side of the formula (everything before the ~ symbol) specifies the variables we want to impute. The right-hand side specifies the variables used to build the linear models. The ‘.’ indicates that we want to use all variables while the ‘-’ is used to specify variables that we do not want to include. The vignettes8 of the package contain more detailed examples. The mean, median, etc. and the predictive models to infer missing values should be trained using data only from the train set to avoid information injection. 5.2 Smoothing Smoothing comprises a set of algorithms with the aim of highlighting patterns in the data or as a preprocessing step to clean the data and remove noise. These methods are widely used on timeseries data but also with spatio-temporal data such as images. With timeseries data, they are often used to emphasize long-term patterns and reduce short-term signal artifacts. For example, in Figure 5.59 a stock chart was smoothed using two methods: moving average and exponential moving average. The smoothed versions make it easier to spot the overall trend rather than focusing on short-term variations. Figure 5.5: Stock chart with two smoothed versions. One with moving average and the other one with an exponential moving average. Source: wikipedia. The most common smoothing method for timeseries is the simple moving average. With this method, the first element of the resulting smoothed series is computed by taking the average of the elements within a window of predefined size. The window’s position starts at the first element of the original series. The second element is computed in the same way but after moving the window one position to the right. Figure 5.6 shows this procedure on a series with \\(5\\) elements and a window size of size \\(3\\). After the third iteration, it is not possible to move the window one more step to the right while covering \\(3\\) elements since the end of the time series has been reached. Because of this, the smoothed series will have some missing values at the end. Specifically, it will have \\(w-1\\) fewer elements where \\(w\\) is the window size. A simple solution is to compute the average of the elements covered by the window even if they are less than the window size. Figure 5.6: Simple moving average step by step with window size = 3. In the previous example the average is taken from the elements to the right of the pointer. There is a variation called centered moving average in which the center point of the window has the same elements to the left and right (Figure 5.7). Note that with this version of moving average some values at the beginning and at the end will be empty. Also note that the window size should be odd. In practice, both versions produce very similar results. Figure 5.7: Centered moving average step by step with window size = 3. In the preprocessing.R script the function movingAvg() implements the simple moving average procedure. In the following code note that the output vector will have the same size as the original one but the last elements will contain NA values when the window can not be moved any longer to the right. movingAvg &lt;- function(x, w = 5){ # Applies moving average to x with a window of size w. n &lt;- length(x) # Total number of points. smoothedX &lt;- rep(NA, n) for(i in 1:(n-w+1)){ smoothedX[i] &lt;- mean(x[i:(i-1+w)]) } return(smoothedX) } We can apply this function to a segment of accelerometer data from the SHEEP AND GOATS data set. datapath &lt;- &quot;../Sheep/S1.csv&quot; df &lt;- read.csv(datapath) # Only select a subset of the whole series. dfsegment &lt;- df[df$timestamp_ms &lt; 6000,] x &lt;- dfsegment$ax # Compute simple moving average with a window of size 21. smoothed &lt;- movingAvg(x, w = 21) Figure 5.8 shows the result after plotting the original vector and the smoothed one. Here, we can see that many of the small peaks are not present anymore in the smoothed version. The window size is a parameter that needs to be defined by the user. If it is set too large some important information may be lost from the signal. Figure 5.8: Original time series and smoothed version using a moving average window of size 21 One of the disadvantages of this method is that the arithmetic mean is sensitive to noise. Instead of computing the mean, one can use the median which is more robust against outlier values. There also exist other derived methods (not covered here) such as weighted moving average and exponential moving average10 which assign more importance to data points closer to the central point in the window. Smoothing a signal before feature extraction is a common practice and is used to remove some of the unwanted noise. 5.3 Normalization Having variables on different scales can have an impact during learning and at inference time. Consider a study where the data was collected using a wristband that has a light sensor and an accelerometer. The measurement unit of the light sensor is lux whereas the accelerometer’s is \\(m/s^2\\). After inspecting the dataset, you realize that the min and max values of the light sensor are \\(0\\) and \\(155\\) respectively. The min and max values for the accelerometer are \\(-0.4\\) and \\(7.45\\) respectively. Why is this a problem? Well, several learning methods are based on distances such as \\(k\\)-nn, Nearest centroid, Dynamic Time Warping, etc. thus, distances will be more heavily affected by bigger scales. Furthermore, other methods like neural networks (covered in chapter 8) are also affected by different scales. They have a harder time learning their parameters (weights) when data is not normalized. On the other hand, some methods are not affected by different scales, for example, tree-based learners such as decision trees and random forests. Since most of the time you may want to try different methods, it is a good idea to normalize your predictor variables. A common normalization technique is to scale all the variables between \\(0\\) and \\(1\\). Suppose there is a numeric vector \\(x\\) that you want to normalize between \\(0\\) and \\(1\\). Let \\(max(x)\\) and \\(min(x)\\) be the maximum and minimum values of \\(x\\). The following can be used to normalize the \\(i^{th}\\) value of \\(x\\): \\[\\begin{equation} z_i = \\frac{x_i - min(x)}{max(x)-min(x)} \\end{equation}\\] where \\(z_i\\) is the new normalized \\(i^{th}\\) value, thus, the previous formula is applied to every value in \\(x\\). The \\(max(x)\\) and \\(min(x)\\) values are parameters learned from the data. Notice that if you will split your data into training and test sets the max and min values (the parameters) are learned only from the train set and then used to normalize both the train and test set. This is to avoid information injection (section 5.5). Be also aware that after the parameters are learned from the train set and once the model is deployed in production, it is likely that some input values will be ‘out of range’. If the train set is not very representative of what you will find in real life, some values will probably be smaller than the learned \\(min(x)\\) and some others will be greater than the learned \\(max(x)\\). Even if the train set is representative of the real-life phenomenon, there is nothing that will prevent some values to be out of range. A simple way to handle this is to truncate the values. In some cases, we do know what are the true minimum and maximum values. For example in image processing, images are usually represented as color intensities between \\(0\\) and \\(255\\). Here, we know that the min value will always be \\(0\\) and the max value will always be \\(255\\). Let’s see an example using the HOME TASKS dataset. The following code first loads the dataset and prints a summary of the first \\(4\\) variables. # Load home activities dataset. dataset &lt;- read.csv(file.path(datasets_path, &quot;home_tasks&quot;, &quot;sound_acc.csv&quot;), stringsAsFactors = T) # Check first 4 variables&#39; min and max values. summary(dataset[,1:4]) #&gt; label v1_mfcc1 v1_mfcc2 v1_mfcc3 #&gt; brush_teeth :180 Min. :103 Min. :-17.20 Min. :-20.90 #&gt; eat_chips :282 1st Qu.:115 1st Qu.: -8.14 1st Qu.: -7.95 #&gt; mop_floor :181 Median :120 Median : -3.97 Median : -4.83 #&gt; sweep :178 Mean :121 Mean : -4.50 Mean : -5.79 #&gt; type_on_keyboard:179 3rd Qu.:126 3rd Qu.: -1.30 3rd Qu.: -3.09 #&gt; wash_hands :180 Max. :141 Max. : 8.98 Max. : 3.27 #&gt; watch_tv :206 Since label is a categorical variable the class counts are printed. For the three remaining variables, we get some statistics including their min and max values. As we can see, the min value of v1_mfcc1 is very different from the min value of v1_mfcc2 and the same is true for the maximum values, thus, we want to have all variables between \\(0\\) and \\(1\\) if we want to use classification methods sensitive to different scales. Let’s assume we want to train a classifier with this data so we divide it into train and test sets: # Divide into 50/50% train and test set. set.seed(1234) folds &lt;- sample(2, nrow(dataset), replace = T) trainset &lt;- dataset[folds == 1,] testset &lt;- dataset[folds == 2,] Now we can define a function that normalizes every numeric or integer variable. If the variable is not numeric or integer it will skip them. The function will take as input a train set and a test set. The parameters (max and min) are learned from the train set and used to normalize both, the train and test sets. # Define a function to normalize the train and test set # based on the parameters learned from the train set. normalize &lt;- function(trainset, testset){ # Iterate columns for(i in 1:ncol(trainset)){ c &lt;- trainset[,i] # trainset column c2 &lt;- testset[,i] # testset column # Skip if the variable is not numeric or integer. if(class(c) != &quot;numeric&quot; &amp;&amp; class(c) != &quot;integer&quot;)next; # Learn the max value from the trainset&#39;s column. max &lt;- max(c, na.rm = T) # Learn the min value from the trainset&#39;s column. min &lt;- min(c, na.rm = T) # If all values are the same set it to max. if(max==min){ trainset[,i] &lt;- max testset[,i] &lt;- max } else{ # Normalize trainset&#39;s column. trainset[,i] &lt;- (c - min) / (max - min) # Truncate max values in testset. idxs &lt;- which(c2 &gt; max) if(length(idxs) &gt; 0){ c2[idxs] &lt;- max } # Truncate min values in testset. idxs &lt;- which(c2 &lt; min) if(length(idxs) &gt; 0){ c2[idxs] &lt;- min } # Normalize testset&#39;s column. testset[,i] &lt;- (c2 - min) / (max - min) } } return(list(train=trainset, test=testset)) } Now we can use the previous function to normalize the train and test sets. The function returns a list of two elements: a normalized train and test sets. # Call our function to normalize each set. normalizedData &lt;- normalize(trainset, testset) # Inspect the normalized train set. summary(normalizedData$train[,1:4]) #&gt; label v1_mfcc1 v1_mfcc2 v1_mfcc3 #&gt; brush_teeth : 88 Min. :0.000 Min. :0.000 Min. :0.000 #&gt; eat_chips :139 1st Qu.:0.350 1st Qu.:0.403 1st Qu.:0.527 #&gt; mop_floor : 91 Median :0.464 Median :0.590 Median :0.661 #&gt; sweep : 84 Mean :0.474 Mean :0.568 Mean :0.616 #&gt; type_on_keyboard: 94 3rd Qu.:0.613 3rd Qu.:0.721 3rd Qu.:0.730 #&gt; wash_hands :102 Max. :1.000 Max. :1.000 Max. :1.000 #&gt; watch_tv : 99 # Inspect the normalized test set. summary(normalizedData$test[,1:4]) #&gt; label v1_mfcc1 v1_mfcc2 v1_mfcc3 #&gt; brush_teeth : 92 Min. :0.0046 Min. :0.000 Min. :0.000 #&gt; eat_chips :143 1st Qu.:0.3160 1st Qu.:0.421 1st Qu.:0.500 #&gt; mop_floor : 90 Median :0.4421 Median :0.606 Median :0.644 #&gt; sweep : 94 Mean :0.4569 Mean :0.582 Mean :0.603 #&gt; type_on_keyboard: 85 3rd Qu.:0.5967 3rd Qu.:0.728 3rd Qu.:0.724 #&gt; wash_hands : 78 Max. :0.9801 Max. :1.000 Max. :1.000 #&gt; watch_tv :107 Now the variables on the train set are exactly between \\(0\\) and \\(1\\) for all numeric variables. For the test set, not all min values will be exactly \\(0\\) but a bit higher. Conversely, some max values will be lower than \\(1\\). This is because the test set may have a min value that is greater than the min value of the train set and a max value that is smaller than the max value of the train set. However, after normalization, all values are guaranteed to be within \\(0\\) and \\(1\\). 5.4 Imbalanced Classes Ideally, classes will be uniformly distributed, that is, there is approximately the same number of instances per class. In real-life (as always), this is not the case. And in many situations (more often than you may think), class counts are heavily skewed. When this happens the dataset is said to be imbalanced. Take as an example, bank transactions. Most of them will be normal whereas a small percent will be fraudulent. In the medical field this is very common. It is easier to collect samples from healthy individuals compared to samples from individuals with some rare conditions. For example, a database may have thousands of images from healthy tissue but just a dozen with signs of cancer. Of course, having just a few cases with diseases is a good thing for the world! but not for machine learning methods. This is because predictive models will try to learn their parameters such that the error is reduced and most of the time this error is based on accuracy, thus, the models will be biased towards making correct predictions for the majority classes (the ones with higher counts) while paying little attention to minority classes. This can be a problem because for some applications we may be more interested in detecting the minority classes (illegal transactions, cancer cases, etc.). Suppose a given database has \\(998\\) instances with class ‘no cancer’ and only \\(2\\) instances with class ‘cancer’. A trivial classifier that always predicts ‘no cancer’ will have an accuracy of \\(98.8\\%\\) but will not be able to detect any of the ‘cancer’ cases! So, what can we do? Collect more data from the minority class. In practice, this can be difficult, expensive, etc. or just impossible because the study was conducted a long time ago and it is no longer possible to replicate the context. Delete data from the majority class. Randomly discard instances from the majority class. In the previous example, we could discard \\(996\\) instances of type ‘no cancer’. The problem with this is that we end up having small datasets and it will not be enough to learn good predictive models. If you have a huge dataset then this can be an option but in practice, this is rarely the case and you have the risk of having underrepresented samples. Create synthetic data. One of the most common solutions is to create synthetic data from the minority classes. In the following sections two methods that do that will be discussed: random oversampling and SMOTE. Adapt your learning algorithm. Another option is to use an algorithm that takes into account class counts and weights them accordingly. This is called cost-sensitive classification. For example, the rpart() method to train decision trees has a weight parameter which can be used to assign more weight to minority classes. When training neural networks it is also possible to assign different weights to different classes. The following two subsections cover two techniques to create synthetic data. 5.4.1 Random Oversampling random-oversampling.Rmd This method consists of duplicating data points from the minority class. The following code will create an imbalanced dataset with \\(200\\) instances of class ‘class1’ and only \\(15\\) instances of class ‘class2’. set.seed(1234) # Create random data n1 &lt;- 200 # Number of points of majority class. n2 &lt;- 15 # Number of points of minority class. # Generate random values for class1. x &lt;- rnorm(mean = 0, sd = 0.5, n = n1) y &lt;- rnorm(mean = 0, sd = 1, n = n1) df1 &lt;- data.frame(label=rep(&quot;class1&quot;, n1), x=x, y=y, stringsAsFactors = T) # Generate random values for class2. x2 &lt;- rnorm(mean = 1.5, sd = 0.5, n = n2) y2 &lt;- rnorm(mean = 1.5, sd = 1, n = n2) df2 &lt;- data.frame(label=rep(&quot;class2&quot;, n2), x=x2, y=y2, stringsAsFactors = T) # This is our imbalanced dataset. imbalancedDf &lt;- rbind(df1, df2) # Print class counts. summary(imbalancedDf$label) #&gt; class1 class2 #&lt; 200 15 If we want to exactly balance the class counts we will need \\(185\\) instances of type ‘class2’. We can use our well known sample() function to pick \\(185\\) points from data frame df2 (which contains only instances of class ‘class2’) and stored them in new.points. Notice the replace = T parameter. This instructs the function to be allowed to pick repeated elements. Then, the new data points are appended to the imbalanced data set which now becomes balanced. # Generate new points from the minority class. new.points &lt;- df2[sample(nrow(df2), size = 185, replace = T),] # Add new points to the imbalanced dataset and save the # result in balancedDf. balancedDf &lt;- rbind(imbalancedDf, new.points) # Print class counts. summary(balancedDf$label) #&gt; class1 class2 #&gt; 200 200 The code associated with this chapter includes a shiny app11 random-oversampling.Rmd. Shiny apps are interactive web applications. This shiny app graphically demonstrates how random oversampling works. Figure 5.9 shows the shiny app. The user can move the slider to generate new data points. Please note that the boundaries do not change as the number of instances increases (or decreases). This is because the new points are just duplicates so they overlap with existing ones. Figure 5.9: Shiny app with random oversampling example. It is a common mistake to generate synthetic data on the entire dataset before splitting into train and test sets. This will cause your model to be highly overfitted since several duplicate data points can end up in both sets. Create synthetic data only from the train set. Random oversampling is simple and effective in many cases. A potential problem is that the models can overfit since there are many duplicate data points. To overcome this, the following method (SMOTE) creates entirely new instances instead of duplicating them. 5.4.2 SMOTE auxiliary_functions/functions.R, smote-oversampling.Rmd The Synthetic Minority Oversampling Technique (SMOTE) (Chawla et al. 2002) can also be used to generate more data points from the minority class. One of the limitations of random oversampling is that it creates duplicates. This has the effect of having fixed boundaries and the classifiers can overspecialize. To avoid this, SMOTE creates entirely new data points. SMOTE operates on the feature space (on the predictor variables). To generate a new point, take the difference between a given point \\(a\\) (taken from the minority class) and one of its randomly selected nearest neighbors \\(b\\). The difference is multiplied by a random number between \\(0\\) and \\(1\\) and added to \\(a\\). This has the effect of selecting a point along the line between \\(a\\) and \\(b\\). Figure 5.10 illustrates the procedure of generating a new point in two dimensions. Figure 5.10: Synthetic point generation. The number of nearest neighbors \\(k\\) is a parameter defined by the user. In their original work (Chawla et al. 2002), the authors set \\(k=5\\). Depending on how many new samples need to be generated, \\(k&#39;\\) neighbors are randomly selected from the original \\(k\\) nearest neighbors. For example, if \\(200\\%\\) oversampling is needed, \\(k&#39;=2\\) neighbors are selected at random out of the \\(k=5\\) and one data point is generated with each of them. This is performed for each data point in the minority class. An implementation of SMOTE is provided in auxiliary_functions/functions.R. An example of how to apply it can be found in preprocessing.R in the corresponding directory of this chapter’s code. The smote.class(completeDf, targetClass, N, k) function has several arguments. The first one is the data frame that contains the minority and majority class, that is, the complete dataset. The second argument is the minority class label. The third argument N is the percent of smote and the last one (k) is the number of nearest neighbors to consider. The following code shows how the function smote.class() can be used to generate new points from the imbalanced dataset that was introduced in the previous section ‘Random Oversampling’. Recall that it has \\(200\\) points of class ‘class1’ and \\(15\\) points of class ‘class2’. # To balance the dataset, we need to oversample 1200%. # This means that the method will create 12 * 15 new points. ceiling(180 / 15) * 100 #&gt; [1] 1200 # Percent to oversample. N &lt;- 1200 # Generate new data points. synthetic.points &lt;- smote.class(imbalancedDf, targetClass = &quot;class2&quot;, N = N, k = 5)$synthetic # Append the new points to the original dataset. smote.balancedDf &lt;- rbind(imbalancedDf, synthetic.points) # Print class counts. summary(smote.balancedDf$label) #&gt; class1 class2 #&gt; 200 195 The parameter N is set to \\(1200\\). This will create \\(12\\) new data points for every minority class instance (\\(15\\)). Thus, the method will return \\(180\\) instances. In this case, \\(k\\) is set to \\(5\\). Finally, the new points are appended to the imbalanced dataset having a total of \\(195\\) samples of class ‘class2’. Again, a shiny app is included with this chapter’s code. The shiny app shows an example of applying SMOTE. Figure 5.11 shows the distribution of points before SMOTE and after applying SMOTE. Note how the boundary of ‘class2’ changes after SMOTE. It slightly spans in all directions. This is particularly visible in the lower right corner. This boundary expansion is what allows the classifiers to generalize better as compared to training them using random oversampled data. Figure 5.11: Shiny app with SMOTE example. 5.5 Information Injection The purpose of dividing the data into train/validation/test sets is to be able to accurately estimate the generalization performance of a predictive model when it is presented with previously unseen data points. So, it is advisable to construct such set splits in a way that they are as independent as possible. Often, before training a model and generating predictions out of it, the data needs to be preprocessed. Preprocessing operations may include imputing missing values, normalizing, and so on. During those operations, some information can be inadvertently transferred from the train to the test set thus, violating the assumption that they are independent. Information injection occurs when information from the train set is transferred to the test set. When having train/validation/test sets information injection occurs when information from the train set leaks into the validation and/or test set. It also happens when information from the validation set is transferred to the test set. Suppose that as one of the preprocessing steps, you need to subtract the mean value for each feature for a given instance. For now, suppose a dataset has a single feature \\(x\\) of numeric type and a categorical response variable \\(y\\). The dataset has \\(n\\) rows. As a preprocessing step, you decide that you need to subtract the mean of \\(x\\) from each data point. Since you want to predict \\(y\\) given \\(x\\), you train a classifier by splitting your data into train and test sets as usual. So you proceed with the steps depicted in Figure 5.12. Figure 5.12: Information injection example. First (a), you compute the \\(mean\\) value of the of variable \\(x\\) from the entire dataset. This \\(mean\\) is known as the parameter. In this case, there is only one parameter but there could be several. For example, we could additionally need to know the standard deviation. Once we know the mean value, the dataset is divided into train and test sets (b). Finally, the \\(mean\\) is subtracted from each element in the train and test sets (c). Without realizing, we have transferred information from the train set to the test set! But, how did this happen? Well, the mean parameter was computed using information from the entire dataset. Then, that \\(mean\\) parameter was used on the test set but it was calculated using data points that also belong to that same test set!. Figure 5.13 shows how to correctly do the preprocessing to avoid information injection. The dataset is first split (a). Then, the \\(mean\\) parameter is calculated only with data points from the train set. Finally, the mean parameter is subtracted from both sets. Here, the mean contains information only from the train set. Figure 5.13: No information injection example. In the previous example, we assumed that the dataset was split into one train and one test set only once. The same idea applies when performing \\(k\\)-fold cross-validation. In each of the \\(k\\) iterations, the preprocessing parameters need to be learned only from the train split. 5.6 One-hot Encoding Several algorithms need some or all input variables to be in numeric format -either the response and/or predictor variables. In R, for most classification algorithms, the class is usually encoded as a factor but some implementations may require it to be in numeric format. Sometimes we may have some categorical variables as predictors such as gender (‘male’, ‘female’). Some algorithms need those to be in numeric format because they, for example, are based on distance computations such as \\(k\\)-nn or Nearest neighbors. Some other models need to perform arithmetic operations on the predictor variables like neural networks. One way to convert categorical variables into numeric ones is called one-hot encoding and it works by creating new variables sometimes called dummy variables which are boolean, one for each possible category. Suppose a dataset has a categorical variable Job (Figure 5.14) with three possible values: programmer, teacher and dentist. This variable can be one-hot encoded by creating \\(3\\) new boolean dummy variables and setting them to \\(1\\) for the corresponding category and \\(0\\) for the rest. Figure 5.14: One-hot encoding example You should be aware of the dummy variable trap which means that one variable can be predicted from the others. For example, if the possible values are just male and female, then if the dummy variable for male is \\(1\\) we know that the dummy variable for female must be \\(0\\). The solution to this is to drop one of the newly created variables. Which one? Any variable can be dropped, it does not matter which one. This trap only applies when the variable is a predictor. If it is a response variable, nothing should be dropped. Figure 5.15 shows a guideline about how to convert non-numeric variables into numeric ones for classification tasks. This is only a guideline and the process will depend on each application. Figure 5.15: Variable conversion. The caret package has a function dummyVars() that can be used to one-hot encode the categorical variables of a data frame. Since the STUDENTS’ MENTAL HEALTH dataset (Nguyen et al. 2019) has several categorical variables, it can be used to demonstrate how to apply dummyVars(). This dataset collected at a University in Japan contains survey responses from students about their mental health and help-seeking behaviors. We begin by loading the data. # Load students mental health behavior dataset. # stringsAsFactors is set to F since the function # that we will use to one-hot encode expects characters. dataset &lt;- read.csv(file.path(datasets_path, &quot;students_mental_health&quot;, &quot;data.csv&quot;), stringsAsFactors = F) Note that the stringsAsFactors parameter is set to FALSE. We need this because dummyVars() needs characters to work properly. Before one-hot encoding the variables we need to do some preprocessing to clean the dataset. This dataset contains several fields with empty characters ‘\"\"’. Thus, we will replace them with NA with the replace_with_na_all() function from the naniar package. This package was first described in the missing values section of this chapter but that function was not discussed. The function takes as first argument the dataset and the second argument is a formula that includes a condition. # The dataset contains several empty strings. # Replace those empty strings with NAs so the following # methods will work properly. # We can use the replace_with_na_all() function # from naniar package to do the replacement. library(naniar) dataset &lt;- replace_with_na_all(dataset, ~.x %in% common_na_strings) In this case, the condition is ~.x %in% common_na_strings which means: replace all fields that contain one of the characters in common_na_strings. The variable common_na_strings contains a set of common strings that can be regarded as missing values for example ‘“NA”’, ‘“na”’, ‘“NULL”’, the empty string ‘\"\"’ and so on. Now, we can use the vis_miss() function described in the missing values section to get a visual idea of the missing values. # Visualize missing values. vis_miss(dataset, warn_large_data = F) Figure 5.16: Missing values in the students mental health dataset. Figure 5.16 shows the output plot. We can see that the last rows contain many missing values so we will discard them and only keep the first rows (\\(1-268\\)). # Since the last rows starting at 269 # are full of missing values we will discard them. dataset &lt;- dataset[1:268,] As an example, we will one-hot encode the Stay_Cate variable which represents how long a student has been at the university: 1 year (Short), 2–3 years (Medium), or at least 4 years (Long). The dummyVars() function takes a formula as its first argument. Here, we specify that we only want to convert Stay_Cate. This function does not do the actual encoding but returns an object that is used with predict() to obtain the encoded variable(s) as a new data frame. # One-hot encode the Stay_Cate variable. # This variable Stay_Cate has three possible # values: Long, Short and Medium. # First, create a dummyVars object with the dummyVars() #function from caret package. library(caret) dummyObj &lt;- dummyVars( ~ Stay_Cate, data = dataset) # Perform the actual encoding using predict() encodedVars &lt;- data.frame(predict(dummyObj, newdata = dataset)) Figure 5.17: One-hot encoded Stay Cate. If we inspect the resulting data frame we see that it has \\(3\\) variables, one for each possible value: Long, Medium and Short. If this variable is going to be used as a predictor variable, we should delete one of its columns to avoid the dummy variable trap. We can do this by setting the parameter fullRank = TRUE. dummyObj &lt;- dummyVars( ~ Stay_Cate, data = dataset, fullRank = TRUE) encodedVars &lt;- data.frame(predict(dummyObj, newdata = dataset)) Figure 5.18: One-hot encoded Stay Cate dropping one of the columns. In this situation, the column with ‘Long’ was discarded. If you want to one-hot encode all variables at once you can use ~ . as the formula. But be aware that the dataset may have some categories encoded as numeric and thus will not be transformed. For example, the Age_cate encodes age categories but the categories are represented as integers from \\(1\\) to \\(5\\). In this case, it may be ok not to encode this variable since lower integer numbers also imply smaller ages and bigger integer numbers represent older ages. If you still want to encode this variable you could first convert it to character by appending a letter at the beginning. Sometimes you should encode a variable, for example, if it represents colors. In that situation, it does not make sense to leave it as numeric since there is not semantic order between colors. Actually, in some very rare situations, it would make sense to leave color categories as integers. For example, if they represent a gradient like white, light blue, blue, dark blue and black in which case this could be treated as an ordinal variable. 5.7 Summary Programming functions that train predictive models expect the data to be in a particular format. Furthermore, some methods make assumptions about the data like having no missing values, having all variables in the same scale and so on. This chapter presented several commonly used methods to preprocess our datasets before using them to train models. When collecting data from different sensors, we can face several sources of variation like sensor’ format, different sampling rates, different scales, and so on. Some preprocessing methods can lead to information injection. This happens when some information from the train set is leaked to the test set. Missing values is a common problem in many data analysis tasks. In R, the naniar package can be used to spot missing values. Imputation is the process of inferring the values of missing values. The simputation package can be used to impute missing values in datasets. Normalization is the process of transforming a set of variables to a common scale. For example from \\(0\\) to \\(1\\). An imbalanced dataset has a disproportionate number of classes of a certain type with respect to the others. Some methods like random over/under sampling and SMOTE can be used to balance a dataset. One-hot-encoding is a method that converts categorical variables into numeric ones. References "],
["unsupervised.html", "Chapter 6 Discovering Behaviors with Unsupervised Learning 6.1 K-means clustering 6.2 The Silhouette Index 6.3 Mining Association Rules 6.4 Summary", " Chapter 6 Discovering Behaviors with Unsupervised Learning So far, we have been working with supervised learning methods, that is, models for which the training instances have two elements: 1) a set of input values (features) and 2) the expected output (label). As mentioned in chapter 1, there are other types of machine learning methods and one of those is unsupervised learning which is the topic of this chapter. In unsupervised learning the training instances do not have a response variable (e.g., a label). Thus, the objective is to extract knowledge from the available data without any type of guidance (supervision). For example, given a set of variables that characterize a person, we would like to find groups of people with similar behaviors. For physical activity behaviors, this could be done by finding groups of very active people versus finding groups of people with low physical activity. Those groups can be useful for delivering appropriate suggestions or services to each group, thus, enhancing and personalizing the user experience. This chapter starts with one of the most popular unsupervised learning algorithms: k-means clustering. Then, an example of how this technique can be applied to find groups of students with similar characteristics is presented. Then, association rules are presented which is another type of unsupervised learning. Finally, association rules are used to find criminal patterns in a database. 6.1 K-means clustering kmeans_steps.R This is one of the most commonly used unsupervised methods due to its simplicity and efficacy. Its objective is to find groups of points such that points in the same group are similar and points from different groups are as dissimilar as possible. The number of groups \\(k\\) needs to be defined a priori. The method is based on computing distances to centroids. The centroid of a set of points is computed by taking the mean of each of its features. The k-means algorithm is as follows: Generate k centroids at random. Repeat until no change or max iterations: Assign each data point to the closest centroid. Update centroids. To measure the distance between a data point and a centroid, the Euclidean distance is typically used but other distances can be used as well depending on the application. As an example let’s cluster user responses from the STUDENTS’ MENTAL HEALTH dataset. To demonstrate how k-means work, we will choose only two variables so we can plot the results. The variables are ToAS (total acculturative stress) and ToSC (total social connectedness). \\(k\\) is set to \\(3\\), that is, we want to group the points into \\(3\\) disjoint groups. The code that implements the k-means algorithm can be found in the script kmeans_steps.R. The algorithm begins by selecting \\(3\\) centroids at random. Figure 6.1 shows a scatterplot of ToAS and ToSC along with the random centroids. Figure 6.1: 3 centroids chosen randomly. Next, at the first iteration, each point is assigned to the closest centroid. This is depicted in Figure 6.2 (top left). Then, the centroids are updated (moved) based on the new assignments. In the next iteration, the points are reassigned to the closest centroids and so on. Figure 6.2 shows the first \\(4\\) iterations of the algorithm. Figure 6.2: First 4 iterations of k-means. From iteration \\(1\\) to \\(2\\) the centroids moved considerably. After that, they began to stabilize. Formally, the algorithm tries to minimize the total within cluster variation of all clusters. The cluster variation of a single cluster \\(C_k\\) is defined as: \\[\\begin{equation} W(C_k) = \\sum_{x_i \\in C_k}{(x_i - \\mu_k)^2} \\tag{6.1} \\end{equation}\\] where \\(x_i\\) is a data point and \\(\\mu_k\\) is the centroid of cluster \\(C_k\\). Thus, the total within cluster variation \\(TWCV\\) is: \\[\\begin{equation} TWCV = \\sum_{i=1}^k{W(C_i)} \\tag{6.2} \\end{equation}\\] that is, the sum of all within-cluster variations across all clusters. The objective is to find the \\(\\mu_k\\) centroids that make \\(TWCV\\) minimal. Finding the global optimum is a difficult problem. However, the iterative algorithm described above often produces good approximations. 6.1.1 Grouping Student Responses group_students.R In the previous example, we only used two variables to perform the clustering. Let’s now use more variables from the STUDENTS’ MENTAL HEALTH dataset to find groups. The full code can be found in group_students.R. After removing missing values, one-hot encoding categorical variables, and some additional cleaning, the following \\(10\\) variables were selected: # Select which variables are going to be used for clustering. selvars &lt;- c(&quot;Stay&quot;,&quot;English_cate&quot;,&quot;Intimate&quot; ,&quot;APD&quot;,&quot;AHome&quot;,&quot;APH&quot;,&quot;Afear&quot;, &quot;ACS&quot;,&quot;AGuilt&quot;,&quot;ToAS&quot;) Additionally, it is advisable to normalize the data between \\(0\\) and \\(1\\) since we are dealing with distance computations and we want to put the same weight on each variable. To plot the \\(10\\) variables, we can use MDS (described in chapter 4) to project the data into \\(2\\) dimensions (Figure 6.3). Figure 6.3: Students responses projected into 2D with MDS. Visually, it seems that there are \\(4\\) distinct groups of points. Based on this initial guess, we can set \\(k=4\\) and use the kmeans() function included in base R to find the groups automatically. clusters &lt;- kmeans(normdf, 4) The first argument of kmeans() is a data frame or a matrix and the second argument the number of clusters. Figure 6.4 shows the resulting clustering. The kmeans() method returns an object that contains several components including cluster that stores the assigned cluster for each data point and centers that stores the centroids. Figure 6.4: Students responses groups when k=4. The k-means algorithm found the same clusters as we would intuitively expect. We can check how different the groups are by inspecting some of the variables. For example, by plotting a boxplot of the Intimate variable (Figure 6.5). This variable is \\(1\\) if the student has an intimate partner or \\(0\\) otherwise. Since there are only two possible values the boxplot looks flat. This shows that cluster_1 and cluster_3 are different from cluster_2 and cluster_3. Figure 6.5: Boxplot of Intimate variable. Additionally, let’s plot the ACS variable which represents the total score of culture shock. This one has a minimum value of \\(3\\) and a max value of \\(13\\). Figure 6.6: Boxplot of ACS variable. cluster_2 and cluster_4 were similar based on the Intimate variable but if we take a look at the difference in medians based on ACS, cluster_2 and cluster_4 are the most dissimilar clusters which gives an intuitive idea of why those two clusters are different. So far, the number of groups \\(k\\) has been chosen arbitrarily or by visual inspection. But, is there an automatic way to select the best k? As always… this depends on the task at hand but there is a method called Silhouette index that can be used to select the optimal \\(k\\) based on an optimality criterion. This index is presented in the next section. 6.2 The Silhouette Index As opposed to supervised learning, in unsupervised learning there is no ground truth to validate the results. In clustering, one way to validate the resulting groups is to plot them and explore the clusters’ data points and look for similarities and/or differences. But sometimes we may also want to automate the process and have a quantitative way to measure how well the clustering algorithm grouped the points with the given set of parameters. If we had such a method we could do parameter optimization, for example, to find the best \\(k\\). Well, there is something called the silhouette index (Rousseeuw 1987) and it can be used to measure the correctness of the clusters. This index is computed for each data point and tells us how well they are clustered. The total silhouette index is the mean of all points’ indices and gives us an idea of how well the points were clustered overall. This index goes from \\(-1\\) to \\(1\\) and I’ll explain in a moment how to interpret it but first let’s see how it is computed. To compute the silhouette index two things are needed: the currently created groups and the distances between points. Let: \\(a(i)=\\) average dissimilarity (distance) of point \\(i\\) to all other points in \\(A\\), where \\(A\\) is the cluster to which \\(i\\) has been assigned to (Figure 6.7). \\(d(i,C)=\\) average dissimilarity between \\(i\\) and all points in some cluster \\(C\\). \\(b(i)=\\min_{C \\neq A}d(i,C)\\). The cluster \\(B\\) for which the minimum is attained is the neighbor of point \\(i\\). (The second best choice for \\(i\\)). Figure 6.7: 3 resulting clusters: A, B and C. Thus, \\(s(i)\\) (the silhouette index of point \\(i\\)) is obtained with the formula: \\[\\begin{equation} s(i) = \\frac{{b(i) - a(i)}}{{\\max \\{ a(i),b(i)\\} }} \\tag{6.3} \\end{equation}\\] When \\(s(i)\\) is close to \\(1\\) it means that the within dissimilarity \\(a(i)\\) is much smaller than the smallest between dissimilarity \\(b(i)\\) thus, \\(i\\) can be considered to be well clustered. When \\(s(i)\\) is close to \\(0\\) it is not clear whether \\(i\\) belongs to \\(A\\) or \\(B\\). When \\(s(i)\\) is close to \\(-1\\), \\(a(i)\\) is larger than \\(b(i)\\) meaning that \\(i\\) may have been misclassified. The total silhouette index \\(S\\) is the average of all indices \\(s(i)\\) of all points. In R, the cluster package has the function silhouette() that computes the silhouette index. The following code snippet clusters the student responses into \\(4\\) groups and computes the index of each point with the silhouette() function. Its first argument is the cluster assignments as returned by kmeans(), and the second argument is a dist object that contains the distances between each pair of points. We can compute this information from our data frame with the dist() function. The silhouette() function returns an object with the silhouette index of each data point. We can compute the total index by taking the average which in this case was \\(0.346\\). library(cluster) # Load the required package. set.seed(1234) clusters &lt;- kmeans(normdf, 4) # Try with k=4 # Compute silhouette indices for all points. si &lt;- silhouette(clusters$cluster, dist(normdf)) # Print first rows. head(si) #&gt; cluster neighbor sil_width #&gt; [1,] 1 4 0.3482364 #&gt; [2,] 2 4 0.3718735 #&gt; [3,] 3 1 0.3322198 #&gt; [4,] 1 4 0.3998996 #&gt; [5,] 1 4 0.3662811 #&gt; [6,] 3 1 0.1463607 # Compute total Silhouette index by averaging the individual indices. mean(si[,3]) # [1] 0.3466427 One nice thing about this index is that it can be visually interpreted. To generate a silhouette plot, use the generic plot() function and pass the object returned by silhouette(). plot(si, cex.names=0.6, col = 1:4, main = &quot;Silhouette plot, k=4&quot;, border=NA) Figure 6.8: Silhouette plot when k=4. Figure 6.8 shows the silhouette plot when \\(k=4\\). The horizontal lines represent the individual silhouette indices. In this plot, all of them are positive. The height of each cluster gives a visual idea of the number of data points contained in it with respect to other clusters. We can see for example that cluster \\(2\\) is the smallest one. On the right side, it’s the number of points in each cluster and their average silhouette index. At the bottom, the total silhouette index is printed (\\(0.35\\)). We can try to cluster the points into \\(7\\) groups instead of \\(4\\) and see what happens. set.seed(1234) clusters &lt;- kmeans(normdf, 7) si &lt;- silhouette(clusters$cluster, dist(normdf)) plot(si, cex.names=0.6, col = 1:7, main = &quot;Silhouette plot, k=7&quot;, border=NA) Figure 6.9: Silhouette plot when k=7. Here, cluster \\(2\\) and \\(4\\) have data points with negative indices and the overall score is \\(0.26\\). This suggests that \\(k=4\\) produces more coherent clusters as compared to \\(k=7\\). In this section, we used the Silhouette index to validate the clustering results. Over the years, several other clustering validation methods have been developed. In their paper, Halkidi, Batistakis, and Vazirgiannis (2001) present an overview of other clustering validation methods. 6.3 Mining Association Rules Association rule mining consists of a set of methods to extract patterns (rules) from transactional data. For example, shopping behavior can be analyzed by finding rules from customers’ shopping transactions. A transaction is an event that involves a set of items. For example, when someone buys a soda, a bag of chips, and a chocolate bar the purchase is registered as one transaction containing \\(3\\) items. I apologize for using this example for those of you with healthy diets. Based on a database that contains many transactions, it is possible to uncover item relationships. Those relationships are usually expressed as implication rules of the form \\(X \\implies Y\\) where \\(X\\) and \\(Y\\) are sets of items. Both sets are disjoint, this means that items in \\(X\\) are not in \\(Y\\) and vice-versa which can be formally represented as \\(X \\cap Y = \\emptyset\\), that is, the intersection of the two sets is the empty set. \\(X \\implies Y\\) is read as: if \\(X\\) then \\(Y\\). The left-hand-side (lhs) \\(X\\) is called the antecedent and the right-hand-side (rhs) \\(Y\\) is called the consequent. In the unhealthy supermarket example, a rule like \\(\\{chips, chocolate\\} \\implies \\{soda\\}\\) can be interpreted as if someone buys chips and chocolate then, it is likely that this same person will also buy soda. These types of rules can be used for targeted advertisements, product placement decisions, etc. The possible number of rules that can be generated grows exponentially as the number of items increases. Furthermore, not all rules may be interesting. The most well-known algorithm to find interesting association rules is called Apriori (Agrawal and Srikant 1994). To quantify if a rule is interesting or not, this algorithm uses two importance measures: support and confidence. Support. The support \\(\\text{supp}(X)\\) of an itemset \\(X\\) is the proportion of transactions that contain all the items in \\(X\\). This quantifies how frequent the itemset is. Confidence. The confidence of a rule \\(\\text{conf}(X \\implies Y)=\\text{supp}(X \\cup Y)/\\text{supp}(X)\\) and can be interpreted as the probability that \\(Y\\) occurs given that \\(X\\) is present. This can also be thought of as the probability that a transaction that contains \\(X\\) also contains \\(Y\\). The \\(\\cup\\) operator is the union of two sets. This means taking all elements from both sets and removing repeated elements. Now that we have a way to measure the rules’ importance, the Apriori algorithm first finds itemsets that satisfy a minimum support and generates rules from those itemsets that satisfy a minimum confidence. Those minimum thresholds are set by the user. The lower the thresholds, the more rules returned by the algorithm. One thing to note is that Apriori only generates rules with itemsets of size \\(1\\) on the right-hand side. Another common metric to measure the importance of a rule is the lift. Lift is typically used after Apriori has generated the rules to further filter and/or rank the results. Lift. The lift of a rule \\(\\text{lift}(X \\implies Y) = \\text{supp}(X \\cup Y) / (\\text{supp}(X)\\text{supp}(Y))\\) is similar to the confidence but it also takes into consideration the frequency of \\(Y\\). A lift of \\(1\\) means that there is no association between \\(X\\) and \\(Y\\). A lift greater than \\(1\\) means that \\(Y\\) is likely to occur if \\(X\\) occurs and a lift less than \\(1\\) means that \\(Y\\) is unlikely to occur when \\(X\\) occurs. Let’s compute all those metrics using an example. The following table shows an example database of transactions. Figure 6.10: Example database with 10 transactions. The support of the itemset consisting of a single item ‘chocolate’ is \\(\\text{supp}(\\{chocolate\\}) = 5/10 = 0.5\\) because ‘chocolate’ appears in \\(5\\) out of the \\(10\\) transactions. The support of \\(\\{chips, soda\\}\\) is \\(3/10 = 0.3\\). For example, the confidence of the rule \\(\\{chocolate, chips\\} \\implies \\{soda\\}\\) is: \\[\\begin{align*} \\text{conf}(\\{chocolate, chips\\} \\implies \\{soda\\})&amp;=\\frac{\\text{supp}(\\{chocolate, chips, soda\\})}{\\text{supp}(\\{chocolate,chips\\})} \\\\ &amp;=(2/10) / (3/10) \\\\ &amp;=0.66 \\end{align*}\\] The lift of \\(\\{soda\\} \\implies \\{ice cream\\}\\) is: \\[\\begin{align*} \\text{lift}(\\{soda\\} \\implies \\{ice cream\\})&amp;=\\frac{\\text{supp}(\\{soda, ice cream\\})}{\\text{supp}(\\{soda\\})\\text{supp}(\\{ice cream\\})} \\\\ &amp;=(2/10) / ((7/10)(3/10)) \\\\ &amp;=0.95 \\end{align*}\\] Association rules mining is unsupervised in the sense that there are no labels or ground truth. Many applications of association rules are targeted to market basket analysis to gain insights into shoppers’ behavior and take actions to increase sales. To find such rules it is necessary to have ‘transactions’ (sets of items), for example, supermarket products. However, this is not the only application of association rules. There are other problems that can be structured as transactions of items. For example in medicine, diseases can be seen as transactions and symptoms as items. Thus, one can apply association rules algorithms to find symptoms and disease relationships. Another application is in recommender systems. Take, for example, movies. Transactions can be the set of movies watched by every user. If you watched a movie \\(m\\) then, the recommender system can suggest another movie that co-occurred frequently with \\(m\\) and that you have not watched yet. Even other types of relational data can be transformed into transaction-like structures to find patterns and this is precisely what we are going to do in the next section to mine criminal patterns. 6.3.1 Finding Rules for Criminal Behavior crimes_process.R crimes_rules.R In this section, we will use association rules mining to find patterns in the HOMICIDE REPORTS12 dataset. This database contains homicide reports from 1980 to 2014 in the United States. The database is structured as a table with \\(24\\) columns and \\(638,454\\) rows. Each row corresponds to a homicide report that includes city, state, year, month, the sex of the victim, sex of the perpetrator, if the crime was solved or not, weapon used, age of the victim and perpetrator, the relationship type between the victim and the perpetrator and some other information. Before trying to find rules, the data needs to be preprocessed and converted into transactions. Each homicide report will be a transaction and the items will be the possible values of \\(3\\) of the columns: Relationship, Weapon, and Perpetrator.Age. The Relationship variable can take values like Stranger, Neighbor, Friend, etc. In total, there are \\(28\\) possible relationship values including Unknown. For the purpose of our analysis, we will remove rows with unknown values in Relationship and Weapon. Since Perpetrator.Age is an integer, we need to convert it into categories. The following age groups are created: child (&lt; \\(13\\) years), teen (\\(13\\) to \\(17\\) years), adult (\\(18\\) to \\(45\\) years) and lateAdulthood (&gt; \\(45\\) years). After these cleaning and preprocessing steps, the dataset has \\(3\\) columns and \\(328,238\\) rows. See Figure 6.11. The script used to perform the preprocessing is crimes_process.R. Figure 6.11: First rows of preprocessed crimes data frame. Now, we have a data frame that contains only the information that we will use and each row will be used to generate one transaction. An example transaction may be {R.Wife, Knife, Adult}. This one represents the case where the perpetrator is an adult who used a knife to kill his wife. Note the ‘R.’ at the beginning of ‘Wife’. This ‘R.’ was added for clarity in order to identify that this item is a relationship. One thing to note is that every transaction will consist of exactly \\(3\\) items. This is a bit different than the market basket case in which every transaction can include a varying number of products. Although this item-size constraint was a design decision based on the structure of the original data, this will not prevent us from performing the analysis to find interesting rules. To find the association rules, the arules package (Hahsler et al. 2019) will be used. This package has an interface to an efficient implementation in C of the Apriori algorithm. This package needs the transactions to be stored as an object of type ‘transactions’. One way to create this object is to use a logical matrix and cast it into a transactions object. The rows of the logical matrix represent transactions and columns represent items. The number of columns equals the total number of possible items with a TRUE value if the item is present in the transaction and FALSE otherwise. In our case, the matrix has \\(46\\) columns. The crimes_process.R script has code to generate this matrix M. The \\(46\\) items (columns of M) are: as.character(colnames(M)) #&gt; [1] &quot;R.Acquaintance&quot; &quot;R.Wife&quot; &quot;R.Stranger&quot; #&gt; [4] &quot;R.Girlfriend&quot; &quot;R.Ex-Husband&quot; &quot;R.Brother&quot; #&gt; [7] &quot;R.Stepdaughter&quot; &quot;R.Husband&quot; &quot;R.Friend&quot; #&gt; [10] &quot;R.Family&quot; &quot;R.Neighbor&quot; &quot;R.Father&quot; #&gt; [13] &quot;R.In-Law&quot; &quot;R.Son&quot; &quot;R.Ex-Wife&quot; #&gt; [16] &quot;R.Boyfriend&quot; &quot;R.Mother&quot; &quot;R.Sister&quot; #&gt; [19] &quot;R.Common-Law Husband&quot; &quot;R.Common-Law Wife&quot; &quot;R.Stepfather&quot; #&gt; [22] &quot;R.Stepson&quot; &quot;R.Stepmother&quot; &quot;R.Daughter&quot; #&gt; [25] &quot;R.Boyfriend/Girlfriend&quot; &quot;R.Employer&quot; &quot;R.Employee&quot; #&gt; [28] &quot;Blunt Object&quot; &quot;Strangulation&quot; &quot;Rifle&quot; #&gt; [31] &quot;Knife&quot; &quot;Shotgun&quot; &quot;Handgun&quot; #&gt; [34] &quot;Drowning&quot; &quot;Firearm&quot; &quot;Suffocation&quot; #&gt; [37] &quot;Fire&quot; &quot;Drugs&quot; &quot;Explosives&quot; #&gt; [40] &quot;Fall&quot; &quot;Gun&quot; &quot;Poison&quot; #&gt; [43] &quot;teen&quot; &quot;adult&quot; &quot;lateAdulthood&quot; #&gt; [46] &quot;child&quot; The following snippet shows how to convert the matrix into an arules transactions object. Before the conversion, the package arules needs to be loaded. Then the transactions are saved in a file transactions.RData. library(arules) # Convert into a transactions object. transactions &lt;- as(M, &quot;transactions&quot;) # Save transactions file. save(transactions, file=&quot;transactions.RData&quot;) Now that the database is in the required format we can start the analysis. The crimes_rules.R script has the code to perform the analysis. First, the transactions file that we generated before is loaded: library(arules) library(arulesViz) # Load preprocessed data. load(&quot;transactions.RData&quot;) Note that additionally to the arules package, we also loaded the arulesViz package (Hahsler 2019). This package has several functions to generate cool plots of the learned rules! A summary of the transactions can be printed with the summary() function: # Print summary. summary(transactions) #&gt; transactions as itemMatrix in sparse format with #&gt; 328238 rows (elements/itemsets/transactions) and #&gt; 46 columns (items) and a density of 0.06521739 #&gt; #&gt; most frequent items: #&gt; adult Handgun R.Acquaintance R.Stranger #&gt; 257026 160586 117305 77725 #&gt; Knife (Other) #&gt; 61936 310136 #&gt; #&gt; element (itemset/transaction) length distribution: #&gt; sizes #&gt; 3 #&gt; 328238 #&gt; #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 3 3 3 3 3 3 #&gt; #&gt; includes extended item information - examples: #&gt; labels #&gt; Relationship1 R.Acquaintance #&gt; Relationship2 R.Wife #&gt; Relationship3 R.Stranger The summary shows the total number of rows (transactions) and the number of columns. It also prints the most frequent items, in this case, adult with \\(257026\\) occurrences, Handgun with \\(160586\\), and so on. The itemset sizes are also displayed. Here, all itemsets have a size of \\(3\\) (by design). Some other summary statistics are also printed. We can use the itemFrequencyPlot() function from the arulesViz package to plot the frequency of items. itemFrequencyPlot(transactions, type = &quot;relative&quot;, topN = 15, main = &#39;Item frequecies&#39;) The type argument specifies that we want to plot the relative frequencies. Use \"absolute\" instead to plot the total counts. topN is used to select how many items are plotted. Figure 6.12 shows the output. Figure 6.12: Frequences of the top 15 items. Now it is time to find some interesting rules! This can be done with the apriori() function as follows: # Run apriori algorithm. resrules &lt;- apriori(transactions, parameter = list(support = 0.001, confidence = 0.5, # Find rules with at least 2 items. minlen = 2, target = &#39;rules&#39;)) The first argument is the transactions object. The second argument parameter specifies a list of algorithm parameters. In this case we want rules with a minimum support of \\(0.001\\) a confidence of at least \\(0.5\\). The minlen argument specifies the minimum number of allowed items in a rule for the antecedent + consequent. We set it to \\(2\\) since we want rules with at least one element in the antecedent and one element in the consequent, for example, {item1 =&gt; item2}. This Apriori algorithm creates rules with only one item in the consequent. Finally, the target parameter is used to specify that we want to find rules because the function can also return item sets of different types (see the documentation for more details). The returned rules are saved in the resrules variable that can be used later to explore the results. We can also print a summary of the returned results. # Print a summary of the results. summary(resrules) #&gt; set of 141 rules #&gt; #&gt; rule length distribution (lhs + rhs):sizes #&gt; 2 3 #&gt; 45 96 #&gt; #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 2.000 2.000 3.000 2.681 3.000 3.000 #&gt; #&gt; summary of quality measures: #&gt; support confidence lift count #&gt; Min. :0.001030 Min. :0.5045 Min. :0.6535 Min. : 338 #&gt; 1st Qu.:0.001767 1st Qu.:0.6478 1st Qu.:0.9158 1st Qu.: 580 #&gt; Median :0.004424 Median :0.7577 Median :1.0139 Median : 1452 #&gt; Mean :0.021271 Mean :0.7269 Mean :1.0906 Mean : 6982 #&gt; 3rd Qu.:0.012960 3rd Qu.:0.8131 3rd Qu.:1.0933 3rd Qu.: 4254 #&gt; Max. :0.376836 Max. :0.9539 Max. :4.2777 Max. :123692 #&gt; #&gt; mining info: #&gt; data ntransactions support confidence #&gt; transactions 328238 0.001 0.5 By looking at the summary, we see that the algorithm found \\(141\\) rules that satisfy the support and confidence thresholds. Then the rule length distribution is printed. Here, \\(45\\) rules are of size \\(2\\) and \\(96\\) rules are of size \\(3\\). Then, some standard statistics are shown for support, confidence, and lift. The inspect() function can be used to print the actual rules. Rules can be sorted by one of the importance measures. The following code sorts by lift and prints the first \\(20\\) rules. Figure 6.13 shows the output. # Print the first n (20) rules with highest lift in decreasing order. inspect(sort(resrules, by=&#39;lift&#39;, decreasing = T)[1:20]) Figure 6.13: Output of the inspect() function. The first rule with a lift of \\(4.27\\) says that if a homicide was committed by an adult and the victim was the stepson, then is it likely that a blunt object was used for the crime. By looking at the rules one can also note that whenever blunt object appears either in the lhs or rhs the victim was probably an infant. Another thing to note is that when the victim was boyfriend, the crime was likely committed with a knife. This is also mentioned in the reports “Homicide trends in the United States” (Cooper, Smith, and others 2012): From 1980 through 2008 “Boyfriends were more likely to be killed by knives than any other group of intimates.” According to rule \\(20\\), crimes involving girlfriend have a strong relationship with strangulation. This can also be confirmed in (Cooper, Smith, and others 2012): From 1980 through 2008 “Girlfriends were more likely to be killed by force…” The resulting rules can be plotted with the plot() function. By default, it generates a scatterplot with the support in the \\(x\\) axis and confidence in the \\(y\\) axis colored by lift. # Plot a default scatterplot of support v.s. confidence colored by lift. plot(resrules) Figure 6.14: Scatterplot of rules support v.s. confidence colored by lift. The plot shows that rules with a high lift also have a low support and confidence. Hahsler (2017) mentioned that rules with high lift typically have low support. The plot can be customized for example to show the support and lift in the axes and colors them by confidence. The axes can be set with the measure parameter and the coloring with the shading parameter. The function also supports different plotting engines including static and interactive. The following code generates a customized interactive plot by setting engine = \"htmlwidget\". This is very handy if you want to know which points correspond to which rules. By hovering the mouse on the desired point the corresponding rule is shown as a tooltip box (Figure 6.15). The interactive plots also allow to zoom in regions by clicking and dragging. # Customize scatterplot to make it interactive # and plot support v.s. lift colored by confidence. plot(resrules, engine = &quot;htmlwidget&quot;, measure = c(&quot;support&quot;, &quot;lift&quot;), shading = &quot;confidence&quot;) Figure 6.15: Interactive scatterplot of rules. The arulesViz package has a nice option to plot rules as a graph. This is done by setting method = \"graph\". We can also make the graph interactive for easier exploration by setting engine=\"htmlwidget\". For clarity, the font size is reduced with cex=0.9. Here we plot the first \\(25\\) rules. # Plot rules as a graph. plot(head(sort(resrules, by = &quot;lift&quot;), n=25), method = &quot;graph&quot;, control=list(cex=.9), engine=&quot;htmlwidget&quot;) Figure 6.16: Interactive graph of rules. Figure 6.16 shows a zoomed-in portion of the entire graph. Circles represent rules and rounded squares items. The size of the circle is relative to the support and color relative to the lift. Incoming arrows represent the items in the antecedent and the outgoing arrow of a circle points to the item in the consequent part of the rule. From this graph, some interesting patterns can be seen. First, when the age category of the perpetrator is lateAdulthood the victims were the husband or ex-wife. When the perpetrator is a teen, the victim was likely a friend or stranger. The arulesViz package has a cool function ruleExplorer() that generates a shiny app with interactive controls and several plot types. When running the following code (output not shown) you may be asked to install additional shiny related packages. # Opens a shiny app with several interactive plots. ruleExplorer(resrules) Sometimes Apriori returns thousands of rules. There is a convenient subset() function to extract rules of interest. For example, we can select only the rules that contain R.Girlfriend in the antecedent (lhs) with: # Subset transactions. rulesGirlfriend &lt;- subset(resrules, subset = lhs %in% &quot;R.Girlfriend&quot;) # Print rules with highest lift. inspect(head(rulesGirlfriend, n = 3, by = &quot;lift&quot;)) Figure 6.17: Output of the inspect() function. In this section, we showed how interesting rules can be extracted from a crimes database. Several preprocessing steps were required to transform the tabular data into transactional data. This example demonstrated how the same data can be represented in different ways (tabular and transactional). The next chapter will cover more details about how data can be transformed into different representations suitable for different types of learning algorithms. 6.4 Summary One of the types of machine learning is unsupervised learning in which there are no labels. This chapter introduced some unsupervised methods such as clustering and association rules. The objective of k-means clustering is to find groups of points such that points in the same group are similar and points from different groups are as dissimilar as possible. The centroid of a group is calculated by taking the mean value of each feature. In k-means, one needs to specify the number of groups \\(k\\) before running the algorithm. The Silhouette Index is a measure that tells us how well a set of points were clustered. This measure can be used to find the optimal number of groups \\(k\\). Association rules can find patterns in an unsupervised manner. The A-priori algorithm is the most well-known method to find association rules. Before using the A-priori algorithm, one needs to format the data as transactions. A transaction is an event that involves a set of items. References "],
["representations.html", "Chapter 7 Encoding Behavioral Data 7.1 Feature Vectors 7.2 Timeseries 7.3 Transactions 7.4 Images 7.5 Recurrence Plots 7.6 Bag-of-Words 7.7 Graphs 7.8 Summary", " Chapter 7 Encoding Behavioral Data Sensor data comes in many different flavors and shapes. Data stored in databases also have different structures (relational, graph, plain text, etc.). As mentioned in chapter 1, before training a predictive model, data goes through a series of steps, from data collection to preprocessing (Figure 1.6). During those steps, data is transformed and shaped with the aim of easing the operations in the subsequent tasks. Finally, the data needs to be encoded in a very specific format as expected by the predictive model. For example, decision trees and many other classifier methods expect their input data to be formatted as feature vectors while Dynamic Time Warping expects the data to be represented as timeseries. When working with images, one usually encodes them as \\(n\\)-dimensional matrices and when it comes to social network analysis, a graph is the preferred representation. So far, I have been mentioning two key terms: encode and representation. The Cambridge Dictionary13 defines the verb encode as: “To put information into a form in which it can be stored, and which can only be read using special technology or knowledge…” while TechTerms.com14 defines it as: “Encoding is the process of converting data from one form to another.” Both definitions are similar but for our case, the second one will make more sense. The Cambridge Dictionary15 defines representation as: “The way that someone or something is shown or described.” TechTerms.com returned no results for that word. From now on, I will use the term encode to refer to the process of transforming the data and representation as the way data is ‘conceptually’ described. Note the ‘conceptually’ part which means the way we humans think about it. This means that data can have a conceptual representation but that does not necessarily mean it is digitally stored in that way. For example, a physical activity like walking captured with a motion sensor can be conceptually represented by humans as a feature vector but its actual digital format inside a computer is binary. Figure 7.1: The real world walking activity as a) human conceptual representation and b) computer format. It is also possible to encode the same data into different representations depending on the application or the predictive model to be used. Each representation has its own advantages and limitations (discussed in the following subsections) and they capture different aspects of the real-world phenomenon. Sometimes it is useful to encode the same data into different representations so more information can be extracted and complemented as discussed in the ‘Multi-view learning’ section 3.4. In the next sections, several types of representations will be presented along with some ideas of how the same raw data can be encoded into different ones. Figure 7.2: Example of some raw data encoded into different representations. 7.1 Feature Vectors From previous chapters, we have already seen how the data can be represented as feature vectors. For example when classifying physical activities (section 2.3.1) or clustering questionnaire answers (section 6.1.1). Feature vectors are compact representations of some real-world phenomenon or object and usually, they are modeled in a computer as numeric arrays. Most machine learning algorithms work with feature vectors. Generating feature vectors requires knowledge of the application domain. Ideally, one wants to have feature vectors that represent the real-world situation as accurately as possible. We could achieve a good mapping by having feature vectors of infinite size, unfortunately, that is infeasible. In practice, small feature vectors are desired because that reduces storage requirements and computational time. The process of generating feature vectors from raw data is known as feature engineering which also involves the process of deciding which features to extract. This requires domain knowledge as the features should capture the information needed to solve the problem. Suppose we want to classify if a person is ‘tired’ or ‘not tired’. We have access to some details about the person like age, height, the activities performed during the last \\(30\\) minutes, and so on. For simplicity, let’s assume we can generate feature vectors of size \\(2\\) and we have two options: Option 1. Feature vectors where the first element is age and the second element is height. Option 2. Feature vectors where the first element is the number of squats done by the user during the last \\(30\\) minutes and the second element is heart rate. Clearly, for this specific classification problem the second option is more likely to produce better results. The first option may not even contain enough information and will lead the predictive model to produce random predictions. With the second option, the boundaries between classes are more clear and classifiers will have an easier time finding them. Figure 7.3: Two different feature vectors for classifying tired and not tired. In R, feature vectors are stored as data frames where rows are individual instances and columns are features. Some of the advantages and limitations of feature vectors are: Advantages: Efficient in terms of memory. Most ML algorithms support them. Efficient in terms of computations compared to other representations. Limitations: Are static in the sense that they cannot capture temporal relationships. A lot of information and/or temporal relationships may be lost. Some features may be redundant leading to decreased performance. Require effort and domain knowledge to extract them. Difficult to plot if the dimension is \\(&gt; 2\\) unless some dimensionality reduction method is applied such as Multidimensional Scaling (chapter 4). 7.2 Timeseries A timeseries is a sequence of data points ordered in time. We have already worked with timeseries data in previous chapters (2) when classifying physical activities and hand gestures. Timeseries can be multi-dimensional. For example, typical inertial sensors capture motion forces in three axes. Timeseries analysis methods can be used to find underlying time-dependent patterns while timeseries forecasting methods aim to predict future data points based on historical data. Timeseries analysis is a very extensive topic and there are already several books that specialize on it. For example, the book “Forecasting: Principles and Practice” by Hyndman and Athanasopoulos (2018) focuses on timeseries forecasting with R. In this book we mainly use timeseries data collected from sensors in the context of behavior predictions using machine learning. We have already seen how classification models (like decision trees) can be trained with timeseries converted into feature vectors (section 2.3.1) or by using the raw timeseries data with Dynamic Time Warping (section 2.5.1). Advantages: Many problems have this form and can be naturally modeled as timeseries. Temporal information is preserved. Easy to plot and visualize. Limitations: Not all algorithms support timeseries of varying lengths so, one needs to truncate and/or do some type of padding. Many timeseries algorithms are slower than the ones that work with feature vectors. Timeseries can be very long, thus, making computations very slow. 7.3 Transactions Sometimes we may want to represent our data as transactions, as we did in section 6.3. Data represented as transactions are usually intended to be used by association rule mining algorithms (see section 6.3). As a minimum, a transaction has a unique identifier and a set of items. Items can be types of products, symptoms, ingredients, etc. A set of transactions is called a database. Figure 7.4 taken from chapter 6 shows an example database with \\(10\\) transactions. In this example, items are sets of products from a supermarket. Figure 7.4: Example database with 10 transactions. Transactions can include additional information like customer id, date, total cost, etc. Transactions can be coded as logical matrices where rows represent transactions and columns represent items. A TRUE value indicates that the particular item is present and FALSE indicates that the particular item is not part of that set. When the number of possible items is huge and item sets contain a small number of items this type of matrix can be inefficient because there are going to be many entries with a value of FALSE, that is, a sparse matrix. Transactions can also be stored as lists or in a relational database such as MySQL. Below are some advantages of representing data as transactions. Advantages: Association rule mining algorithms such as Apriori can be used to extract interesting behavior relationships. Recommendation systems can be built based on transactional data. Limitations: Can be inefficient to store them as a logical matrix. 7.4 Images timeseries_to_images.R plot_activity_images.R Images are rich visual representations that capture a lot of information –including spatial relationships. Pictures taken from a camera, drawings, scanned documents, etc. already are represented as images. However, other types of non-image data can be converted into images. One of the main advantages of analyzing images is that they retain spatial information (distance between pixels). This information can be useful when using predictive models that can take advantage of those properties such as Convolutional Neural Networks (CNNs) which will be presented in chapter 8. CNNs have proven to produce state of the art results in many vision-based tasks and are very flexible models in the sense that they can be adapted for a variety of applications with little effort. Before CNNs were introduced by Lecun (LeCun et al. 1998), image classification used to be feature-based. One first needed to extract hand-crafted features from images and then use a classifier such as decision trees, \\(k\\)-nn, etc. Also, images can be flattened into one-dimensional arrays where each element represents a pixel (Figure 7.5). Then, those \\(1\\)D arrays can be used as feature vectors to perform training and inference. Figure 7.5: Flattening a matrix into a 1D array. Flattening an image can lead to information loss and the dimension of the resulting vector can be very high, sometimes limiting its applicability and/or performance. Feature extraction from images can also be a complicated task and very application dependent. CNNs have changed that. They take as input raw images, that is, matrices and automatically extract features and perform classification or regression. What if our data are not represented as images but we would still want to take advantage of featureless models like CNNs? Depending on the type of data, it may be possible to encode it as an image. For example, timeseries data can be encoded as an image. In fact, a timeseries can already be considered an image with a height of \\(1\\) but they can be reshaped into square matrices. Take for example the SMARTPHONE ACTIVITIES dataset which contains accelerometer data for each of the \\(x\\), \\(y\\), and \\(z\\) axes. The script timeseries_to_images.R shows how the acceleration timeseries can be converted to images. A window size of \\(100\\) is defined. Since the sampling rate was \\(20\\) Hz it means each window corresponds to \\(100/20 = 5\\) seconds. For each window, we have \\(3\\) timeseries (\\(x\\),\\(y\\),\\(z\\)). We can reshape each of them into \\(10 \\times 10\\) matrices by arranging the elements into columns. Then, the three matrices can be stacked to form a \\(3\\)D image similar to an RGB image. Figure 7.6 shows the process of reshaping \\(3\\) timeseries of size \\(9\\) into \\(3 \\times 3\\) matrices to generate an RGB-like image. Figure 7.6: Encoding 3 accelerometer timeseries as an image. The script then moves to the next window with no overlap and repeats the process. Actually, the script saves each image as one line of text. The first \\(100\\) elements correspond to the \\(x\\) axis, the next \\(100\\) to \\(y\\), and the remaining to \\(z\\). Thus each line has \\(300\\) values. Finally, the user id and the corresponding activity label are added at the end. This format will make it easy to read the file and reconstruct the images later on. The resulting file is called images.txt and is already included in the smartphone_activities dataset folder. The script plot_activity_images.R shows how to read the images.txt file and reconstruct the images so we can plot them. Figure 7.7 shows three different activities plotted as colored images of \\(10 \\times 10\\) pixels. Before generating the plots, the images were normalized between \\(0\\) and \\(1\\). Figure 7.7: Three activities captured with an accelerometer represented as images. We can see that the patterns for ‘jogging’ look more “chaotic” compared to the others while the ‘sitting’ activity looks like a plain solid square. Then, we can use those images to train a CNN and perform inference. CNNs will be covered in chapter 8 and we’ll also see how to build adaptive models using these activity images. Advantages: Spatial relationships can be captured. Can be multi-dimensional. For example \\(3\\)D RGB images. Can be efficiently processed with CNNs. Limitations: Computational time can be higher than when processing feature vectors. Still, modern hardware and methods allow us to perform operations very efficiently. It can take some extra processing to convert non-image data into images. 7.5 Recurrence Plots Recurrence plots (RPs) are visual representations similar to images but typically they are one-dimensional. They are encoded as \\(n \\times n\\) matrices, that is, the same number of rows and columns (a square matrix). Even though these are like a special case of images, I thought it would be worth having them in their own subsection! Just as with images, timeseries can be converted into RPs and then used to train a CNN. A recurrence plot (RP) is a visual representation of time patterns of dynamical systems (for example, timeseries). RPs were introduced by Eckmann, Kamphorst, and Ruelle (1987) and they depict all the times when a trajectory is roughly in the same state. They are visual representations of the dynamics of a system. Biological systems possess behavioral patterns and activity dynamics that can be captured with RPs, for example, the dynamics of ant colonies (Neves 2017). At this point, you may be curious about how a RP looks like. So let me begin by showing a picture16 of \\(4\\) time series with their respective RP (Figure 7.8). Figure 7.8: Four timeseries (top) with their respective RPs (bottom). Source: wikipedia. The first RP (leftmost) does not seem to have a clear pattern (white noise) whereas the other three show some patterns like diagonals of different sizes, some square and circular shapes, and so on. RPs can be characterized by small-scale and large-scale patterns. Examples of small-scale patterns are diagonals, horizontal/vertical lines, dots, etc. Large-scale patterns are called typology and they depict the global characteristics of the dynamic system.17 The visual interpretation of RPs requires some experience and is out of the scope of this book. However, they can be used as a visual pattern extraction tool to represent the data and then, in conjunction with machine learning methods like CNNs, used to solve classification problems. There exists an objective way to analyze RPs and is called recurrence quantification analysis (RQA) (Zbilut and Webber 1992). It introduces several measures18 like percentage of recurrence points (recurrence rate), percentage of points that form vertical lines (laminarity), average length of diagonal lines, length of the longest diagonal line, etc. Those measures can then be used as features to train classifiers such as decision trees, \\(k\\)-nn, etc. But how are RPs computed? Well, that is the topic of the next section. 7.5.1 Computing Recurence Plots It’s time to delve into the details about how these mysterious plots are computed. Suppose there is a timeseries with \\(n\\) elements (points). To compute its RP we need to compute the distance between each pair of points. We can store this information in a \\(n \\times n\\) matrix. Let’s call this a distance matrix \\(D\\). Then, we need to define a threshold \\(\\epsilon\\). For each entry in \\(D\\) if the distance is less or equal than the threshold \\(\\epsilon\\) then it is set to \\(1\\) and \\(0\\) otherwise. Formally, a recurrence of a state at time \\(i\\) at a different time \\(j\\) is marked within a two-dimensional squared matrix with ones and zeros where both axes represent time: \\[\\begin{equation} R_{i,j} \\left( x \\right) = \\begin{cases} 1 &amp; \\textbf{if } \\lvert\\lvert \\vec{x}_i - \\vec{x}_j \\rvert \\rvert \\leq \\epsilon \\\\ 0 &amp; \\textbf{otherwise}, \\end{cases} \\tag{7.1} \\end{equation}\\] where \\(\\vec{x}\\) are the states and \\(\\lvert\\lvert \\cdot \\rvert \\rvert\\) is a norm (for example Euclidean distance). \\(R_{i,j}\\) is the square matrix and will be \\(1\\) if \\(\\vec{x}_i \\approx \\vec{x}_j\\) up to an error \\(\\epsilon\\). The \\(\\epsilon\\) is important since systems often do not recur exactly to a previously visited state. The threshold \\(\\epsilon\\) needs to be set manually which can be difficult in some situations. If not set properly, the RP can end up having excessive ones or zeros. If you plan to use RPs as part of an automated process and fed them to a classifier, you can use the distance matrix instead. The advantage is that you don’t need to specify any parameter except for the distance function. The distance matrix can be defined as: \\[\\begin{equation} \\label{eq:distance_matrix} D_{i,j} \\left( x \\right) = \\lvert\\lvert \\vec{x}_i - \\vec{x}_j \\rvert \\rvert \\end{equation}\\] which is similar to Equation (7.1) but without the extra step of applying a threshold. Advantages: RPs capture dynamic patterns of a system. They can be used to extract small and large scale patterns. Timeseries can be easily encoded as RPs. Can be used as input to CNNs for supervised learning tasks. Limitations: Computationally intensive since all pairs of distances need to be calculated. Their visual interpretation requires experience. A threshold needs to be defined and it is not always easy to find the correct value. However, the distance matrix can be used instead. 7.5.2 Recurrence Plots of Hand Gestures recurrence_plots.R In this section, I am going to show you how to compute recurrence plots in R using as an example the HAND GESTURES dataset. The code can be found in the script recurrence_plots.R. First, we need a norm (distance function) for example the Euclidean distance: # Computes Euclidean distance between x and y. norm2 &lt;- function(x, y){ return(sqrt((x - y)^2)) } The following function computes a distance matrix and a recurrence plot and returns both of them. The first argument x is a vector representing a timeseries, e is the threshold and f is a distance function. rp &lt;- function(x, e, f=norm2){ #x: vector #e: threshold #f: norm (distance function) N &lt;- length(x) # This will store the recurrence plot. M &lt;- matrix(nrow=N, ncol=N) # This will store the distance matrix. D &lt;- matrix(nrow=N, ncol=N) for(i in 1:N){ for(j in 1:N){ # Compute the distance between a pair of points. d &lt;- f(x[i], x[j]) # Store result in D. # Start filling values from bottom left. D[N - (i-1), j] &lt;- d if(d &lt;= e){ M[N - (i-1), j] &lt;- 1 } else{ M[N - (i-1), j] &lt;- 0 } } } return(list(D=D, RP=M)) } This function first defines two square matrices M and D to store the recurrence plot and the distance matrix, respectively. Then, it iterates the matrices from bottom left to top right and fills the corresponding values for M and D. The distance between elements i and j from the vector is computed. That distance is directly stored in D. To generate the RP we check if the distance is less or equal to the threshold. If that is the case the corresponding entry in M is set to \\(1\\). Finally, both matrices are returned by the function. Now, we can try our rp() function on the HAND GESTURES dataset to convert one of the timeseries into a RP. First, we read one of the gesture files. For example, the first gesture ‘1’ from user \\(1\\). We only extract the acceleration from the \\(x\\) axis and store it in variable x. df &lt;- read.csv(&quot;../../data/hand_gestures/1/1_20130703-120056.txt&quot;, header = F) x &lt;- df$V1 If we plot vector x we get something like in Figure 7.9. # Plot vector x. plot(x, type=&quot;l&quot;, main=&quot;Hand gesture 1&quot;, xlab = &quot;time&quot;, ylab = &quot;&quot;) Figure 7.9: Acceleration of x of gesture 1. Now the rp() function that we just defined is used to calculate the RP and distance matrix of vector x. We set a threshold of \\(0.5\\) and store the result in res. # Compute RP and distance matrix. res &lt;- rp(x, 0.5, norm2) Let’s first plot the distance matrix stored in res$D. The pheatmap() function can be used to generate the plot. library(pheatmap) pheatmap(res$D, main=&quot;Distance matrix of gesture 1&quot;, cluster_row = FALSE, cluster_col = FALSE, legend = F, color = colorRampPalette(c(&quot;white&quot;, &quot;black&quot;))(50)) Figure 7.10: Distance matrix of gesture 1. From figure 7.10 we can see that the diagonal cells are all white. Those represent values of \\(0\\), the distance between a point and itself. Apart from that, there are no other human intuitive patterns to look for. Now, let’s plot the recurrence plot stored in res$RP. pheatmap(res$RP, main=&quot;RP with threshold = 0.5&quot;, cluster_row = FALSE, cluster_col = FALSE, legend = F, color = colorRampPalette(c(&quot;white&quot;, &quot;black&quot;))(50)) Figure 7.11: RP of gesture 1 with a threshold of 0.5. Here, we can see that this is kind of an inverted version of the distance matrix. Now, the diagonal is black because small distances are encoded as ones. There are also some clusters of points and vertical and horizontal line patterns. If we wanted to build a classifier, we would not need to interpret those extraterrestrial images. We could just treat each distance matrix or RP as an image and feed them directly to a CNN (CNNs will be covered in chapter 8). Finally, we can try to see what happens if we change the threshold. Figure 7.12 shows two RPs. In the left one, a small threshold of \\(0.01\\) was used. Here, many details were lost and only very small distances show up. On the right plot, a big threshold of \\(1.5\\) was used. Here, the plot is cluttered with black pixels which makes it difficult to see any patterns. On the other hand, a distance matrix will remain the same regardless of the threshold selection. Figure 7.12: RP of gesture 1 with two different thresholds. 7.6 Bag-of-Words The main idea of the Bag-of-Words (BoW) encoding is to represent a complex entity as a set of its constituent parts. It is called Bag-of-Words because one of the first applications was in natural language processing. Say there is a set of documents about different topics such as medicine, arts, engineering, etc. and you would like to classify them automatically based on their words. In BoW, each document is represented as a table that contains the unique words across all documents and their respective counts for each document. With this representation, one may see that documents about medicine will contain higher counts of words like treatment, diagnosis, health, etc. compared to documents about art or engineering. Figures 7.13 and 7.14 show the conceptual view and the table view, respectively. Figure 7.13: Conceptual view of two documents as BoW. Figure 7.14: Table view of two documents as BoW. From these representations, it is now easy to build a document classifier. The word-counts table can be used as an input feature vector. That is, each position in the feature vector represents a word and its value is an integer representing the total count for that word. Note that in practice documents will differ in length, thus, it is a good idea to use percentages instead of total counts. This can be achieved by dividing each word count by the total number of counts. Also note that some frequent words like ‘the’, ‘is’, ‘it’ can cause problems, so some extra preprocessing is needed. This was a simple example but if you are interested in more advanced text processing techniques I refer you to the book “Text Mining with R: A Tidy Approach” by Silge and Robinson (2017). BoW can also be used for image classification in complex scenarios. For example when dealing with composed scenes like classrooms, parks, shops, streets, etc. First, the scene (document) can be decomposed into smaller elements (words) by identifying objects like trees, chairs, cars, cashiers, etc. In this case, instead of bags of words we have bags of objects but the idea is the same. The object identification part can be done in a supervised manner where there is already a classifier that assigns labels to objects. Using a supervised approach can work in some simple cases but is not scalable for more complex ones. Why?. Because the classifier would need to be trained for each type of object. Furthermore, those types of objects need to be manually defined beforehand. If we want to apply this method on scenes where most of their elements do not have a corresponding label in the object classifier we will be missing a lot of information and will end up having incomplete word count tables. A possible solution is to instead, use an unsupervised approach. The image scene can be divided into squared (but not necessarily) patches. Conceptually, each patch may represent an independent object (a tree, a chair, etc.). Then, feature extraction can be performed on each patch so ultimately patches are encoded as feature vectors. Again, each feature vector represents an individual possible object inside the complex scene. At this point, those feature vectors do not have a label so we can’t build the BoW (table counts) for the whole scene. Then, how are those unlabeled feature vectors useful? We could use a pre-trained classifier to assign them labels –but we would be relying into the supervised approach along with its aforementioned limitations. Instead, we can use an unsupervised method, for example, K-means! which was presented in chapter 6. We can cluster all the unlabeled feature vectors into \\(k\\) groups where \\(k\\) is the number of possible unique labels. After the clustering, we can compute the centroid of each group. To assign a label to an unlabeled feature vector, we can compute the closest centroid and use its id as the label. The id of each centroid can be an integer number. Intuitively, similar feature vectors will end up in the same group. For example, there could be a group of objects that look like chairs another one for objects that look like cars and so on. Usually, it may happen that elements in the same groups will not look similar for the human eye, but they are similar in the feature space. Also, the objects’ shape inside the groups may not make sense at all for the human eye. If the objective is to classify the complex scene, then we do not necessarily need to understand the individual objects nor they need to have a corresponding mapping into a real-world object. Once the feature vectors are labeled, we can now build the word-count table but instead of having ‘meaningful’ words, the entries will be ids with their corresponding counts. As you might have guessed, one limitation is that we do not know how many clusters (labels) there should be for a given problem. One approach is to try for different values of \\(k\\) and use the one that optimizes your performance metric of interest. But, what this BoW thing has to do with behavior? Well, we can use this method to decompose complex behaviors into simpler ones and encode them as BoW as we will see in the next subsection for complex activities analysis. Advantages Able to represent complex things by decomposing them into simpler parts. The resulting BoW can be very efficient and effective for classification tasks. Can be used in several domains including text, computer vision, sensor data, etc. The BoW can be constructed in an unsupervised manner. Limitations Temporal and spatial information is not preserved. It may require some effort to define how to generate the words. There are cases where one needs to find the optimal number of words. 7.6.1 BoW for Complex Activities. bagwords/bow_functions.R bagwords/bow_run.R So far, I have been talking about BoW applications for text and images. In this section, I will show you how to decompose complex activities from accelerometer data into simpler activities and encode them as BoW. In chapters 2 and 3 we trained supervised models for simple activity recognition. Those activities were some like: walking, jogging, standing, etc. For those, it is sufficient to divide them into windows of size equivalent to a couple of seconds in order to infer their labels. On the other hand, complex activities happen in longer periods of time and they are composed of many simple activities. One example is the activity shopping. When we are shopping we perform many different activities including walking, taking groceries, paying, standing while looking at the stands, and so on. Another example is commuting. When we commute, we need to walk but also take the train, or drive, or cycle, etc. Using the same approach for simple activity classification on complex ones may not work. Representing a complex activity using fixed-size windows, could cause some conflicts. For example, a window may be covering the time span when the user was walking, but walking can be present in different types of complex activities. If a window happens to be part of a segment when the person was walking, there is not enough information to know which was the complex activity at that time. This is where BoW comes into play. If we represent a complex activity as a bag of simple activities then, a classifier will have an easier time differentiating between classes. For instance, when exercising, the frequencies (counts) of high-intensity activities (like running or jogging) will be higher compared to when someone is shopping. In practice, it would be very tedious to label all the possible simple activities to form the BoW. We will use the unsupervised approach discussed in the previous section to automatically find labels for the simple activities so we only need labels for the complex ones. Here, I will use the COMPLEX ACTIVITIES dataset which consists of five complex activities: ‘commuting’, ‘working’, ‘being at home’, ‘shopping’ and ‘exercising’. The duration of the activities varies from some minutes to a couple of hours. Accelerometer data at \\(50\\) Hz. was collected with a cellphone placed in the user’s belt. The dataset has \\(80\\) accelerometer files, each representing a complex activity. The task is to go from the raw accelerometer data of the complex activity to a BoW representation where each word will represent a simple activity. The overall steps are as follows: Divide the raw data into small fixed-length windows and generate feature vectors from them. Intuitively, these are the simple activities. Cluster the feature vectors. Label the vectors by assigning them to the closest centroid. Build the word-count table. Figure 7.15: BoW steps. From raw signal to BoW table. Figure 7.15 shows the overall steps graphically. All the functions to perform the above steps are implemented in bow_functions.R. The functions are called in the appropriate order in bow_run.R. First of all, and to avoid overfitting, we need to hold out an independent set of instances. These instances will be used to generate the clusters and their respective centroids. The dataset is already divided into a train and test set. The train set contains \\(13\\) instances out of the \\(80\\). The remaining \\(67\\) are assigned to the test set. In the first step, we need to extract the feature vectors from the raw data. This is implemented in the function extractSimpleActivities(). This function divides the raw data of each file into fixed-length windows of size \\(150\\) which corresponds to \\(3\\) seconds. Each window can be thought of as a simple activity. For each window, it extracts \\(14\\) features like mean, standard deviation, correlation between axes, etc. The output is stored in the folder simple_activities/. Each file corresponds to one of the complex activities and each row in a file is a feature vector (simple activity). At this time the feature vectors (simple activities) are unlabeled. Notice that in the script bow_run.R the function is called twice: # Extract simple activities for train set. extractSimpleActivities(train = TRUE) # Extract simple activities for test set (may take some minutes). extractSimpleActivities(train = FALSE) This is because we divided the data into a train and test sets. So we need to extract the features from both sets by setting the train parameter accordingly. The second step consists of clustering the extracted feature vectors. To avoid overfitting, this step is only performed on the train set. The function clusterSimpleActivities() implements this step. The feature vectors are grouped into \\(15\\) groups. This can be changed by setting constants$wordsize &lt;- 15 to some other value. The function stores all feature vectors from all files in a single data frame and runs k-means. Finally, the resulting centroids are saved in the text file clustering/centroids.txt inside the train set directory. The next step is to label each feature vector (simple activity) by assigning it to its closest centroid. The function assignSimpleActivitiesToCluster() reads the centroids from the text file and for each simple activity in the test set it finds the closest centroid using the Euclidean distance. The label (an integer from \\(1\\) to \\(15\\)) of the closest centroid is assigned and the resulting files are saved in the labeled_activities/ directory. Each file contains the assigned labels (integers) for the corresponding feature vectors file in the simple_activities/ directory. Thus, if a file inside simple_activities/ has \\(100\\) feature vectors then, its corresponding file in labeled_activities/ should have \\(100\\) labels. In the last step, the function convertToHistogram() will generate the bag of words from the labeled activities. The BoW are stored as histograms (encoded as vectors) with each element representing a label and its corresponding counts. In this case, the labels are \\(w1..w15\\). The \\(w\\) stands for word and was only appended for clarity to show that this is a label. This function will convert the counts into percentages (normalization) in case we want to perform classification, that is, the percentage of time that each word (simple activity) occurred during the entire complex activity. The resulting histograms/histograms.csv file contains the BoW as one histogram per row. One per each complex activity. The first column is the complex activity’s label in text format. Figures 7.16 and 7.17 show the histogram for ‘working’ and ‘exercising’. The x-axis shows the labels of the simple activities and the y-axis their relative frequencies. Figure 7.16: Histogram of working activity. Figure 7.17: Histogram of exercising activity. Here, we can see that the ‘working’ activity is composed mainly by the simple activities w1, w3, and w12. The exercising activity is mainly composed of w15 and w14 which perhaps are high-intensity movements like jogging or running. Once the complex activities are encoded as BoW (histograms), one could train a classifier using the histogram frequencies as features. 7.7 Graphs Graphs are one of the most general data structures (and my favorite one). The two basic components of a graph are its vertices and edges. Vertices are also called nodes and edges are also called arcs. Vertices are connected by edges. Figure 7.18 shows three different types of graphs. Graph (a) is an undirected graph that consists of \\(3\\) vertices and \\(3\\) edges. Graph (b) is a directed graph, that is, its edges have a direction. Graph (c) is a weighted directed graph because its edges have a direction and they also have an associated weight. Figure 7.18: Three different types of graphs. Weights can represent anything, for example, distances between cities or number of messages sent between devices. In the previous graph, the vertices also have a label (integer numbers but could be strings). In general, vertices and edges can have any number of attributes, not just weight and/or labels. Many data structures like binary trees and lists are graphs with constraints. For example, a list is also a graph in which all vertices are connected as a sequence: a-&gt;b-&gt;c. Trees are also graphs with the constraint that there is only one root node and nodes can only have edges to their children. Graphs are very useful to represent many types of real-world things like interactions, social relationships, geographical locations, the world wide web, and so on. There are two main ways to encode a graph. The first one is as an adjacency list. An adjacency list consists of a list of tuples per each node. The tuples represent edges. The first element of a tuple indicates the target node and the second element the weight of the edge. Figure 7.19-b shows the adjacency list representation of the corresponding weighted directed graph in the same figure. The second main way to encode a graph is as an adjacency matrix. This is a square \\(n\\times n\\) matrix where \\(n\\) is the number of nodes. Edges are represented as entries in the matrix. If there is an edge between node \\(a\\) and \\(b\\), the corresponding cell contains the edge’s weight where rows represent the source nodes and columns the destination nodes. Otherwise, it contains a \\(0\\) or just an empty value. Figure 7.19-c shows the corresponding adjacency matrix. The disadvantage of the adjacency matrix is that for sparse graphs (many nodes and few edges), a lot of space is wasted. In practice, this can be overcome by using a sparse matrix implementation. Figure 7.19: Different ways to store a graph. Advantages: Many real-world situations can be naturally represented as graphs. Some partial order is preserved. Specialized graph analytics can be performed to gain insights and understand the data. See for example the book by Samatova et al. (2013). Can be plotted and different visual properties can be tuned to convey information such as edge width and colors, vertex size and color, distance between nodes, etc. Limitations: Some graph analytic algorithms are computationally demanding. It can be difficult to use graphs to solve classification problems. It is not always clear if the data can be represented as a graph. 7.7.1 Complex Activities as Graphs plot_graphs.R In the previous section, it was shown how complex activities can be represented as Bag-of-Words. This was done by decomposing the complex activities into simpler ones. The BoW is composed of the simple activities counts (frequencies). In the process of building the BoW in the previous section, some intermediate text files stored in labeled_activities/ were generated. These files contain the sequence of simple activities (their ids as integers) that constitute the complex activity. From these sequences, histograms were generated and in doing so, the order was lost. One thing we can do is build a graph where vertices represent simple activities and edges represent the interactions between them. For instance, if we have a sequence of activities ids like: \\(3,2,2,4\\) we can represent this as a graph with \\(3\\) vertices and \\(3\\) edges. One vertex per activity. The first edge would go from vertex \\(3\\) to vertex \\(2\\), the next one from vertex \\(2\\) to vertex \\(2\\), and so on. In this way we can use a graph to capture the interactions between simple activities. The script plot_graphs.R implements a function named ids.to.graph() that reads the sequence files from labeled_activities/ and converts them into weighted directed graphs. The weight of the edge \\((a,b)\\) is equal to the total number of transitions from vertex \\(a\\) to vertex \\(b\\). The script uses the igraph package (Csardi and Nepusz 2006) to store and plot the resulting graphs. The ids.to.graph() function receives as its first argument the sequence of ids. Its second argument indicates whether the edge weights should be normalized or not. If normalized, the sum of all weights will be \\(1\\). The following code snippet reads one of the sequence files, converts it into a graph, and plots the graph. datapath &lt;- &quot;../labeled_activitires/&quot; # Select one of the &#39;work&#39; complex activities. filename &lt;- &quot;2_20120606-111732.txt&quot; # Read it as a data frame. df &lt;- read.csv(paste0(datapath, filename), header = F) # Convert the sequence of ids into an igraph graph. g &lt;- ids.to.graph(df$V1, relative.weights = T) # Plot the result. set.seed(12345) plot(g, vertex.label.cex = 0.7, edge.arrow.size = 0.2, edge.arrow.width = 1, edge.curved = 0.1, edge.width = E(g)$weight * 8, edge.label = round(E(g)$weight, digits = 3), edge.label.cex = 0.4, edge.color = &quot;orange&quot;, edge.label.color = &quot;black&quot;, vertex.color = &quot;skyblue&quot; ) Figure 7.20 shows the resulting plot. The plot can be customized to change the vertex and edge color, size, curvature, etc. For more details please read the igraph package documentation. Figure 7.20: Complex activity ‘working’ plotted as a graph. Nodes are simple activities and edges transitions between them. The width of the edges is proportional to its weight. For instance, transitions from simple activity \\(3\\) to itself are very frequent (\\(53.2\\%\\) of the time) for the ‘work’ complex activity but transitions from \\(8\\) to \\(4\\) are very infrequent. Note that with this graph representation, some temporal dependencies are preserved but the complete sequence order is lost, still this captures more information compared to BoW. The relationships between consecutive simple activities are preserved. It is also possible to get the adjacency matrix with the method as_adjacency_matrix(). as_adjacency_matrix(g) #&gt; 6 x 6 sparse Matrix of class &quot;dgCMatrix&quot; #&gt; 1 11 12 3 4 8 #&gt; 1 1 1 . 1 . . #&gt; 11 . 1 1 1 1 . #&gt; 12 . 1 . . . . #&gt; 3 1 1 . 1 . 1 #&gt; 4 . . . 1 1 . #&gt; 8 . . . 1 1 . In this matrix, there is a \\(1\\) if the edge is present and a ‘.’ if there is no edge. However, this adjacency matrix does not contain information about the weights. We can print the adjacency matrix with weights by specifying attr = \"weight\". as_adjacency_matrix(g, attr = &quot;weight&quot;) #&gt; 6 x 6 sparse Matrix of class &quot;dgCMatrix&quot; #&gt; 1 11 12 3 4 8 #&gt; 1 0.06066946 0.001046025 . 0.023012552 . . #&gt; 11 . 0.309623431 0.00209205 0.017782427 0.001046025 . #&gt; 12 . 0.002092050 . . . . #&gt; 3 0.02405858 0.017782427 . 0.532426778 . 0.00209205 #&gt; 4 . . . 0.002092050 0.002092050 . #&gt; 8 . . . 0.001046025 0.001046025 . The adjacency matrices can then be used to train a classifier. Since many classifiers expect one-dimensional vectors and not matrices, one can flatten the matrix. This is left as an exercise for the reader to try. Which representation produces better classification results (adjacency matrix or BoW)? The book “Practical graph mining with R” (Samatova et al. 2013) is a good source to learn more about graph analytics with R. 7.8 Summary Depending on the problem at hand, the data can be encoded in different forms. Representing data in a particular way, can simplify the problem solving process and the application of specialized algorithms. This chapter presented different ways in which data can be encoded along with some of their advantages/disadvantages. Feature vectors are fixed-size arrays that capture the properties of an instance. This is the most common form of data representation in machine learning. Most machine learning algorithms expect their inputs to be encoded as feature vectors. Transactions is another way in which data can be encoded. This representation is appropriate for association rule mining algorithms. Data can also be represented as images. Algorithms like CNNs (covered in chapter 8) can work directly on images. The Bag-of-Words representation is useful when we want to model a complex behavior as a composition of simpler ones. A graph is a general data structure composed of vertices and edges and is used to model relationships between entities. It is possible to convert data into multiple representations. For example, timeseries can be converted into images, recurrence plots, etc. References "],
["deeplearning.html", "Chapter 8 Predicting Behavior with Deep Learning 8.1 Introduction to Artificial Neural Networks 8.2 Keras and TensorFlow with R 8.3 Classification with Neural Networks 8.4 Overfitting 8.5 Fine-Tuning a Neural Network 8.6 Convolutional Neural Networks 8.7 CNNs with Keras 8.8 Smiles Detection with a CNN 8.9 Summary", " Chapter 8 Predicting Behavior with Deep Learning Deep learning (DL) consists of a set of model architectures and algorithms with applications in supervised, semi-supervised, unsupervised and reinforcement learning. Deep learning is mainly based on artificial neural networks (ANNs). One of the main characteristics of DL is that the models are composed of several levels. Each level transforms its input into more abstract representations. For example, for an image recognition task, the first level corresponds to raw pixels, the next level transforms pixels into simple shapes like horizontal/vertical lines, diagonals, etc. The next level may abstract more complex shapes like wheels, windows, and so on; and the final level could detect if the image contains a car or a human, or maybe both. Examples of DL architectures include deep neural networks (DNNs), convolutional neural networks (CNNs), recurrent neural networks (RNNs), and autoencoders, to name a few. One of the reasons of the success of DL is due to its flexibility to deal with different types of data and problems. For example, CNNs can be used for image classification, RNNs can be used for timeseries data, autoencoders can be used to generate new data, and so on. Another advantage of DL is that it is not always required to do feature engineering. That is, extract different features depending on the problem domain. Depending on the problem and the DL architecture, it is possible to feed the raw data (with some preprocessing) to the model. The model will then, automatically extract features at each level with an increasing level of abstraction. DL has achieved state-of-the-art results in many different tasks including speech recognition, image recognition, translation, etc. It has also been successfully applied to different types of behavior prediction. In this chapter, an introduction to artificial neural networks will be presented. Next, we will see how to train deep models in R using Keras and TensorFlow. We will apply these models to behavior prediction tasks. This chapter also includes a section on convolutional neural networks and their application to behavior prediction. 8.1 Introduction to Artificial Neural Networks Artificial neural networks (ANNs) are mathematical models inspired by the brain. Here, I would like to emphasize the word inspired because ANNs do not model how a biological brain actually works. In fact, there is little knowledge about how the the brain works. ANNs are composed of units (also called neurons or nodes) and connections between units. Each unit can receive inputs from other units. Those inputs are processed inside the unit and produce an output. Typically, units are arranged into layers (as we will see later) and connections between units have an associated weight. Those weights are learned during training and they are the core elements that make a network behave in a certain way. For the rest of the chapter, I will mostly use the term units to refer to neurons/nodes. From time to time, I will use the term network to refer to artificial neural networks. Before going into details of how multi-layer ANNs work, let’s start with a very simple neural network consisting of a single unit. See Figure 8.1. Even though this network only has one node, it is already composed of several interesting elements which are the basis of more complex networks. First, it has \\(n\\) input variables \\(x_1 \\ldots x_n\\) which are real numbers. Second, the unit has a set of \\(n\\) weights \\(w_1 \\ldots w_n\\) associated with each input. These weights can take real numbers as values. Finally, there is an output \\(y&#39;\\) which is binary (it can take two values: \\(1\\) or \\(0\\)). Figure 8.1: A neural network composed of a single unit (perceptron). This simple network consisting of one unit with a binary output is called a perceptron and was proposed by Rosenblatt (1958). This single unit also known as perceptron is capable of making binary decisions based on the input and the weights. To get the final decision \\(y&#39;\\) the inputs are multiplied by their corresponding weights and the results are summed. If the sum is greater than a given threshold, then the output is \\(1\\) and \\(0\\) otherwise. Formally: \\[\\begin{equation} y&#39; = \\begin{cases} 1 &amp; \\textit{if } \\sum_{i}{w_i x_i &gt; t}, \\\\ 0 &amp; \\textit{if } \\sum_{i}{w_i x_i \\leq t} \\end{cases} \\tag{8.1} \\end{equation}\\] We can use a perceptron to make important decisions in life. For example, suppose you need to decide whether or not to go to the movies. Assume this decision is based on two pieces of information: You have money to pay the entrance (or not) and, it is a horror movie (or not). There are two additional assumptions as well: The movie theater only projects \\(1\\) film. You don’t like horror movies. This decision-making process can be modeled with the perceptron of Figure 8.2. This perceptron has two binary input variables: money and horror. Each variable has an associated weight. Suppose there is a decision threshold of \\(t=3\\). Finally, there is a binary output: \\(1\\) means you should go to the movies and \\(0\\) indicates that you should not go. In this example, the weights (\\(5\\) and \\(-3\\)) and the threshold \\(t=3\\) were already provided. The weights and the threshold are called the parameters of the network. Later, we will see how the parameters can be learned automatically from data. Figure 8.2: Perceptron to decide whether or not to go to the movies based on two input variables. Suppose that today was payday and the theater is projecting an action movie. Then, we can set the input variables \\(money=1\\) and \\(horror=0\\). Now we want to decide if we should go to the movie theater or not. To get the final answer we can use Equation (8.1). This formula tells us that we need to multiply each input variable with their corresponding weights and add them: \\[\\begin{align*} (money)(5) + (horror)(-3) \\end{align*}\\] Substituting money and horror with their corresponding values: \\[\\begin{align*} (1)(5) + (0)(-3) = 5 \\end{align*}\\] Since \\(5 &gt; t\\) (remember the threshold \\(t=3\\)), the final output will be \\(1\\), thus, the advice is to go to the movies. Let’s try the scenario when you have money but they are projecting a horror movie: \\(money=1\\), \\(horror=1\\). \\[\\begin{align*} (1)(5) + (1)(-3) = 2 \\end{align*}\\] In this case, \\(2 &lt; t\\) and the final output is \\(0\\). Even if you have money, you should not waste it on a movie that you know you most likely will not like. This process of applying operations to the inputs and obtaining the final result is called forward propagation because the inputs are ‘pushed’ all the way through the network (a single perceptron in this case). For bigger networks, the outputs of the current layer become the inputs of the next layer, and so on. For convenience, a simplified version of Equation (8.1) can be used. This alternative representation is useful because it provides flexibility to change the internals of the units (neurons) as we will see. The first simplification consists of representing the inputs and weights as vectors: \\[\\begin{equation} \\sum_{i}{w_i x_i} = \\boldsymbol{w} \\cdot \\boldsymbol{x} \\end{equation}\\] The summation becomes a dot product between \\(\\boldsymbol{w}\\) and \\(\\boldsymbol{x}\\). Next, the threshold \\(t\\) can be moved to the left and renamed to \\(b\\) which stands for bias. This is only notation but you can still think of the bias as a threshold. \\[\\begin{equation} y&#39; = f(\\boldsymbol{x}) = \\begin{cases} 1 &amp; \\textit{if } \\boldsymbol{w} \\cdot \\boldsymbol{x} + b &gt; 0, \\\\ 0 &amp; \\textit{otherwise} \\end{cases} \\end{equation}\\] The output \\(y&#39;\\) is a function of \\(\\boldsymbol{x}\\) with \\(\\boldsymbol{w}\\) and \\(b\\) as fixed parameters. One thing to note is that first, we are performing the operation \\(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b\\) and then, another operation is applied to the result. In this case, it is a comparison. If the result is greater than \\(0\\) the final output is \\(1\\). You can think of this second operation as another function. Call it \\(g(x)\\). \\[\\begin{equation} f(\\boldsymbol{x}) = g(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b) \\tag{8.2} \\end{equation}\\] In neural networks terminology, this \\(g(x)\\) is known as the activation function. Its result indicates how much active this unit is based on its inputs. If the result is \\(1\\), it means that this unit is active. If the result is \\(0\\), it means the unit is inactive. This new notation allows us to use different activation functions by substituting \\(g(x)\\) with some other function in Equation (8.2). In the case of the perceptron, the activation function \\(g(x)\\) is the threshold function, which is known as the step function: \\[\\begin{equation} g(x) = step(x) = \\begin{cases} 1 &amp; \\textit{if } x &gt; 0 \\\\ 0 &amp; \\textit{if } x \\leq 0 \\end{cases} \\tag{8.3} \\end{equation}\\] Figure 8.3 shows the plot of the step function. Figure 8.3: The step function. It is worth noting that perceptrons have two major limitations: The output is binary. Perceptrons are linear functions. The first limitation imposes some restrictions on its applicability. For example, a perceptron cannot be used to predict real-valued outputs which is a fundamental aspect for regression problems. The second limitation makes the perceptron only capable of solving linear problems. Figure 8.4 graphically shows this limitation. In the first case, the outputs of the OR logical operator can be classified (separated) using a line. On the other hand, it is not possible to classify the output of the XOR function using a single line. Figure 8.4: OR and the XOR logical operators. To overcome those limitations, several modifications to the perceptron were introduced. This allows us to build models capable of solving more complex non-linear problems. One such modification is to change the activation function. Another improvement is to add the ability to have several layers of interconnected units. In the next section, two new types of units will be presented. Then, the following section will introduce neural networks also known as multilayer perceptrons which are more complex models built by connecting many units and arranging them into layers. 8.1.1 Sigmoid and ReLU Units As previously mentioned, perceptrons have some limitations that restrict their applicability including the fact that they are linear models. In practice, problems are complex and most of them are non-linear. One way to overcome this limitation is to introduce non-linearities and this can be done by using a different type of activation function. Remember that a unit can be modeled as \\(f(x) = g(wx+b)\\) where \\(g(x)\\) is some activation function. For the perceptron, \\(g(x)\\) is the step function. However, another practical limitation not mentioned before is that the step function can change abruptly from \\(0\\) to \\(1\\) and vice versa. Small changes in \\(x\\),\\(w\\), or \\(b\\) can completely change the output. This is a problem during learning and inference time. Instead, we would prefer a smooth version of the step function, for example, the sigmoid function which is also known as the logistic function: \\[\\begin{equation} s(x) = \\frac{1}{1 + e^{-x}} \\tag{8.4} \\end{equation}\\] This function has an ‘S’ shape (Figure 8.5) and as opposed to a step function, this one is smooth. The range of this function is from \\(0\\) to \\(1\\). Figure 8.5: Sigmoid function. If we substitute the activation function in Equation (8.2) with the sigmoid function we get our sigmoid unit: \\[\\begin{equation} f(x) = \\frac{1}{1 + e^{-(w \\cdot x + b)}} \\tag{8.5} \\end{equation}\\] Sigmoid units have been one of the most commonly used types of units when building bigger neural networks. Another advantage is that the outputs are real values that can be interpreted as probabilities. For instance, if we want to make binary decisions we can set a threshold. For example, if the output of the sigmoid unit is \\(&gt; 0.5\\) then return a \\(1\\). Of course, that threshold would depend on the application. If we need more confidence about the result we can set a higher threshold. In the last years, another type of unit has been successfully applied to train neural networks, the rectified linear unit or ReLU for short. The activation function of this unit is the rectifier function: \\[\\begin{equation} rectifier(x) = \\begin{cases} 0 &amp; \\textit{if } x &lt; 0, \\\\ x &amp; \\textit{if } x \\geq 0 \\end{cases} \\tag{8.6} \\end{equation}\\] Figure 8.6: Rectifier function. This one is also called the ramp function and is one of the simplest non-linear functions and probably the most common one used in modern big neural networks. These units present several advantages, being among them, efficiency during training and inference time. In practice, many other activation functions are used but the most common ones are sigmoid and ReLU units. In the following link, you can find an extensive list of activation functions: https://en.wikipedia.org/wiki/Activation_function So far, we have been talking about single units. In the next section, we will see how these single units can be assembled to build bigger artificial neural networks. 8.1.2 Assembling Units into Layers Perceptrons, sigmoid and ReLU units can be thought of as very simple neural networks. By connecting several units, one can build more complex neural networks. For historical reasons, neural networks are also called multilayer perceptrons regardless if the units are perceptrons or not. Typically, units are grouped into layers. Figure 8.7 shows an example neural network with \\(3\\) layers. An input layer with \\(3\\) nodes, a hidden layer with \\(2\\) units and an output layer with \\(1\\) unit. Figure 8.7: Example neural network. In this type of diagram, the nodes represent units (perceptrons, sigmoids, ReLUs, etc.) except for the input layer. In the input layer, nodes represent input variables (input features). In the above example, the \\(3\\) nodes in the input layer simply indicate that the network takes as input \\(3\\) variables. In this layer, no operations are performed. This network only has one hidden layer. Hidden layers are called like that because they do not have direct contact with the external world. Finally, there is an output layer with a single unit. We could also have an output layer with more than one unit. Most of the time, we will have fully connected neural networks. That is, all units have incoming connections from all nodes in the previous layer (as in the previous example). For each specific problem, we need to define several building blocks for the network. For example, the number of layers, the number of units in each layer, the type of units (sigmoid, ReLU, etc.), and so on. This is known as the architecture of the network. Choosing a good architecture for a given problem is not a trivial task. It is advised to start with an architecture that was used to solve a similar problem and then fine-tune it for your specific problem. There exist some automatic ways to optimize the network architecture but those methods are out of the scope of this book. We already saw how a unit can produce a result based on the inputs by using forward propagation. For more complex networks the process is the same! Consider the network shown in Figure 8.8. It consists of two inputs and one output. It also has one hidden layer with \\(2\\) units. Figure 8.8: Example of forward propagation. Each node is labeled as \\(n_{l,n}\\) where \\(l\\) is the layer and \\(n\\) is the unit number. The two input values are \\(1\\) and \\(0.5\\). They could be temperature measurements, for example. Each edge has an associated weight. For simplicity, let’s assume that the activation function of the units is the identity function \\(g(x)=x\\). The bold underlined number inside the nodes of the hidden and output layers are the biases. Here we assume that the network is already trained (later we will see how those weights and biases are learned). To get the final result, for each node, its inputs are multiplied by their corresponding weights and added. Then, the bias is added. Next, the activation function is applied. In this case, it is just the identify function (returns the same value). The outputs of the nodes in the hidden layer become the inputs of the next layer and so on. In this example, first we need to compute the outputs of nodes \\(n_{2,1}\\) and \\(n_{2,2}\\): output of \\(n_{2,1} = (1)(2) + (0.5)(1) + 1 = 3.5\\) output of \\(n_{2,2} = (1)(-3) + (0.5)(5) + 0 = -0.5\\) Finally, we can compute the output of the last node using the outputs of the previous nodes: output of \\(n_{3,1} = (3.5)(1) + (-0.5)(-1) + 3 = 7\\). 8.1.3 Deep Neural Networks By increasing the number of layers and the number of units in each layer, one can build more complex networks. But what is a deep neural network (DNN)? It is not a strict rule but some people say that a network with more than \\(2\\) hidden layers is a deep network. Yes, that’s all it takes to build a DNN! Figure 8.9 shows an example of a deep neural network. Figure 8.9: Example of a deep neural network. A DNN has nothing special compared to a traditional neural network except that it has many layers. One of the reasons why they became so popular until recent years is because before it was not possible to efficiently train them. With the advent of specialized hardware like graphics processing units (GPUs), it is now possible to efficiently train big DNNs. The introduction of ReLU units was also a key factor that allowed the training of even bigger networks. The availability of big quantities of data was another key factor that allowed the development of deep learning technologies. Note that deep learning is not limited to DNNs but it also encompasses other types of architectures like convolutional networks and recurrent neural networks, to name a few. Convolutional layers will be covered later in this chapter. 8.1.4 Learning the Parameters We have seen how forward propagation can be used at inference time to compute the output of the network based on the input values. In the previous examples, we assumed that the network’s parameters (weights and biases) were already learned. In practice, you most likely will be using libraries and frameworks to build and train neural networks. Later in this chapter, I will show you how to use TensorFlow and Keras within R. Before that, I will show you how networks’ parameters are learned and how we can code and train a very simple network from scratch. Back to the problem, the objective is to find the parameters’ values based on training data such that the predicted result for any input data point is as close as possible as the true value. Put in other words, we want to find the parameters’ values that reduce the network´s prediction error. One way to estimate the network’s error is by computing the squared difference between the prediction \\(y&#39;\\) and the real value \\(y\\): \\(error = (y&#39; - y)^2\\). This is how the error can be computed for a single training data point. The error function is typically called the loss function and denoted by \\(L(\\theta)\\) where \\(\\theta\\) represents the parameters of the network (weights and biases). In this example the loss function is \\(L(\\theta)=(y&#39;- y)^2\\). If there is more than one training data point (which is often the case), the loss function is just the average of the individual squared differences which is known as the mean squared error (MSE): \\[\\begin{equation} L(\\theta) = \\frac{1}{N} \\sum_{n=1}^N{(y&#39;_n - y_n)^2} \\tag{8.7} \\end{equation}\\] The mean squared error (MSE) loss function is commonly used for regression problems. For classification problems, the average cross-entropy loss function is usually preferred (covered later in this chapter). The problem of finding the best parameters can be formulated as an optimization problem, that is, find the optimal parameters such that the loss function is minimized. This is the learning/training phase of a neural network. Formally, this can be stated as: \\[\\begin{equation} \\operatorname*{arg min}_{\\theta} L(\\theta) \\tag{8.8} \\end{equation}\\] This notation means: find and return the weights and biases that make the loss function be as small as possible. The most common method to train neural networks is called gradient descent. The algorithm updates the parameters in an iterative fashion based on the loss. This algorithm is suitable for complex functions with millions of parameters. Suppose there is a network with only \\(1\\) weight and no bias with MSE as loss function (Equation (8.7). Figure 8.10 shows a plot of the loss function. This is a quadratic function that only depends on the value of \\(w\\). The task is to find the \\(w\\) where the function is at its minimum. Figure 8.10: Gradient descent in action. Gradient descent starts by assigning \\(w\\) a random value. Then, at each step and based on the error, \\(w\\) is updated in the direction that minimizes the loss function. In the previous figure, the global minimum is found after \\(5\\) iterations. In practice, loss functions are more complex and have many local minima (Figure 8.11). For complex functions, it is difficult to find a global minimum but gradient descent can find a local minimum that is good enough. Figure 8.11: Function with 1 global minimum and several local minima. But in what direction and how much is \\(w\\) moved in each iteration? The direction and magnitude are estimated by computing the derivative of the loss function with respect to the weight \\(\\frac{\\partial L}{\\partial w}\\). The derivative is also called the gradient and denoted by \\(\\nabla L\\). The iterative gradient descent procedure is listed below: loop until convergence or max iterations (epochs) for each \\(w_i\\) in \\(W\\) do: \\(w_i = w_i - \\alpha \\frac{\\partial L(W)}{\\partial w_i}\\) The outer loop is run until the algorithm converges or until a predefined number of iterations is reached. Each iteration is also called an epoch. Each weight is updated with the rule: \\(w_i = w_i - \\alpha \\frac{\\partial L(W)}{\\partial w_i}\\). The derivative part will give us the direction and magnitude. The \\(\\alpha\\) is called the learning rate and it controls how ‘fast’ we move. The learning rate is a constant defined by the user, thus, it is a hyperparameter. A high learning rate can cause the algorithm to miss the local minima and the loss can start to increase. A small learning rate will cause the algorithm to take more time to converge. Figure 8.12 illustrates both scenarios. Figure 8.12: a) Big learning rate. b) Small learning rate. Selecting an appropriate learning rate will depend on the application but common values are between \\(0.0001\\) and \\(0.05\\). Let’s see how gradient descent works with a step by step example. Consider a very simple neural network consisting of an input layer with only one input feature and an output layer with one unit. To make it even simpler, the activation function of the output unit is the identity function \\(f(x)=x\\). Assume that as training data we have a single data point. Figure 8.13 shows the simple network and the training data. The training data point only has one input variable (\\(x\\)) and an output (\\(y\\)). We want to train this network such that it can make predictions on new data points. The training point has an input feature of \\(x=3\\) and the expected output is \\(y=1.5\\). For this particular training point, it seems that the output is equal to the input divided by \\(2\\). Thus, based on this single training data point the network should learn how to divide any other input by \\(2\\). Figure 8.13: a) A simple neural network consisting of one unit. b) The training data with only one row. Before we start the training we need to define \\(3\\) things: The loss function. This is a regression problem so we can use the MSE. Since there is a single data point our loss function becomes \\(L(w)=(y&#39; - y)^2\\). Here, \\(y\\) is the ground truth output value and \\(y&#39;\\) is the predicted value. We know how to make predictions using forward propagation. In this case, it is the product between the input value and the single weight, and the activation function has no effect (it returns the same value as its input). We can rewrite the loss function as \\(L(w)=(xw - y)^2\\). We need to define a learning rate. For now, we can set it to \\(\\alpha = 0.05\\). The weights need to be initialized at random. Let’s assume the single weight is ‘randomly’ initialized with \\(w=2\\). Now we use gradient descent to iteratively update the weight. Remember that the updating rule is: \\[\\begin{equation} w = w - \\alpha \\frac{\\partial L(w)}{\\partial w} \\end{equation}\\] The partial derivative of the loss function with respect to \\(w\\) is: \\[\\begin{equation} \\frac{\\partial L(w)}{\\partial w} = 2x(xw - y) \\end{equation}\\] If we substitute the derivative in the updating rule we get: \\[\\begin{equation} w = w - \\alpha 2x(xw - y) \\end{equation}\\] We already know that \\(\\alpha=0.05\\), the input value is \\(x=3\\), the output is \\(y=1.5\\) and the initial weight is \\(w=2\\). So we can start updating \\(w\\). Figure 8.14 shows the initial state (iteration 0) and \\(3\\) additional iterations. In the initial state, \\(w=2\\) and with that weight the loss is \\(20.25\\). In iteration \\(1\\), the weight is updated and now its value is \\(0.65\\). With this new weight, the loss is \\(0.2025\\). That was a substantial reduction in the error! After three iterations we see that the final weight is \\(w=0.501\\) and the loss is very close to zero. Figure 8.14: First 3 gradient descent iterations (epochs). Now, we can start doing predictions with our very simple neural network! To do so, we use forward propagation on the new input data using the learned weight \\(w=0.501\\). Figure 8.15 shows the predictions on new training data points that were never seen by the network before. Figure 8.15: Example predictions on new data points. Even though the predictions are not perfect, they are very close to the expected value (division by \\(2\\)) considering that the network is very simple and was only trained with a single data point and for only \\(3\\) epochs! If the training set has more than one data point, then we need to compute the derivative of each point and accumulate them (the derivative of a sum is equal to the sum of the derivatives). In the previous example, the update rule becomes: \\[\\begin{equation} w = w - \\alpha \\sum_{i=1}^N{2x_i(x_i w - y)} \\end{equation}\\] This means that before updating a weight, first, we need to compute the derivative for each point and add them. This needs to be done for every parameter in the network. Thus, one epoch is a pass through all training points and all parameters. 8.1.5 Parameter Learning Example in R gradient_descent.R In the previous section, we went step by step to train a neural network with a single unit and with a single training data point. Here, we will see how we can implement that simple network in R but when we have more training data. The code can be found in the script gradient_descent.R. This code implements the same network as the previous example. That is, one neuron, one input, no bias, and activation function \\(f(x) = x\\). We start by creating a sample training set with \\(3\\) points. Again, the output is the input divided by \\(2\\). train_set &lt;- data.frame(x = c(3.0,4.0,1.0), y = c(1.5, 2.0, 0.5)) # Print the train set. print(train_set) #&gt; x y #&gt; 1 3 1.5 #&gt; 2 4 2.0 #&gt; 3 1 0.5 Then we need to implement \\(3\\) functions: forward propagation, the loss function, and the derivative of the loss function. # Forward propagation w*x fp &lt;- function(w, x){ return(w * x) } # Loss function (y - y&#39;)^2 loss &lt;- function(w, x, y){ predicted &lt;- fp(w, x) # This is y&#39; return((y - predicted)^2) } # Derivative of the loss function. 2x(xw - y) derivative &lt;- function(w, x, y){ return(2.0 * x * ((x * w) - y)) } Now we are all set to implement the gradient.descent() function. The first parameter is the train set, the second parameter is the learning rate \\(\\alpha\\) and the last parameter is the number of epochs. The initial weight is initialized to some ‘random’ number (selected manually here for the sake of the example). The function returns the final learned weight. # Gradient descent. gradient.descent &lt;- function(train_set, lr = 0.01, epochs = 5){ w = -2.5 # Initialize weight at &#39;random&#39; for(i in 1:epochs){ derivative.sum &lt;- 0.0 loss.sum &lt;- 0.0 # Iterate each data point in train_set. for(j in 1:nrow(train_set)){ point &lt;- train_set[j, ] derivative.sum &lt;- derivative.sum + derivative(w, point$x, point$y) loss.sum &lt;- loss.sum + loss(w, point$x, point$y) } # Update weight. w &lt;- w - lr * derivative.sum # mean squared error (MSE) mse &lt;- loss.sum / nrow(train_set) print(paste0(&quot;epoch: &quot;, i, &quot; loss: &quot;, formatC(mse, digits = 8, format = &quot;f&quot;), &quot; w = &quot;, formatC(w, digits = 5, format = &quot;f&quot;))) } return(w) } Now, let’s train the network with a learning rate of \\(0.01\\) and for \\(10\\) epochs. This function will print for each epoch, the loss and the current weight. #### Train the 1 unit network with gradient descent #### lr &lt;- 0.01 # set learning rate. set.seed(123) # Run gradient decent to find the optimal weight. learned_w = gradient.descent(train_set, lr, epochs = 10) #&gt; [1] &quot;epoch: 1 loss: 78.00000000 w = -0.94000&quot; #&gt; [1] &quot;epoch: 2 loss: 17.97120000 w = -0.19120&quot; #&gt; [1] &quot;epoch: 3 loss: 4.14056448 w = 0.16822&quot; #&gt; [1] &quot;epoch: 4 loss: 0.95398606 w = 0.34075&quot; #&gt; [1] &quot;epoch: 5 loss: 0.21979839 w = 0.42356&quot; #&gt; [1] &quot;epoch: 6 loss: 0.05064155 w = 0.46331&quot; #&gt; [1] &quot;epoch: 7 loss: 0.01166781 w = 0.48239&quot; #&gt; [1] &quot;epoch: 8 loss: 0.00268826 w = 0.49155&quot; #&gt; [1] &quot;epoch: 9 loss: 0.00061938 w = 0.49594&quot; #&gt; [1] &quot;epoch: 10 loss: 0.00014270 w = 0.49805&quot; From the output, we can see that the loss decreases as the weight is updated. The final value of the weight at iteration \\(10\\) is \\(0.49805\\). We can now make predictions on new data. # Make predictions on new data using the learned weight. fp(learned_w, 7) #&gt; [1] 3.486366 fp(learned_w, -88) #&gt; [1] -43.8286 Now, you can try to change the training set to make the network learn a different arithmetic operation! In the previous example, we considered a very simple neural network consisting of a single unit. In this case, the partial derivative with respect to the single weight was calculated directly. For bigger networks with more layers and activations, the final output becomes a composition of functions. That is, the activation values of a layer \\(l\\) depend on its weights which are also affected by the previous layer’s \\(l-1\\) weights and so on. So, the derivatives (gradients) can be computed using the chain rule \\(f(g(x))&#39; = f&#39;(g(x)) \\cdot g&#39;(x)\\). This can be performed efficiently by an algorithm known as backpropagation. “What backpropagation actually lets us do is compute the partial derivatives \\(\\partial C_x / \\partial w\\) and \\(\\partial C_x / \\partial b\\) for a single training example.” (Michael Nielsen, 2019)19. Here, \\(C\\) refers to the loss function which is also called the cost function. In modern deep learning libraries like TensorFlow, this procedure is efficiently implemented with a computational graph. If you want to learn the details about backpropagation I recommend you to check this post by DEEPLIZARD (https://deeplizard.com/learn/video/XE3krf3CQls) which consists of \\(5\\) parts including videos. 8.1.6 Stochastic Gradient Descent We have seen how gradient descent iterates over all training points before updating each parameter. To recall, an epoch is one pass through all parameters and for each parameter, the derivative with each training point needs to be computed. If the training set consists of thousands or millions of points this method becomes very time-consuming. Furthermore, in practice neural networks do not have one or two parameters but thousands or millions. In those cases, the training can be done more efficiently by using stochastic gradient descent (SGD). This method adds two main modifications to the classic gradient descent: At the beginning, the training set is shuffled (this is the stochastic part). This is necessary for the method to work. The training set is divided into \\(b\\) batches with \\(m\\) data points each. This \\(m\\) is known as the batch size and is a hyperparameter that we need to define. Then, at each epoch all batches are iterated and the parameters are updated based on each batch and not the entire training set, for example: \\[\\begin{equation} w = w - \\alpha \\sum_{i=1}^m{2x_i(x_i w - y)} \\end{equation}\\] Again, an epoch is one pass through all parameters and all batches. Now you may be wondering why this method is more efficient if an epoch still involves the same number of operations but they are split into chunks. Part of this is because since the parameter updates are more frequent, the loss also improves quicker. Another reason is that the operations within each batch can be optimized and performed in parallel, for example, by using a GPU. One thing to note is that each update is based on less information by only using \\(m\\) points instead of the entire data set. This can introduce some noise in the learning but at the same time this can help to get out of local minima. In practice, SGD needs more epochs to converge compared to gradient descent but overall, it will take less time. From now on, this is the method we will use to train our networks. Typical batch sizes are: \\(4\\),\\(8\\),\\(16\\),\\(32\\),\\(64\\),\\(128\\), etc. There is a divided opinion in this respect. Some say it’s better to choose small batch sizes but others say the bigger the better. For any particular problem, it is difficult to say what batch size is the optimal. Usually, one needs to choose the batch size empirically by trying different ones. Be aware that when using GPUs, a big batch size can cause out of memory errors since the GPU may not have enough memory to allocate the batch. 8.2 Keras and TensorFlow with R TensorFlow20 is an open-source computational library used mainly for machine learning and more specifically, for deep learning. It has many available tools and extensions to perform a wide variety of tasks such as data pre-processing, model optimization, reinforcement learning, probabilistic reasoning, to name a few. TensorFlow is very flexible and is used for research, development, and in production environments. It provides an API that contains the necessary building blocks to build different types of neural networks including CNNs, Autoencoders, Recurrent Neural Networks, etc. It has two main versions. A CPU version and a GPU version. The latter allows the execution of programs by taking advantage of the computational power of graphic processing units. This makes training models much faster. Despite all this flexibility and power, it can take some time to learn the basics. Sometimes you need a way to build and test machine learning models in a simple way, for example, when trying new ideas or prototyping. Fortunately, there exists an interface to TensorFlow called Keras21. Keras offers an API that abstracts many of the TensorFlow’s details making it easier to build and train machine learning models. Keras is what I will use when building deep learning models in this book. Keras does not only provide an interface to TensorFlow but also to other deep learning engines such as Theano22, Microsoft Cognitive Toolkit23, etc. Keras was developed by François Chollet and later, it was integrated with TensorFlow. Most of the time its API should be enough to do common tasks and it provides ways to add extensions in case that is not enough. In this book, we will use only a subset of all the available Keras functions, but that will be enough for our purposes of building models to predict behaviors. If you want to learn more about Keras, I recommend you the book “Deep Learning with R” by Chollet and Allaire (2018). Examples in this book will use Keras with TensorFlow as the backend. In R, we can access Keras through the keras package (Allaire and Chollet 2019). Instructions on how to install Keras and TensorFlow can be found in Appendix A. At this point, I would recommend you to install them since the next section will make use of Keras. In the next section, we will start with a simple model built with Keras and the following examples will introduce more functions. By the end of this chapter, you will be able to build and train efficient deep neural networks including convolutional neural networks. 8.2.1 Keras Example keras_simple_network.R If you haven’t already installed Keras and TensorFlow, I would recommend you to do so at this point. Instructions on how to install the required software can be found in Appendix A. In the previous section, I showed you how to implement gradient descent in R (see gradient_descent.R). Now, I will show you how to implement the same simple network but using Keras. To recall, our network has one unit, one input, one output, and no bias. The code can be found in the script keras_simple_network.R. First, the keras library is loaded and a sample training set is created. Then, the function keras_model_sequential() is used to instantiate a new empty model. It is called sequential because it is composed of a sequence of layers. At this point it does not have any layers yet. library(keras) # Generate a train set. # First element is the input x and # the second element is the output y. train_set &lt;- data.frame(x = c(3.0,4.0,1.0), y = c(1.5, 2.0, 0.5)) # Instantiate a sequential model. model &lt;- keras_model_sequential() We can now start adding layers (only one in this example). To do so, the layer_dense() method can be used. The dense name means that this will be a densely (fully) connected layer. This layer will be the output layer with a single unit. model %&gt;% layer_dense(units = 1, use_bias = FALSE, activation = &#39;linear&#39;, input_shape = 1) The first argument units = 1 specifies the number of units in this layer. By default, a bias is added in each layer. To make it the same as in the previous example we will not use a bias so use_bias is set to FALSE. The activation specifies the activation function. Here it is set to 'linear' which means that no activation function is applied \\(f(x)=x\\). Finally, we need to specify the number of inputs with input_shape. In this case, there is only one feature. Before training the network we need to compile the model and specify the learning algorithm. In this case, stochastic gradient descent with a learning rate of \\(\\alpha=0.01\\). We also need to specify which loss function we would like to use (we’ll use mean squared error). At every epoch, some performance metrics can be computed. Here, we specify that we want the mean squared error and mean absolute error. These metrics are computed on the train data. After compiling the model the summary() method can be used to print a textual description of it. Figure 8.16 shows the output of the summary() function. model %&gt;% compile( optimizer = optimizer_sgd(lr = 0.01), loss = &#39;mse&#39;, metrics = list(&#39;mse&#39;,&#39;mae&#39;) ) summary(model) Figure 8.16: Summary of the simple neural network. From this output, we can see that the network consists of a single dense layer with \\(1\\) unit. To start the actual training procedure we need to call the fit() function. Its first argument is the input training data (features) as a matrix. The second argument specifies the corresponding true outputs. We will let the algorithm run for \\(30\\) epochs. The batch size is set to \\(3\\) which is also the total number of examples in our data. In this example the dataset is very small so we can just set the batch size equal to the total number of instances. In practice, datasets can contain thousands of instances but the batch size will be relatively small (e.g., \\(8\\), \\(16\\), \\(32\\), etc.). Additionally, there is a validation_split parameter that specifies the fraction of the train data to be used for validation. Here, I set it to \\(0\\) (the default) since the dataset is very small. If the validation split is greater than \\(0\\) its performance metrics will also be computed. The verbose parameter sets the amount of information to be printed during training. A \\(0\\) will not print anything. A \\(2\\) will print one line of information per epoch. The last parameter view_metrics specifies if you want the progress of the loss and performance metrics to be plotted. The fit() function returns an object with summary statistics collected during training that we are saving in the variable history. history &lt;- model %&gt;% fit( as.matrix(train_set$x), as.matrix(train_set$y), epochs = 30, batch_size = 3, validation_split = 0, verbose = 2, view_metrics = TRUE ) Figure 8.17 shows the output of the fit() function in RStudio. In the console, the training loss, mean squared error, and mean absolute error are printed during each epoch. In the viewer pane, plots of the same metrics are shown. Here, we can see that the loss is nicely decreasing over time. The loss at epoch \\(30\\) should be something close to \\(0\\). Figure 8.17: fit() function output. The information saved in the history variable can be plotted with plot(history). This will generate plots for the loss, MSE, and MAE. These results can be slightly different every time the training is run due to random weight initializations made by the back end. Once the model is trained, we can perform inference on new data points with the predict_on_batch() function. Here we are passing three data points. model %&gt;% predict_on_batch(c(7, 50, -220)) #&gt; [,1] #&gt; [1,] 3.465378 #&gt; [2,] 24.752701 #&gt; [3,] -108.911880 From the results, it can be seen that we got more or less the expected results. Try setting a higher learning rate, for example, \\(0.05\\). With this learning rate, the algorithm will converge much faster. In my computer, at epoch \\(11\\) the loss was already \\(0\\). One practical thing to note is that if you make any changes in the compile() or fit() functions you will have to run again the code that instantiates and defines the network. This is because the model object saves the current state including the learned weights. If you run the fit() function again on a previously trained model, it will start with the previously learned weights. 8.3 Classification with Neural Networks Neural networks are trained iteratively by modifying their weights aiming to minimize the loss function. When the network predicts real numbers, the MSE loss function is normally used. For classification problems, the network should predict what is the most likely class out of \\(k\\) possible categories. To make a neural network work for classification problems, we need to introduce new elements to its architecture: Add more units to the output layer. Use a softmax activation function in the output layer. Use average cross-entropy as the loss function. Let’s start with point number \\(1\\) (add more units to the output layer). This means that if the number of classes is \\(k\\), then the last layer needs to have \\(k\\) units, one for each class. That’s it!. Figure 8.18 shows an example of a neural network with an output layer having \\(3\\) units. Each unit predicts a score for each of the \\(3\\) classes. Let’s call the vector of predicted scores \\(y&#39;\\). Figure 8.18: Neural network with 3 output scores. Softmax is applied to the scores and the cross-entropy with the true scores is calculated. This gives us an estimate of the similarity between the network’s predictions and the true values. Point number \\(2\\) says that a softmax activation function should be used in the output layer. When training the network, just as with regression, we need a way to compute the error between the predicted values \\(y&#39;\\) and the true values \\(y\\). In this case, \\(y\\) is a one-hot encoded vector with a \\(1\\) at the position of the true class and \\(0s\\) elsewhere. If you are not familiar with one-hot encoding, you can check the topic in chapter 5. As opposed to other classifiers like decision trees, \\(k\\)-nn, etc., neural networks need the classes to be one-hot encoded. With regression problems, one way to compare the prediction with the true value is by using the squared difference: \\((y&#39; - y)^2\\). With classification, \\(y\\) and \\(y&#39;\\) are vectors so we need a way to compare them. The true values \\(y\\) are represented as a vector of probabilities with a \\(1\\) at the position of the true class. The output scores \\(y&#39;\\) do not necessarily sum up to \\(1\\) thus, they are not proper probabilities. Before comparing \\(y\\) and \\(y&#39;\\) we need them both to be probabilities. The softmax activation function is used to convert \\(y&#39;\\) into a vector of probabilities. The softmax function is applied individually to each element of a vector: \\[\\begin{equation} softmax(\\boldsymbol{x},i) = \\frac{e^{\\boldsymbol{x}_i}}{\\sum_{j}{e^{\\boldsymbol{x}_j}}} \\tag{8.9} \\end{equation}\\] where \\(\\boldsymbol{x}\\) is a vector and \\(i\\) is an index pointing to a particular element in the vector. Thus, to convert \\(y&#39;\\) into a vector of probabilities we need to apply softmax to each of its elements. One thing to note is that this activation function depends on all the values in the vector (the output values of all units). Figure 8.18 shows the resulting vector of probabilities after applying softmax to each element of \\(y&#39;\\). In R this can be implemented like the following: # Scores from the figure. scores &lt;- c(3.0, 0.03, 1.2) # Softmax function. softmax &lt;- function(scores){ exp(scores) / sum(exp(scores)) } probabilities &lt;- softmax(scores) print(probabilities) #&gt; [1] 0.82196136 0.04216934 0.13586930 print(sum(probabilities)) # Should sum up to 1. #&gt; [1] 1 We used R vectorization capabilities to compute the final vector of probabilities within the same function without having to iterate through each element. When using Keras, these operations are efficiently computed by the backend (for example, TensorFlow). Finally, point \\(3\\) states that we need to use average cross-entropy as the loss function. Now that we have converted \\(y&#39;\\) into probabilities, we can compute its dissimilarity with \\(y\\). The distance (dissimilarity) between two vectors (\\(A\\),\\(B\\)) of probabilities can be computed using the cross-entropy: \\[\\begin{equation} CE(A,B) = - \\sum_{i}{B_i log(A_i)} \\tag{8.10} \\end{equation}\\] Thus, to get the dissimilarity between \\(y&#39;\\) and \\(y\\) first we apply softmax to \\(y&#39;\\) (to transform it into proper probabilities) and then, we can compute the cross entropy between the resulting vector of probabilities and \\(y\\): \\[\\begin{equation} CE(softmax(y&#39;),y). \\end{equation}\\] In R this can be implemented with the following: # Cross-entropy CE &lt;- function(A,B){ - sum(B * log(A)) } y &lt;- c(1, 0, 0) print(CE(softmax(scores), y)) #&gt; [1] 0.1960619 Be aware that when computing the cross-entropy with equation (8.10) order matters. The first element should be the predicted scores \\(y&#39;\\) and the second element should be the true one-hot encoded vector \\(y\\). We don’t want to apply a log function to a vector with values of \\(0\\). Most of the time, the predicted scores \\(y&#39;\\) will be different from \\(0\\) that’s why we prefer to apply the log function to them. In the very rare case when the predicted scores have zeros, we can add a very small number. In practice, this is taken care of by the backend (e.g., Tensorflow). Now we know how to compute the cross-entropy for each training instance. The total loss function is then, the average cross-entropy across the training points. The next section shows how to build a neural network for classification using Keras. 8.3.1 Classification of Electromyography Signals keras_electromyography.R In this example, we will train a neural network with Keras to classify hand gestures based on muscle electrical activity. The ELECTROYMYOGRAPHY dataset will be used here. The electrical activity was recorded with an electromyography (EMG) sensor worn as an armband. The data were collected and made available by Yashuk (2019). The armband device has \\(8\\) sensors which are placed on the skin surface and measure electrical activity from the right forearm at a sampling rate of \\(200\\) Hz. A video of the device can be found here: https://youtu.be/1u5-G6DPtkk The data contains \\(4\\) different gestures: 0-rock, 1-scissors, 2-paper, 3-OK, and has \\(65\\) columns. The last column is the class label from \\(0\\) to \\(3\\). The first \\(64\\) columns are electrical measurements. \\(8\\) consecutive readings for each of the \\(8\\) sensors. The objective is to use the first \\(64\\) variables to predict the class. The script keras_electromyography.R has the full code. We start by splitting the dataset into train (\\(60\\%\\)), validation (\\(10\\%\\)) and test (\\(30\\%\\)) sets. We will use the validation set to monitor the performance during each epoch. We also need to normalize the three sets but only learning the normalization parameters from the train set. The normalize() function included in the script will do the job. One last thing we need to do is to format the data as matrices and one-hot encode the class. The following code defines a function that takes as input a data frame and the expected number of classes. It assumes that the first columns are the features and the last column contains the class. First, it converts the features into a matrix and stores them in x. Then, it converts the class into an array and one-hot encodes it using the to_categorical() function from Keras. The classes are stored in y and the function returns a list with the features and one-hot encoded classes. Then, we can call the function with the train, validation, and test sets. # Define a function to format features and one-hot encode the class. format.to.array &lt;- function(data, numclasses = 4){ x &lt;- as.matrix(data[, 1:(ncol(data)-1)]) y &lt;- as.array(data[, ncol(data)]) y &lt;- to_categorical(y, num_classes = numclasses) l &lt;- list(x=x, y=y) return(l) } # Format data trainset &lt;- format.to.array(trainset, numclasses = 4) valset &lt;- format.to.array(valset, numclasses = 4) testset &lt;- format.to.array(testset, numclasses = 4) Let’s print the first one-hot encoded classes from the train set: head(trainset$y) #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 0 0 1 0 #&gt; [2,] 0 0 1 0 #&gt; [3,] 0 0 1 0 #&gt; [4,] 0 0 0 1 #&gt; [5,] 1 0 0 0 #&gt; [6,] 0 0 0 1 The first three instances belong to the class ‘paper’ because the \\(1s\\) are in the third position. The corresponding integers are 0-rock, 1-scissors, 2-paper, 3-OK. So ‘paper’ comes in the third position. The fourth instance belongs to the class ‘OK’, the fifth to ‘rock’, and so on. Now it’s time to define the neural network architecture! We will do so inside a function: # Define the network&#39;s architecture. get.nn &lt;- function(ninputs = 64, nclasses = 4, lr = 0.01){ model &lt;- keras_model_sequential() model %&gt;% layer_dense(units = 32, activation = &#39;relu&#39;, input_shape = ninputs) %&gt;% layer_dense(units = 16, activation = &#39;relu&#39;) %&gt;% layer_dense(units = nclasses, activation = &#39;softmax&#39;) model %&gt;% compile( loss = &#39;categorical_crossentropy&#39;, optimizer = optimizer_sgd(lr = lr), metrics = c(&#39;accuracy&#39;) ) return(model) } The first argument takes the number of inputs (features), the second argument specifies the number of classes and the last argument is the learning rate \\(\\alpha\\). The first line instantiates an empty keras sequential model. Then we add three layers. The first two are hidden layers and the last one will be the output layer. The input layer is implicitly defined when setting the input_shape parameter in the first layer. The first hidden layer has \\(32\\) units with a ReLU activation function. Since this is the first hidden layer we also need to specify what is the expected input by setting the input_shape. In this case, it is the number of inputs which will be \\(64\\) as it is the number of features. The next hidden layer has \\(16\\) ReLU units. For the output layer, the number of units needs to be equal to the number of classes (\\(4\\) in this case). Since this is a classification problem we also set the activation function to softmax. Then, the model is compiled and the loss function is set to categorical_crossentropy because this is a classification problem. Stochastic gradient descent is used with a learning rate passed as a parameter. During training, we want to monitor the accuracy. Finally, the function returns the compiled model. Now we can call our function to create a model. This one will have \\(64\\) inputs, \\(4\\) outputs and we will use a learning rate of \\(0.01\\). It is always useful to print a summary of the model with the summary() function. model &lt;- get.nn(64, 4, lr = 0.01) summary(model) Figure 8.19: Summary of the network. From the summary, we can see that the network has \\(3\\) layers. The second column shows the output shape which in this case corresponds to the number of units in each layer. The last column shows the number of parameters of each layer. For example, the first layer has \\(2080\\) parameters! Those come from the weights and biases. There are \\(64\\) (inputs) * \\(32\\) (units) = \\(2048\\) weights plus the \\(32\\) biases (one for each unit). The biases are included by default on each layer unless otherwise specified. The second layer receives \\(32\\) inputs on each of its \\(16\\) units. Thus \\(32\\) * \\(16\\) + \\(16\\) (biases) = \\(528\\). The last layer has \\(16\\) inputs from the previous layer on each of its \\(4\\) units plus \\(4\\) biases giving a total of \\(68\\) parameters. In total, the network has \\(2676\\) parameters. Here, we can see how fast the number of parameters grows when adding more layers and units. Now, we can use the fit() function to train the model. history &lt;- model %&gt;% fit( trainset$x, trainset$y, epochs = 300, batch_size = 8, validation_data = list(valset$x, valset$y), verbose = 1, view_metrics = TRUE ) The model is trained for \\(300\\) epochs with a batch size of \\(8\\). We used the validation_data parameter to specify the validation set to compute the performance on unseen data. The training will take some minutes to complete. Bigger models can take hours or even several days. Thus, it is a good idea to save a model once it is trained. # Save model. save_model_hdf5(model, &quot;electromyography.hdf5&quot;) We can load a previously saved model with: # Load model. model &lt;- load_model_hdf5(&quot;electromyography.hdf5&quot;) Figure 8.20 shows the train and validation loss and accuracy as produced by plot(history). We can see that both the training and validation loss are decreasing over time. The accuracy increases over time. Figure 8.20: Loss and accuracy of the electromyography model. Now, we can evaluate the performance of the trained model with the test set using the evaluate() function. # Evaluate model. model %&gt;% evaluate(testset$x, testset$y) #&gt; loss accuracy #&gt; 0.4045424 0.8474576 The accuracy was pretty decent (\\(\\approx 84\\%\\)). If you want to get the actual class predictions you can use the predict_classes() function. # Predict classes. classes &lt;- model %&gt;% predict_classes(testset$x) head(classes) #&gt; [1] 2 2 1 3 0 1 Note that this function returns the classes with numbers starting with \\(0\\) just as in the original dataset. Sometimes it is also useful to get the actual predicted scores for each class. This can be done with the predict_on_batch() function. # Make predictions on the test set. predictions &lt;- model %&gt;% predict_on_batch(testset$x) head(predictions) #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1.957638e-05 8.726048e-02 7.708290e-01 1.418910e-01 #&gt; [2,] 3.937355e-05 2.571992e-04 9.965665e-01 3.136863e-03 #&gt; [3,] 4.261451e-03 7.343097e-01 7.226156e-02 1.891673e-01 #&gt; [4,] 8.669784e-06 2.088269e-04 1.339851e-01 8.657974e-01 #&gt; [5,] 9.999956e-01 7.354113e-26 1.299388e-08 4.451362e-06 #&gt; [6,] 2.513005e-05 9.914154e-01 7.252949e-03 1.306421e-03 If we want to get the actual classes from the scores we can get the index of the maximum column. Then we subtract \\(-1\\) so classes start at \\(0\\). classes &lt;- max.col(predictions) - 1 head(classes) #&gt; [1] 2 2 1 3 0 1 Since the true classes are also one-hot encoded we need to do the same to get the ground truth. groundTruth &lt;- max.col(testset$y) - 1 # Compute accuracy. sum(classes == groundTruth) / length(classes) #&gt; [1] 0.8474576 We can convert the integers to class strings by mapping them and then generate a confusion matrix. # Convert classes to strings. # Class mapping by index: rock 0, scissors 1, paper 2, ok 3. mapping &lt;- c(&quot;rock&quot;, &quot;scissors&quot;, &quot;paper&quot;, &quot;ok&quot;) # Need to add 1 because indices in R start at 1. str.predictions &lt;- mapping[classes+1] str.groundTruth &lt;- mapping[groundTruth+1] library(caret) cm &lt;- confusionMatrix(as.factor(str.predictions), as.factor(str.groundTruth)) cm$table #&gt; Reference #&gt; Prediction ok paper rock scissors #&gt; ok 681 118 24 27 #&gt; paper 54 681 47 12 #&gt; rock 29 18 771 1 #&gt; scissors 134 68 8 867 Try to modify the network by making it deeper (adding more layers) and fine-tune the hyperparameters like the learning rate, batch size, etc. to increase the performance. 8.4 Overfitting One important thing to look at when training a network is overfitting. That is, when the model memorizes instead of learning (see chapter 1). Overfitting means that the model becomes very specialized at mapping inputs to outputs from the train set but fails to do so with new test samples. One of the reasons is that a model can become too complex and with so many parameters that it will perfectly adapt to its training data but will miss more general patterns that allow it to perform well on unseen instances. To control for this, one can plot loss/accuracy curves during training epochs. Figure 8.21: Loss and accuracy curves. In Figure 8.21 we can see that after some epochs the validation loss starts to increase even though the train loss is still decreasing. This is because the model is getting better on reducing the error on the train set but its performance starts to decrease when presented with new instances. Conversely, one can observe a similar effect with the accuracy. The model keeps improving its performance on the train set but at some point, the accuracy on the validation set starts to decrease. Usually, one stops the training before overfitting starts to occur. In the following, I will introduce you to two common techniques to combat overfitting in neural networks. 8.4.1 Early Stopping keras_electromyography_earlystopping.R Neural networks are trained for several epochs using gradient descent. But the question is: For how many epochs?. As can be seen in Figure 8.21, too many epochs can lead to overfitting and too few can cause underfitting. Early stopping is a simple but effective method to reduce the risk of overfitting. The method consists of setting a large number of epochs and stop updating the network’s parameters when a condition is met. For example, one condition can be to stop when there is no performance improvement on the validation set after \\(n\\) epochs or when there is a decrease of some percent in accuracy. Keras provides some mechanisms to implement early stopping and this is accomplished via callbacks. A callback is a function that is run at different stages during training such as at the beginning or end of an epoch or at the beginning or end of a batch, etc. Callbacks are passed as a list to the fit() function. You can define custom callbacks or use some of the built-in ones including callback_early_stopping(). This callback will cause the training to stop when a metric stops improving. The metric can be accuracy, loss, etc. The following callback will stop the training if after \\(10\\) epochs (patience) there is no improvement of at least \\(1\\%\\) (min_delta) in accuracy on the validation set. callback_early_stopping(monitor = &quot;val_acc&quot;, min_delta = 0.01, patience = 10, verbose = 1, mode = &quot;max&quot;) The min_delta parameter specifies the minimum change in the monitored metric to qualify as an improvement. The mode specifies if training should be stopped when the metric has stopped decreasing if it is set to \"min\". If it is set to \"max\", training will stop when the monitored metric has stopped increasing. It may be the case that the best validation performance was achieved not by the model in the last epoch but at some previous point. By setting the restore_best_weights parameter to TRUE the model weights from the epoch with the best value of the monitored metric will be restored. The script keras_electromyography_earlystopping.R shows how to use the early stopping callback in Keras with the electromyography dataset. The following code is an extract that shows how to define the callback and pass it to the fit() function. # Define early stopping callback. my_callback &lt;- callback_early_stopping(monitor = &quot;val_acc&quot;, min_delta = 0.01, patience = 50, verbose = 1, mode = &quot;max&quot;, restore_best_weights = TRUE) history &lt;- model %&gt;% fit( trainset$x, trainset$y, epochs = 500, batch_size = 8, validation_data = list(valset$x, valset$y), verbose = 1, view_metrics = TRUE, callbacks = list(my_callback) ) This code will cause the training to stop if after \\(50\\) epochs there is no improvement in accuracy of at least \\(1\\%\\) and will restore the model’s weights to the ones during the epoch with the highest accuracy. The following Figure shows how the training stopped at epoch \\(237\\). Figure 8.22: Early stopping example. If we evaluate the final model on the test set we see that the accuracy is \\(85.2\\%\\) a small increase compared to the \\(84\\%\\) that we got when training for \\(300\\) epochs without early stopping. # Evaluate model. model %&gt;% evaluate(testset$x, testset$y) #&gt; $loss #&gt; [1] 0.4202231 #&gt; $acc #&gt; [1] 0.8525424 8.4.2 Dropout Dropout is another technique used to reduce overfitting proposed by Srivastava et al. (2014). It consists of ‘dropping’ some of the units from a hidden layer for each sample during training. In theory, it can also be applied to input and output layers but that is not very common. The incoming and outgoing connections of a dropped unit are discarded. Figure 8.23 shows an example of applying dropout to a network. In b), the middle unit was removed from the network whereas in c), the top and bottom units were removed. Figure 8.23: Dropout example. Each unit has an associated probability \\(p\\) (independent of other units) of being dropped. This probability is another hyperparameter but typically it is set to \\(0.5\\). Thus, during each iteration and for each sample, half of the units are discarded. The effect of this, is having more simple networks (see Figure 8.23) and thus, less prone to overfitting. Intuitively, you can also think of dropout as training an ensemble of neural networks, each having a slightly different structure. From the perspective of one unit that receives inputs from the previous hidden layer with dropout, approximately half of its incoming connections will be gone (if \\(p=0.5\\)). See Figure 8.24. Figure 8.24: Incoming connections to one unit when the previous layer has dropout. Dropout has the effect of making units not to rely on any single incoming connection, thus, this makes the whole network able to compensate for the lack of connections by learning alternative paths. In practice and for many applications, this can result in a more robust model. A side effect of applying dropout is that the expected value of the activation function of a unit will be diminished because half of the previous activations will be \\(0\\). Recall that the output of a neuron is computed as: \\[\\begin{equation} f(\\boldsymbol{x}) = g(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b) \\end{equation}\\] where \\(\\boldsymbol{x}\\) contains the input values from the previous layer, \\(\\boldsymbol{w}\\) the corresponding weights and \\(g()\\) is the activation function. With dropout, approximately half of the values of \\(\\boldsymbol{x}\\) will be \\(0\\). To compensate for that, the input values need to be scaled, in this case, by a factor of \\(2\\). \\[\\begin{equation} f(\\boldsymbol{x}) = g(\\boldsymbol{w} \\cdot 2 \\boldsymbol{x} + b) \\end{equation}\\] In modern implementations, this scaling is done during training so at inference time there is no need to apply dropout. The predictions are done as usual. In Keras, the layer_dropout() can be used to add dropout to any layer. Its parameter rate is a float between \\(0\\) and \\(1\\) that specifies the fraction of units to drop. The following code snippet builds a neural network with \\(2\\) hidden layers. Then, dropout with a rate of \\(0.5\\) is applied to both of them. model &lt;- keras_model_sequential() model %&gt;% layer_dense(units = 256, activation = &#39;relu&#39;, input_shape = 1000) %&gt;% layer_dropout(0.5) %&gt;% layer_dense(units = 128, activation = &#39;relu&#39;) %&gt;% layer_dropout(0.5) %&gt;% layer_dense(units = 2, activation = &#39;softmax&#39;) It is very common to apply dropout to networks in computer vision because the inputs are images or videos containing a lot of input values (pixels) but the number of samples is often very limited causing overfitting. In section 8.6 convolutional neural networks (CNNs) will be introduced which are suitable for computer vision problems. In the corresponding smile detection example (section 8.8), we will use dropout. When building CNNs, dropout is almost always added to the different layers. 8.5 Fine-Tuning a Neural Network When deciding for a neural network’s architecture, no formula will tell you how many hidden layers or number of units each layer should have. There is also no formula for determining the batch size, the learning rate, type of activation function, for how many epochs should we train the network, and so on. All those are called the hyperparameters of the network. Hyperparameter tuning is a complex optimization problem and there is a lot of research going on that tackles the issue from different angles. My suggestion is to start with a simple architecture that has been used before to solve a similar problem and fine-tune it for your specific task. If you are not aware of any network that has been used for a similar problem, there are still some guidelines (described below) to get you started. Always keep in mind that those are only recommendations, so you do not need to abide by them and you should feel free to try configurations that deviate from those guidelines depending on your problem at hand. Training neural networks is a time-consuming process, especially in deep networks. Training a network can take from several minutes to weeks. In many cases, performing cross-validation is not feasible. A common practice is to divide the data into train/validation/test sets. The training data is used to train a network with a given architecture and a set of hyperparameters. The validation set is used to evaluate the generalization performance of the network. Then, you can try different architectures and hyperparameters and evaluate the performance again and again with the validation set. Typically, the network’s performance is monitored during training epochs by plotting the loss and accuracy of the train and validation sets. Once you are happy with your model, you test its performance on the test set only once and that is the result that is reported. Here are some starting point guidelines, however, also take into consideration that those hyperparameters can be dependent on each other. So, if you modify a hyperparameter it may impact other(s). Number of hidden layers. Most of the time one or two hidden layers should be enough to solve not too complex problems. The advice here is to start with one hidden layer and if that one is not enough to capture the complexity of the problem, then add another layer and so on. Number of units. If a network has too few units it can underfit, that is, the model will be too simple to capture the underlying data patterns. If the network has too many units this can result in overfitting. Also, it will take more time to learn the parameters. Some guidelines mention that the number of units should be somewhere between the number of input features and the number of units in the output layer24. Huang (2003) has even proposed a formula for the two-hidden layer case to calculate the number of units that are enough to learn \\(N\\) samples: \\(2\\sqrt{(m+2)N}\\) where \\(m\\) is the number of output units. But like this, there are many other similar formulas. My suggestion is to first gain some practice and intuition with simple problems and a good way to do this is with the TensorFlow playground (https://playground.tensorflow.org/). This is a web-based implementation of a neural network that you can fine-tune to solve a predefined set of classification and regression problems. For example, Figure 8.25 shows how I tried to solve the XOR problem with a neural network with \\(1\\) hidden layer and \\(1\\) unit with a sigmoid activation function. After more than \\(1,000\\) epochs the loss is still quite high (\\(0.3\\)). Try to add more neurons and/or hidden layers and see if you can solve the XOR problem with fewer epochs. Figure 8.25: TensorFlow playground. Batch size. Batch sizes range between \\(4\\) and \\(512\\). Big batch sizes provide a better estimate of the gradient but are more computationally expensive. On the other hand, small batch sizes are faster to compute but will incur in more noise in the gradient estimation requiring more epochs to converge. When using a GPU or other specialized hardware, the computations can be performed in parallel thus, allowing bigger batch sizes to be computed in a reasonable time. Some people argue that the noise introduced with small batch sizes is good to escape from local minima. Keskar et al. (2016) showed that in practice, big batch sizes can result in degraded models. A good starting point is \\(32\\) which is the default in Keras. Learning rate. This is one of the most important hyperparameters. The learning rate specifies how fast gradient descent ‘moves’ when trying to find an optimal minimum. However, this doesn’t mean that the algorithm will learn faster if the learning rate is set to a high value. If it is too high, the loss can start oscillating. If it is too low, the learning will take a lot of time. One way to fine-tune it is to start with the default one. In Keras, the default learning rate for stochastic gradient descent is \\(0.01\\). Then, based on the loss plot across epochs, you can decrease/increase it. If learning is taking long, try to increase it. If the loss seems to be oscillating or stock try reducing it. Typical values are \\(0.1\\), \\(0.01\\), \\(0.001\\), \\(0.0001\\), \\(0.00001\\). Additionally to stochastic gradient descent, Keras provides implementations of other optimizers25 like Adam26 which have adaptive learning rates, but still, one needs to specify an initial one. 8.6 Convolutional Neural Networks Convolutional neural networks or CNNs for short, have become extremely popular due to their capacity to solve computer vision problems. Most of the time they are used for image classification tasks but can also be used for regression and for time series data. If we wanted to perform image classification with a traditional neural network, first we would need to either build a feature vector by: extracting features from the image or, flattening the image pixels into a 1D array. The first solution requires a lot of image processing expertise and domain knowledge. Extracting features from images is not a trivial task and requires a lot of preprocessing to reduce noise, artifacts, segment the objects of interest, remove background, etc. Additionally, considerable effort is spent on feature engineering. The drawback of the second solution is that spatial information is lost, that is, the relationship between neighboring pixels. CNNs solve the two previous problems by automatically extracting features while preserving spatial information. As opposed to traditional networks, CNNs can take as input \\(n\\)-dimensional images and process them efficiently. The main building blocks of a CNN are: Convolution layers Pooling operations Traditional fully connected layers Figure 8.26 shows a simple CNN and its basic components. First, the image goes through a convolution layer with \\(4\\) kernels (details about the convolution operation are described below). This layer is in charge of extracting features by applying the kernels on top of the image. The result of this operation is a convolved image, also known as feature maps. The number of feature maps is equal to the number of kernels, in this case, \\(4\\). Then, a pooling operation is applied on top of the feature maps. This operation reduces the size of the feature maps by downsampling them (details on this below). The output of the pooling operation is a set of feature maps with reduced size. Here, the outputs are \\(4\\) reduced feature maps since the pooling operation is applied to each feature map independently of the others. Then, the feature maps are flattened into a one-dimensional array. Conceptually, this array represents all the features extracted from the previous steps. These features are then used as inputs to a neural network with its respective input, hidden, and output layers. An ’*’ and underlined text means that parameter learning occurs in that layer. For example, in the convolution layer, the parameters of the kernels need to be learned. On the other hand, the pooling operation does not require parameter learning since it is a fixed operation. Finally, the parameters of the neural network are learned too, including the hidden layers and the output layer. Figure 8.26: Simple CNN architecture. An ’*’ indicates where parameter learning occurs. One can build more complex CNNs by stacking more convolution layers and pooling operations. By doing so, the level of abstraction increases. For example, the first convolution extracts simple features like horizontal, vertical, diagonal lines, etc. The next convolution could extract more complex features like squares, triangles, and so on. The parameter learning of all layers (including the convolution layers) occurs during the same forward and backpropagation step just as with a normal neural network. Both, the features and the classification task are learned at the same time! During learning, batches of images are forward propagated and the parameters are adjusted accordingly to minimize the error (for example, the average cross-entropy for classification). The same methods for training normal neural networks are used for CNNs, for example, stochastic gradient descent. Each kernel in a convolution layer can have an associated bias which is also a parameter to be learned. By default, Keras uses a bias for each kernel. Furthermore, an activation function can be applied to the outputs of the convolution layer. This is applied element-wise. ReLU is the most common one. At inference time, the convolution layers and pooling operations act as feature extractors by generating feature maps that are ultimately flattened and passed to a normal neural network. It is also common to use the first layers as feature extractors and then replace the neural network with another model (Random Forest, SVM, etc.). In the following sections, details about the convolution and pooling operations are presented. 8.6.1 Convolutions Convolutions are used to automatically extract feature maps from images. A convolution operation consists of a kernel also known as a filter which is a matrix with real values. Kernels are usually much smaller than the original image. For example, for a grayscale image of height and width of \\(100\\)x\\(100\\) a typical kernel size would be \\(3\\)x\\(3\\). The size of the kernel is a hyperparameter. The convolution operation consists of applying the kernel over the image staring at the upper left corner and moving forward row by row until reaching the bottom right corner. The stride controls how many elements the kernel is moved at a time and this is also a hyperparameter. A typical value for the stride is \\(1\\). The convolution operation computes the sum of the element-wise product between the kernel and the image region it is covering. The output of this operation is used to generate the convolved image (feature map). Figure 8.27 shows the first two iterations and the final iteration of the convolution operation on an image. In this case, the kernel is a \\(3\\)x\\(3\\) matrix with \\(1\\)s in its first row and \\(0\\)s elsewhere. The original image has a size of \\(5\\)x\\(5\\)x\\(1\\) (height, width, depth) and it seems to have the number \\(7\\) in it. Figure 8.27: Convolution operation with a kernel of size 3x3 and stride=1. Iterations 1, 2 and 9. In the first iteration, the kernel is aligned with the upper left corner of the original image. An element-wise multiplication is performed and the results are summed. The operation is shown at the top of the figure. In the first iteration, the result was \\(3\\) and it is set at the corresponding position of the final convolved image (feature map). In the next iteration, the kernel is moved one position to the right and again, the final result is \\(3\\) which is set in the next position of the convolved image. The process continues until the kernel reaches the bottom right corner. At the last iteration (9), the result is \\(1\\). Now, the convolved image (feature map) represents the features extracted by this particular kernel. Also, note that the feature map is a \\(3\\)x\\(3\\) matrix which is smaller than the original image. It is also possible to force the feature map to have the same size as the original image by padding it with zeros. Before learning starts, the kernel values are initialized at random. In this example, the kernel has \\(1\\)s in the first row and it has \\(3\\)x\\(3=9\\) parameters. This is whats makes CNNs so efficient since the same kernel is applied to the entire image. This is known as ‘parameter sharing’. Our kernel has \\(1\\)s at the top and zeros elsewhere so it seems that this kernel learned to detect horizontal lines. If we look at the final convolved image we see that the horizontal lines were emphasized by this kernel. This would be a good candidate kernel to differentiate between \\(7\\)s and \\(0\\)s, for example. Since \\(0\\)s does not have long horizontal lines. But maybe it will have difficulties discriminating between \\(7\\)s and \\(5\\)s since both have horizontal lines at the top. In this example, only \\(1\\) kernel was used but in practice, you may want to have more kernels, each in charge of identifying the best features for the given problem. For example, another kernel could learn to identify diagonal lines which would be useful to differentiate between \\(7\\)s and \\(5\\)s. The number of kernels per convolution layer is a hyperparameter. In the previous example, we could have defined to have \\(4\\) kernels instead of one. In that case, the output of that layer would have been \\(4\\) feature maps of size \\(3\\)x\\(3\\) each (Figure 8.28). Figure 8.28: A convolution with 4 kernels. The output is 4 feature maps. What would be the output of a convolution layer with \\(4\\) kernels of size \\(3\\)x\\(3\\) if it is applied to an RGB color image of size \\(5\\)x\\(5\\)x\\(3\\))? In that case, the output will be the same (\\(4\\) feature maps of size \\(3\\)x\\(3\\)) as if the image were in grayscale (\\(5\\)x\\(5\\)x\\(1\\)). Remember that the number of output feature maps is equal to the number of kernels regardless of the depth of the image. However, in this case, each kernel will have a depth of \\(3\\). Each depth is applied independently to the corresponding R, G, and B image channels. Thus, each kernel has \\(3\\)x\\(3\\)x\\(3=27\\) parameters that need to be learned. After applying each kernel to each image channel (in this example, \\(3\\) channels), the results of each channel are added and this is why we end up with one feature map per kernel. The following course website has a nice interactive animation of how convolutions are applied to an image with \\(3\\) channels: https://cs231n.github.io/convolutional-networks/. In the next section (CNNs with Keras), a couple of examples that demonstrate how to calculate the number of parameters and the outputs’ shape will be presented as well. 8.6.2 Pooling Operations Pooling operations are typically applied after convolution layers. Their purpose is to reduce the size of the data and to emphasize important regions. These operations perform a fixed computation on the image and do no have learnable parameters. Similar to kernels, we need to define a window size. Then, this window is moved throughout the image and a computation is performed on the pixels covered by the window. The difference with kernels is that this window is just a guide but do not have parameters to be learned. The most common pooling operation is max pooling which consists of selecting the highest value. Figure 8.29 shows an example of a max pooling operation on a \\(4\\)x\\(4\\) image. The window size is \\(2\\)x\\(2\\) and the stride is \\(2\\). The latter means that the window moves \\(2\\) places at a time. Figure 8.29: Max pooling with a window of size 2x2 and stride = 2. The result of this operation is an image of size \\(2\\)x\\(2\\) which is half of the original one. Aside from max pooling, average pooling can be applied instead. In that case, it computes the mean value across all values covered by the window. 8.7 CNNs with Keras keras_cnns.R Keras provides several functions to define convolution layers and pooling operations. In TensorFlow, image dimensions are specified with the following order: height, width, and depth. In Keras, the layer_conv_2d() function is used to add a convolution layer to a sequential model. This function has several arguments but the \\(6\\) most common ones are: filters,kernel_size,strides,padding,activation, and input_shape. # Convolution layer. layer_conv_2d(filters = 4, # Number of kernels. kernel_size = c(3,3), # Kernel size. strides = c(1,1), # Stride. padding = &quot;same&quot;, # Type of padding. activation = &#39;relu&#39;, # Activation function. input_shape = c(5,5,1)) # Input image dimensions. # Only specified in first layer. The filters param specifies the number of kernels. The kernel_size specifies the kernel size (height, width). The strides is an integer or list of \\(2\\) integers, specifying the strides of the convolution along the width and height (the default is \\(1\\)). The padding can take two possible strings: \"same\" or \"valid\". If padding=\"same\" the input image will be padded with zeros based on the kernel size and strides such that the convolved image has the same size as the original one. If padding=\"valid\" it means no padding is applied. The default is \"valid\". The activation parameter takes as input a string with the name of the activation function to use. The input_shape parameter is required when this layer is the first one and specifies the dimensions of the input image. To add a max pooling operation you can use the layer_max_pooling_2d() function. Its most important parameter is pool_size. layer_max_pooling_2d(pool_size = c(2, 2)) The pool_size specifies the window size (height, width). By default, the strides will be equal to pool_size but if desired, this can be changed with the strides parameter. This function also accepts a padding parameter similar to the one for layer_max_pooling_2d(). In Keras, if the stride is not specified, it defaults to the window size (pool_size parameter). To illustrate this convolution and pooling operations I will use two simple examples. The complete code for the two examples can be found in the script keras_cnns.R. 8.7.1 Example 1 Let’s create our first CNN in Keras. For now, this CNN will not be trained but only its architecture will be defined. The objective is to understand the building blocks of the network. In the next section, we will build and train a CNN that detects smiles from image faces. Our network will consist of \\(1\\) convolution layer, \\(1\\) max pooling layer, \\(1\\) fully connected hidden layer, and \\(1\\) output layer as if this were a classification problem. The code to build such a network is shown below and the output of the summary() function in Figure 8.30. library(keras) model &lt;- keras_model_sequential() model %&gt;% layer_conv_2d(filters = 4, kernel_size = c(3,3), padding = &quot;valid&quot;, activation = &#39;relu&#39;, input_shape = c(10,10,1)) %&gt;% layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;% layer_flatten() %&gt;% layer_dense(units = 32, activation = &#39;relu&#39;) %&gt;% layer_dense(units = 2, activation = &#39;softmax&#39;) summary(model) Figure 8.30: Output of summary(). The first convolution layer has \\(4\\) kernels of size \\(3\\)x\\(3\\) and a ReLU as the activation function. The padding is set to \"valid\" so no padding will be done. The input image has a size of \\(10\\)x\\(10\\)x\\(1\\) (height, width, depth). Then, we are applying max pooling with a window size of \\(2\\)x\\(2\\). Later, the output is flattened and fed into a fully connected layer with \\(32\\) units. Finally, the output layer as \\(2\\) units with a softmax activation function for classification. From the summary, if you look at the output of the first Conv2D layer it shows (None, 8, 8, 4). The ‘None’ means that the number of input images is not fixed and will depend on the batch size. The next two numbers correspond to the height and width which are both \\(8\\). This is because the image was not padded and after applying the convolution operation on the original \\(10\\)x\\(10\\) height and width image, its dimensions are reduced to \\(8\\). The last number (\\(4\\)) is the number of feature maps which is equal to the number of kernels (filters=4). The number of parameters is \\(40\\) (last column). This is because there are \\(4\\) kernels with \\(3\\)x\\(3=9\\) parameters each, and there is one bias per kernel included by default: \\(4 \\times 3 \\times 3 \\times + 4 = 40\\). The output of MaxPooling2D is (None, 4, 4, 4). The height and width are \\(4\\) because the pool size was \\(2\\) and the stride was \\(2\\). This had the effect of reducing to half the height and width of the output of the previous layer. Max pooling preserves the number of feature maps, thus, the last number is \\(4\\) (the number of feature maps from the previous layer). Max pooling does not have any learnable parameters since it applies a fixed operation every time. Before passing the downsampled feature maps to the next fully connected layer they need to be flattened into a \\(1\\)-dimensional array. This is done with the layer_flatten() function and its output has a shape of (None, 64) which corresponds to the \\(4 \\times 4 \\times 4 =64\\) features of the previous layer. The next fully connected layer has \\(32\\) units each with a connection with every one of the \\(64\\) input features. Each unit has a bias. Thus the number of parameters is \\(64 \\times 32 + 32 = 2080\\). Finally the output layer has \\(32 \\times 2 + 2=66\\) parameters. And the entire network has \\(2,186\\) parameters! Now, you can try to modify, the kernel size, the strides, the padding, and input shape and see how the output dimensions and the number of parameters vary. 8.7.2 Example 2 Now let’s try another example but this time the input image will have a depth of \\(3\\) simulating an RGB image. model2 &lt;- keras_model_sequential() model2 %&gt;% layer_conv_2d(filters = 16, kernel_size = c(3,3), padding = &quot;same&quot;, activation = &#39;relu&#39;, input_shape = c(28,28,3)) %&gt;% layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;% layer_flatten() %&gt;% layer_dense(units = 64, activation = &#39;relu&#39;) %&gt;% layer_dense(units = 5, activation = &#39;softmax&#39;) summary(model2) Figure 8.31: Output of summary(). The output height and width of the first Conv2D layer is \\(28\\) which is the same as the input image size. This is because this time we set padding = \"same\" and the image dimensions were preserved. The \\(16\\) corresponds to the number of feature maps which was set with filters = 16. The total parameter count for this layer is \\(448\\). Each kernel has \\(3 \\times 3 = 9\\) parameters. There are \\(16\\) kernels but each kernel has a \\(depth=3\\) because the input image is RGB. \\(9 \\times 16[kernels] \\times 3[depth] + 16[biases] = 448\\). Notice that even though each kernel has a depth of \\(3\\) the output number of feature maps of this layer is \\(16\\) and not \\(16 \\times 3 = 48\\). This is because as mentioned before, each kernel produces a single feature map regardless of the depth because the values are summed depth-wise. The rest of the layers are similar to the previous example. 8.8 Smiles Detection with a CNN keras_smile_detection.R In this section, we will build a CNN that detects smiling and non-smiling faces from pictures from the SMILES dataset. This information could be used, for example, to analyze smiling patterns during job interviews, exams, etc. For this task, we will use a cropped (Sanderson and Lovell 2009) version of the Labeled Faces in the Wild (LFW) database (Huang et al. 2008). A subset of the database was labeled by Arigbabu et al. (2016), Arigbabu (2017). The labels are provided as two text files, each, containing the list of files that correspond to smiling and non-smiling faces. The dataset can be downloaded from: http://conradsanderson.id.au/lfwcrop/ and the labels list from: https://data.mendeley.com/datasets/yz4v8tb3tp/5. See Appendix B for instructions on how to setup the dataset. The smiling set has \\(600\\) pictures and the non-smiling has \\(603\\) pictures. Figure 8.32 shows an example of one image from each of the sets. Figure 8.32: Example images of the smiling dataset. The script keras_smile_detection.R has the full code of the analysis. First, we load the list of smiling pictures. datapath &lt;- file.path(datasets_path,&quot;smiles&quot;) smile.list &lt;- read.table(paste0(datapath,&quot;SMILE_list.txt&quot;)) head(smile.list) #&gt; V1 #&gt; 1 James_Jones_0001.jpg #&gt; 2 James_Kelly_0009.jpg #&gt; 3 James_McPherson_0001.jpg #&gt; 4 James_Watt_0001.jpg #&gt; 5 Jamie_Carey_0001.jpg #&gt; 6 Jamie_King_0001.jpg # Substitute jpg with ppm. smile.list &lt;- gsub(&quot;jpg&quot;, &quot;ppm&quot;, smile.list$V1) The SMILE_list.txt points to the names of pictures in jpg format but we have them in ppm format so the jpg extension is replaced by ppm with the gsub() function. Since the images are in ppm format, we can use the pixmap library (Bivand, Leisch, and Maechler 2011) to read and plot them. The print() function can be used to display the image properties. Here, we can see that these are RGB images of \\(64\\)x\\(64\\) pixels. library(pixmap) # Read an smiling face. img &lt;- read.pnm(paste0(datapath,&quot;faces/&quot;, smile.list[10]), cellres = 1) # Plot the image. plot(img) # Print its properties. print(img) #&gt; Pixmap image #&gt; Type : pixmapRGB #&gt; Size : 64x64 #&gt; Resolution : 1x1 #&gt; Bounding box : 0 0 64 64 Then we are going to load the images into two arrays smiling.images and nonsmiling.images (code omitted here). If we print the array dimensions we see that there are \\(600\\) smiling images of size \\(64 \\times 64 \\times 3\\). # Print dimensions. dim(smiling.images) #&gt; [1] 600 64 64 3 If we print the minimum and maximum values we see that they are \\(0\\) and \\(1\\) so there is no need for normalization. max(smiling.images) #&gt; [1] 1 min(smiling.images) #&gt; [1] 0 The next step is to randomly split the dataset into train and test sets. We will use \\(85\\%\\) for the train set and \\(15\\%\\) for the test set. We will use the validation_split parameter of the fit() function to choose a small percent (\\(10\\%\\)) of the train set to be used as the validation set during training. After creating the train and test sets, the train set images and labels are stored in trainX and trainY respectively and the test set data is stored in testX and testY. The labels in trainY and testY were one-hot encoded. Now that the data is in place, let’s build the CNN. model &lt;- keras_model_sequential() model %&gt;% layer_conv_2d(filters = 8, kernel_size = c(3,3), activation = &#39;relu&#39;, input_shape = c(64,64,3)) %&gt;% layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;% layer_dropout(0.25) %&gt;% layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = &#39;relu&#39;) %&gt;% layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;% layer_dropout(0.25) %&gt;% layer_flatten() %&gt;% layer_dense(units = 32, activation = &#39;relu&#39;) %&gt;% layer_dropout(0.5) %&gt;% layer_dense(units = 2, activation = &#39;softmax&#39;) Our CNN consists of two convolution layers each followed by a max pooling operation and dropout. The feature maps are then flattened and passed to a fully connected layer with \\(32\\) units followed by a dropout. Since this is a binary classification problem (‘smile’ v.s. ‘non-smile’) the output layer has \\(2\\) units with a softmax activation function. Now the model can be compiled and the fit() function used to begin the training! # Compile model. model %&gt;% compile( loss = &#39;categorical_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.01), metrics = c(&quot;accuracy&quot;) ) # Fit model. history &lt;- model %&gt;% fit( trainX, trainY, epochs = 50, batch_size = 8, validation_split = 0.10, verbose = 1, view_metrics = TRUE ) We are using a stochastic gradient descent optimizer with a learning rate of \\(0.01\\) and cross-entropy as the loss function. We can use \\(10\\%\\) of the train set as the validation set by setting validation_split = 0.10. Once the training is done, we can plot the loss and accuracy of each epoch. plot(history) Figure 8.33: Train/test loss and accuracy. After epoch \\(25\\) it looks like the training loss is decreasing faster than the validation loss. After epoch \\(40\\) it seems that the model starts to overfit (the validation loss is increasing a bit). If we look at the accuracy, it seems that it starts to get flat after epoch \\(30\\). We can evaluate the model on the test set: # Evaluate model on test set. model %&gt;% evaluate(testX, testY) #&gt; $loss #&gt; [1] 0.1862139 #&gt; $acc #&gt; [1] 0.9222222 An accuracy of \\(92\\%\\) is pretty decent if we take into account that we didn’t have to do any image preprocessing or feature extraction! We can print the predictions of the first \\(16\\) test images. Figure 8.34: Predictions of the first 16 test set images. Correct predictions are in green and incorrect ones in red. From those \\(16\\), all but one were correctly classified. The correct ones are shown in green and the incorrect one in red. Some faces seem to be smiling (last row, third image) but the mouth is closed, though. It seems that this CNN classifies images as ‘smiling’ only when the mouth is open which may be the way the train labels were defined. 8.9 Summary Deep learning (DL) consists of a set of different architectures and algorithms. As of now, it mainly focuses on artificial neural networks (ANNs). This chapter introduced two main types of DL models (ANNs and CNNs) and their application to behavior analysis. Artificial neural networks (ANNs) are mathematical models inspired by the brain. But that does not mean they work the same as the brain. The perceptron is one of the simplest ANNs. ANNs consist of an input layer, hidden layer(s) and an output layer. Deep networks has many hidden layers. Gradient descent can be used to learn the parameters of a network. Overfitting is a recurring problem in ANNs. Some methods like dropout and early stopping can be used to reduce the effect of overfitting. A convolutional neural network (CNN) is a type of ANN that can process \\(N\\)-dimensional arrays very efficiently. They are used mainly for computer vision tasks. CNNs consist of convolution and pooling layers. References "],
["multiuser.html", "Chapter 9 Multi-User Validation 9.1 Mixed Models 9.2 User-Independent Models 9.3 User-Dependent Models 9.4 User-Adaptive Models 9.5 Summary", " Chapter 9 Multi-User Validation Every person is different. We all have different physical and mental characteristics. Every person reacts in different ways to the same stimulus and conducts physical and motor activities in particular ways. As we have seen, predictive models rely on the training data; and for user-oriented applications, this data encodes their behaviors. When building predictive models, we want them to be general and to perform accurately on new unseen instances. Sometimes this generalization capability comes at a price, especially in multi-user settings. A multi-user setting is one in which the results depend heavily on the target user, that is, the user on which the predictions are made. Take, for example, a hand gesture recognition system. At inference time, a specific person (the target user) performs a gesture and the system should recognize it. The input data comes directly from the user. On the other hand, a non-multi-user system does not depend on a particular person. A classifier that labels fruits on images or a regression model that predicts house prices does not depend directly on a particular person. Some time ago I had to build an activity recognition system based on inertial data from a wrist band. So I collected the data, trained the models, and evaluated them. The performance results were good. However, it turned out that when the system was tested on a new sample group it failed. The reason? The training data was collected from people within a particular age group (young) but the target market of the product was for much older people. Older people tend to walk more slowly, thus, the system was predicting ‘no movement’ when in fact, the person was walking at a very slow pace. This is an extreme example but even within the same age groups, there are going to be differences between users (inter-user variance). Or even the same user will evolve over time and will change her/his behaviors (intra-user variance). So, how can we evaluate multi-user systems to reduce the unexpected effects once the system is deployed? Most of the time there’s going to be surprises when testing a system on new users so we need to be aware of that. But in this chapter, I will present \\(3\\) types of models that will help you to reduce that uncertainty to some extent so you will have a better idea of how the system will behave when tested on more realistic conditions. The models are: mixed models, user-independent models, and user-dependent models. We will see how to train each type of model using a database with actions recorded with a motion capture system. After that, I will also show you how to build adaptive models that are tuned with the objective of increasing their performance for a particular user. 9.1 Mixed Models Mixed models are trained and validated as usual without considering information about mappings between data points and users. Suppose we have a dataset as shown in Figure 9.1. The first column is the user id, the second column the label we want to predict and the last two columns are two arbitrary features. Figure 9.1: Example dataset with a binary label and 2 features. With a mixed model we would just remove the userid column and perform \\(k\\)-fold cross-validation or hold-out validation as usual. In fact, this is what we have been doing so far. By doing so, some random data points will end up in the train set and others in the test set regardless of which data point was generated by which user. The user rows are just mixed, thus the mixed model name. This model assumes that the data was generated by a single user. One of the disadvantages of validating a system using a mixed model is that the performance results could be overestimated. When randomly splitting into train and test sets, some data points for a given user will end up in each of the splits. At inference time, when showing a test sample belonging to a particular user to the model, it is likely that the training set of that model included some data from that particular user. Thus, the model already knows a little bit about that user so we can expect an accurate prediction. However, this assumption not always holds true. If the model is to be used on a new user that the model has never seen before, then, it may not produce very accurate predictions. When should a mixed model be used to validate a system? When you know that you will have some available data to train the model for all the users that are intended to use the system. In many cases, a dataset has already missing information about the mapping between rows and users. That is, a userid column is not present. In those cases, the best performance estimation would be through the use of a mixed model. To demonstrate the differences between the different types of models (mixed, user-independent, and user-dependent) I will use the SKELETON ACTIONS dataset. First, a brief description of the dataset is presented including details about how the features were extracted. Then, the dataset is used to train a mixed model and in the following subsections, it is used to train user-independent and user-dependent models. 9.1.1 Skeleton Action Recognition with Mixed Models preprocess_skeleton_actions.R classify_skeleton_actions.R To demonstrate the \\(3\\) different types of models I chose the UTD-MHAD dataset (Chen, Jafari, and Kehtarnavaz 2015) and from now on, I will refer to it as the SKELETON ACTIONS dataset. This database is suitable because it was collected by \\(8\\) persons (\\(4\\) females/\\(4\\) males) and each file has a subject id, thus, we know which actions were collected by which users. The number of actions is \\(27\\) and some of the actions are: ‘right-hand wave’, ‘two hand front clap’, ‘basketball shoot’, ‘front boxing’, etc. The data was recorded using a Kinect camera and an inertial sensor unit and each subject repeated each of the \\(27\\) actions \\(4\\) times. More information about the collection process and pictures can be consulted on the original dataset website https://personal.utdallas.edu/~kehtar/UTD-MHAD.html. For our examples, I will only focus on the skeleton data generated by the Kinect camera. These data consists of human body joints (\\(20\\) joints). Each file contains one action for one user and one repetition. The file names have the structure aA_sS_tT_skeleton.mat. The A is the action id, the S is the subject id and the T is the trial (repetition) number. For each time frame, the \\(3\\)D positions of the \\(20\\) joints are recorded. The script preprocess_skeleton_actions.R shows how to read the files and plot the actions. The files are stored in Matlab format. The library R.matlab (Bengtsson 2018) can be used to read the files. # Path to one of the files. filepath &lt;- &quot;/skeleton_actions/a7_s1_t1_skeleton.mat&quot; # Read skeleton file. df &lt;- readMat(filepath)$d.skel # Print dimensions. dim(df) #&gt; [1] 20 3 66 From the file name, we can see that this corresponds to action \\(7\\) (basketball shoot), from subject \\(1\\) and trial \\(1\\). The readMat() function is used to read the file contents and store them as a \\(3\\)D array in df. If we print the dimensions we see that the first one corresponds to the number of joints, the second one are the positions (x, y, z), and the last dimension is the number of frames, in this case \\(66\\) frames. We can extract the first time-frame as follows: # Select the first frame. frame &lt;- data.frame(df[, , 1]) # Print dimensions. dim(frame) #&gt; [1] 20 3 Each frame can then be plotted. The plotting code is included in the script. The Figure below shows how the skeleton looks like for \\(6\\) of the time frames. The script also has code to play the action as an animation. Figure 9.2: Skeleton of basketball shoot action. 6 frames sampled from the entire sequence. We will represent each action (file) as a feature vector. The same script also shows the code to extract the feature vectors from each action. To extract the features, a reference point in the skeleton is selected, in this case the spine (joint \\(3\\)). Then, for each time frame, the distance between all joints (excluding the reference point) and the reference point is calculated. Finally, for each distance, the mean, min, and max are computed across all time frames. Since there are \\(19\\) joints (excluding the spine), we end up with \\(19*3=57\\) features. Figure 9.3 shows how the final dataset looks like. Only showing the first \\(4\\) features out of the \\(57\\) and user id and labels. Figure 9.3: First rows of the skeleton dataset after feature extraction showing the first 4 features. The following examples assume that the file dataset.csv with the extracted features already exsits in the skeleton_actions/ directory. To generate this file, run the feature extraction code in the script preprocess_skeleton_actions.R. Once the dataset in a suitable format, we can proceed to train our mixed model. The script containing the full code for training the different types of models is classify_skeleton_actions.R. This script makes use of the dataset.csv file. First, the auxiliary functions are loaded because we will use the normalize() function to normalize the data. We will use a Random Forest for the classification and the caret package to compute the performance metrics. source(file.path(&quot;..&quot;,&quot;auxiliary_functions&quot;,&quot;globals.R&quot;)) source(file.path(&quot;..&quot;,&quot;auxiliary_functions&quot;,&quot;functions.R&quot;)) library(randomForest) library(caret) # Path to the csv file containing the extracted features. # preprocess_skeleton_actins.R contains # the code used to extract the features. filepath &lt;- file.path(datasets_path, &quot;skeleton_actions&quot;, &quot;dataset.csv&quot;) # Load dataset. dataset &lt;- read.csv(filepath, stringsAsFactors = T) # Extract unique labels. unique.actions &lt;- as.character(unique(dataset$label)) # Print the unique labels. print(unique.actions) #&gt; [1] &quot;a1&quot; &quot;a10&quot; &quot;a11&quot; &quot;a12&quot; &quot;a13&quot; &quot;a14&quot; &quot;a15&quot; &quot;a16&quot; &quot;a17&quot; #&gt; [10] &quot;a18&quot; &quot;a19&quot; &quot;a2&quot; &quot;a20&quot; &quot;a21&quot; &quot;a22&quot; &quot;a23&quot; &quot;a24&quot; &quot;a25&quot; #&gt; [19] &quot;a26&quot; &quot;a27&quot; &quot;a3&quot; &quot;a4&quot; &quot;a5&quot; &quot;a6&quot; &quot;a7&quot; &quot;a8&quot; &quot;a9&quot; The unique.actions variable stores the name of all actions. We will need it later to define the levels of the factor object. Next, we generate \\(10\\) folds and define some variables to store the performance metrics including the accuracy, recall, and precision. In each iteration during cross-validation, we will compute and store those performance metrics. k &lt;- 10 # Number of folds. set.seed(1234) folds &lt;- sample(k, nrow(dataset), replace = TRUE) accuracies &lt;- NULL; recalls &lt;- NULL; precisions &lt;- NULL In the next code snippet, the actual cross-validation is performed. This is just the usual cross-validation procedure. The normalize() function defined in the auxiliary functions is used to normalize the data by only learning the parameters from the train set and applying them to the test set. Then, the actual Random Forest is fitted with the train set. One thing to note here is that the userid field is removed: trainset[,-1] since we are not using users’ information in the mixed model. Then, predictions on the test set are obtained and the accuracy, recall, and precision are computed during each iteration. # Perform k-fold cross-validation. for(i in 1:k){ trainset &lt;- dataset[which(folds != i,),] testset &lt;- dataset[which(folds == i,),] #Normalize. res &lt;- normalize(trainset, testset) trainset &lt;- res$train testset &lt;- res$test rf &lt;- randomForest(label ~., trainset[,-1]) preds.rf &lt;- as.character(predict(rf, newdata = testset[,-1])) groundTruth &lt;- as.character(testset$label) cm.rf &lt;- confusionMatrix(factor(preds.rf, levels = unique.actions), factor(groundTruth, levels = unique.actions)) accuracies &lt;- c(accuracies, cm.rf$overall[&quot;Accuracy&quot;]) metrics &lt;- colMeans(cm.rf$byClass[,c(&quot;Recall&quot;, &quot;Specificity&quot;, &quot;Precision&quot;, &quot;F1&quot;)], na.rm = TRUE) recalls &lt;- c(recalls, metrics[&quot;Recall&quot;]) precisions &lt;- c(precisions, metrics[&quot;Precision&quot;]) } Finally, the average performance across folds of each of the metrics is printed. # Print performance metrics. mean(accuracies) #&gt; [1] 0.9277258 mean(recalls) #&gt; [1] 0.9372515 mean(precisions) #&gt; [1] 0.9208455 The results look promising with an average accuracy of \\(92.7\\%\\), a recall of \\(93.7\\%\\), and a precision of \\(92\\%\\). One important thing to remember is that the mixed model assumes that the training data contains instances belonging to users in the test set. Thus, the model already knows a little bit about the users in the test set. Imagine that now you want to estimate what would be the performance of the model in a situation where a completely new user is shown to the model, that is, the model does not know anything about this user. We can model these situations using a user-independent model which is the topic of the next section. 9.2 User-Independent Models The user-independent model allows us to estimate the performance of a system on new users. That is, the model does not contain any information about the target user. This resembles a scenario when the user wants to use a service out-of-the-box without having to go through a calibration process or having to collect training data. To build a user-independent model we just need to make sure that the training data does not contain any information about the users on the test set. We can achieve this by splitting the dataset into two disjoint groups of users based on their ids. For example, assign \\(70\\%\\) of the users to the train set and the remaining to the test set. If the dataset is small, one way to optimize it is by performing leave-one-user-out validation: If the dataset has \\(n\\) users, then, \\(n\\) iterations are performed. In each iteration, one user is selected as the test set and the remaining ones are used as the train set. Figure 9.4 illustrates leave-one-user-out validation for the first \\(2\\) iterations of the procedure. Figure 9.4: First 2 iterations of leave-one-user-out validation. By doing this, we guarantee that the model knows anything about the target user. To implement this leave-one-user-out validation method in our skeleton recognition case, let’s first define some initialization variables. These include the unique.users variable which stores the ids of all users in the database. As before, we will compute the accuracy, recall, and precision so we define variables to store those metrics for each user. # Get a list of unique users. unique.users &lt;- as.character(unique(dataset$userid)) # Print the unique user ids. unique.users #&gt; [1] &quot;s1&quot; &quot;s2&quot; &quot;s3&quot; &quot;s4&quot; &quot;s5&quot; &quot;s6&quot; &quot;s7&quot; &quot;s8&quot; accuracies &lt;- NULL; recalls &lt;- NULL; precisions &lt;- NULL Then, we iterate through each user, build the corresponding train and test sets, and train the classifiers. Here, we make sure that the test set only includes data points belonging to a single user. set.seed(1234) for(user in unique.users){ testset &lt;- dataset[which(dataset$userid == user),] trainset &lt;- dataset[which(dataset$userid != user),] # Normalize. Not really needed since RF # is not affected by different features scale. res &lt;- normalize(trainset, testset) trainset &lt;- res$train testset &lt;- res$test rf &lt;- randomForest(label ~., trainset[,-1]) preds.rf &lt;- as.character(predict(rf, newdata = testset[,-1])) groundTruth &lt;- as.character(testset$label) cm.rf &lt;- confusionMatrix(factor(preds.rf, levels = unique.actions), factor(groundTruth, levels = unique.actions)) accuracies &lt;- c(accuracies, cm.rf$overall[&quot;Accuracy&quot;]) metrics &lt;- colMeans(cm.rf$byClass[,c(&quot;Recall&quot;, &quot;Specificity&quot;, &quot;Precision&quot;, &quot;F1&quot;)], na.rm = TRUE) recalls &lt;- c(recalls, metrics[&quot;Recall&quot;]) precisions &lt;- c(precisions, metrics[&quot;Precision&quot;]) } Now we print the average performance metrics across users. mean(accuracies) #&gt; [1] 0.5807805 mean(recalls) #&gt; [1] 0.5798611 mean(precisions) #&gt; [1] 0.6539715 Those numbers are surprising! In the previous section, our mixed model had an accuracy of \\(92.7\\%\\) and now the user-independent model has an accuracy of only \\(58\\%\\)! This is because the latter didn’t know anything about the target user. Since each person different, the user-independent model was not able to capture the patterns of new users and this had a big impact on the performance. When should a user-independent model be used to validate a system? When you expect the system to be used out-of-the-box by new users and the system does not have any data from those new users. The main advantage of the user-independent model is that it does not require training data from the target users so they can start using it right away at the expense of lower accuracy. The opposite case is when a model is trained specifically for the target user. This model is called the user-dependent model and will be presented in the next section. 9.3 User-Dependent Models A user-dependent model is trained with data belonging only to the target user. In general, this type of model performs better compared to the mixed model and user-independent model. This is because the model captures the particularities of a specific user. The way to evaluate user-dependent models is to iterate through each user, and for each user, build and test a model only with her/his data. The per-user evaluation can be done using \\(k\\)-fold cross-validation, for example. For the skeleton database, we only have \\(4\\) instances per type of action. The number of unique classes (\\(27\\)) is also high compared to the number of instances per action. If we do, for example, \\(10\\)-fold cross-validation, it is very likely that the train sets will not contain examples for several of the possible actions. To avoid this, we will do leave-one-out validation within each user. This means that we iterate through each instance. In each iteration, the instance is used as the test set and the remaining ones are used for the train set. unique.users &lt;- as.character(unique(dataset$userid)) accuracies &lt;- NULL; recalls &lt;- NULL; precisions &lt;- NULL set.seed(1234) # Iterate through each user. for(user in unique.users){ print(paste0(&quot;Evaluating user &quot;, user)) user.data &lt;- dataset[which(dataset$userid == user), -1] # Leave-one-out cross validation within each user. predictions.rf &lt;- NULL; groundTruth &lt;- NULL for(i in 1:nrow(user.data)){ # Normalize. Not really needed since RF # is not affected by different features scale. testset &lt;- user.data[i,] trainset &lt;- user.data[-i,] res &lt;- normalize(trainset, testset) testset &lt;- res$test trainset &lt;- res$train rf &lt;- randomForest(label ~., trainset) preds.rf &lt;- as.character(predict(rf, newdata = testset)) predictions.rf &lt;- c(predictions.rf, preds.rf) groundTruth &lt;- c(groundTruth, as.character(testset$label)) } cm.rf &lt;- confusionMatrix(factor(predictions.rf, levels = unique.actions), factor(groundTruth, levels = unique.actions)) accuracies &lt;- c(accuracies, cm.rf$overall[&quot;Accuracy&quot;]) metrics &lt;- colMeans(cm.rf$byClass[,c(&quot;Recall&quot;, &quot;Specificity&quot;, &quot;Precision&quot;, &quot;F1&quot;)], na.rm = TRUE) recalls &lt;- c(recalls, metrics[&quot;Recall&quot;]) precisions &lt;- c(precisions, metrics[&quot;Precision&quot;]) } # end of users iteration. We iterated through each user and performed the leave-one-out-validation for each, independently of the others and stored their results. We can now compute the average performance across all users. # Print average performance across users. mean(accuracies) #&gt; [1] 0.943114 mean(recalls) #&gt; [1] 0.9425154 mean(precisions) #&gt; [1] 0.9500772 This time, the average accuracy was \\(94.3\\%\\) which is higher than the accuracy achieved with the mixed model and the user-independent model. The average recall and precision were also higher compared to the other types of models. The high performance is because each model was targeted to a particular user. When should a user-dependent model be used to validate a system? When the model will be trained only using data from the target user that will use the system. In general, user-dependent models have the best accuracy. The disadvantage is that they require training data from the target user and for some applications, collecting training data can be very difficult and expensive. Can we have a system that has the best of both worlds from user dependent/independent models? That is, a model that is as accurate as a user-dependent model but requires small quantities of training data from the target user. The answer is yes, and how we can do that is covered in the next section User-Adaptive Models. 9.4 User-Adaptive Models We have already talked about some of the limitations of user-dependent and user-independent models. On one hand, user-dependent models require training data from the target user. In many situations, collecting training data is difficult. On the other hand, user-independent models do not need data from the target user but are less accurate. To overcome those limitations, models that evolve over time as more information is available can be built. One can start with a user-independent model and as more data is available from the target user, the model can be updated accordingly. In this situation, there is no need to wait to start using the system and as new feedback is available, the model gets better and better by learning the specific patterns of a particular user. In this section, I will explain how a technique called transfer learning can be used to build an adaptive model that updates itself as new training data is available. First, in the following subsection the idea of transfer learning is introduced and next, the method is used to build an adaptive model for activity recognition. 9.4.1 Transfer Learning In machine learning, transfer learning refers to the idea of using the knowledge gained when solving a problem to solve another different problem. The new problem can be a similar one but also could be very unrelated. For example, a model trained to detect smiles from images could also be used to predict gender (of course with some fine-tuning). In humans, learning is a lifelong process in which many tasks are interrelated. When we are faced with a new problem, we tend to find solutions that have worked in the past for similar problems. However, in machine learning most of the time the models are trained from scratch for every new problem. For many tasks, training a model from scratch is very time consuming and requires a lot of effort, especially during the data collection and labeling phase. The idea of transfer learning dates back to 1991 (Pratt et al. 1991) but with the advent of deep learning and in particular, with convolutional neural networks (see chapter 8), it has gained popularity because it has proven to be a valuable tool when solving challenging problems. In 2014 a CNN architecture called VGG16 was proposed by Simonyan and Zisserman (2014) and won the ILSVR image recognition competition. This CNN was trained with more than \\(1\\) million images to recognize \\(1000\\) categories and it consists of several convolution layers, max pooling operations, and fully connected layers. In total, the network has \\(\\approx 138\\) million parameters and it took some weeks to train. What if you wanted to add a new category to the \\(1000\\) labels? Or maybe, you only want to focus on a subset of the categories? With transfer learning you can take advantage of a network that has already been trained and adapt it to your particular problem. In the case of deep learning, the approach consists of ‘freezing’ the first layers of a network and only retrain and/or modify the last layers to adapt them for the particular problem. During training, the frozen layers’ parameters will not change and the unfrozen ones are updated as usual during the gradient descent procedure. As discussed in chapter 8, the first layers can act as feature extractors and can be reused. With this approach, you can easily retrain a VGG16 network in an average computer and within a reasonable time. In fact, Keras already provides interfaces to common pre-trained architectures that you can reuse. In the following section we will use this idea to build a user-adaptive model for activity recognition using transfer learning. 9.4.2 A User-Adaptive Model for Activity Recognition keras/adaptive_cnn.R For this example, we will use the SMARTPHONE ACTIVITIES dataset encoded as images . In chapter 7 (section: Images) I showed you how timeseries data can be represented as an image with an example of how an activity can be represented as an RBG color image where each channel corresponds to one of the acceleration axes (x, y, z). We will use the file images.txt that already contains the activities in image format. The procedure of converting the raw data into this format was explained in chapter 7 and the corresponding code is in script timeseries_to_images.R. Since the input data are images, we will use a convolutional neural network (see chapter 8). The main objective will be to build an adaptive model with a small amount of training data from the target user. What we will do first is build a user-independent model. That is, we will select one of the users as the target user. We will train a user-independent model with data from the remaining users (excluding the target user). Then, we will apply transfer learning on this model to adapt it to the target user. The target user’s data will be split into a test set and an adaptive set. The test set will be used to evaluate the performance of the model and the adaptive set will be used to fine-tune the model. The adaptive set is used to simulate that we have obtained new data from the target user. The complete code is in script keras/adaptive_cnn.R. First, we start by reading the images file. Each row corresponds to one activity. The last two columns are the userid and the class. The first \\(300\\) columns correspond to the image pixels. Each image has a size of \\(10 \\times 10 \\times 3\\) (height, width, depth). # Path to smartphone activities in image format. filepath &lt;- file.path(datasets_path, &quot;smartphone_activities&quot;, &quot;images.txt&quot;) # Read data. df &lt;- read.csv(filepath, stringsAsFactors = F) # Shuffle rows. set.seed(1234) df &lt;- df[sample(nrow(df)),] The rows happen to be ordered by user and activity, so we shuffle them to ensure that the model is not biased towards the last users and activities. Since we will train a CNN using keras, we need the classes to be in integer format. The following code is used to append a new column intlabel to the database. This new column contains the classes as integers. We also create a variable mapping to keep track of the mapping between integers and the actual labels. By printing the mapping variable we can see that for example, the ‘Walking’ label has a corresponding integer value of \\(0\\), ‘Downstairs’ \\(1\\), and so on. ## Convert labels to integers starting at 0. ## # Get the unique labels. labels &lt;- unique(df$label) mapping &lt;- 0:(length(labels) - 1) names(mapping) &lt;- labels print(mapping) #&gt; Walking Downstairs Jogging Standing Upstairs Sitting #&gt; 0 1 2 3 4 5 # Append labels as integers at the end of data frame. df$intlabel &lt;- mapping[df$label] Now we store the unique users’ ids in the users variable. After printing it, you can notice that there are \\(19\\) distinct users in this database. The original database has more users but we only kept those that performed all the activities. Then, we can select one of the users to act as the target user. I will just select one of them at random (turned out to be user \\(24\\)). Feel free to select another user if you want. # Get the unique user ids. users &lt;- unique(df$userid) # Print all user ids. print(users) #&gt; [1] 29 20 18 8 32 27 3 36 34 5 7 12 6 21 24 31 13 33 19 # Choose one user at random to be the target user. targetUser &lt;- sample(users, 1) Next, we split the data into two sets. The first set trainset contains the data from all users but excluding the target user. Then we create two variables: train.y and train.x. The first one has the labels as integers and the second one has the actual image pixels (features). # Split into train and target user sets. # The train set includes data from all users excluding targetUser. trainset &lt;- df[df$userid != targetUser,] # Save train labels in a separate variable. train.y &lt;- trainset$intlabel # Save train pixels in a separate variable. train.x &lt;- as.matrix(trainset[,-c(301,302,303)]) Now we split the target’s user data into \\(50\\%\\) test data and \\(50\\%\\) adaptive data such that we end up with \\(4\\) variables: target.adaptive.y Integer labels for the adaptive data of the target user. target.adaptive.x Pixels of the adaptive data of the target user. target.test.y Integer labels for the test data of the target user. target.test.x Pixels of the test data of the target user. # This contains all data from the target user. target.data &lt;- df[df$userid == targetUser,] # Split the target user&#39;s data into 50% adaptive # and 50% test data. idxs &lt;- sample(nrow(target.data), size = nrow(target.data) * 0.5, replace = FALSE) # Target user adaptive set. # This data is used to adapt the model to the target user. target.adaptive &lt;- target.data[idxs,] # Save target.adaptive labels in a separate variable. target.adaptive.y &lt;- target.adaptive$intlabel # Save target.adaptive pixels in a separate variable. target.adaptive.x &lt;- as.matrix(target.adaptive[,-c(301,302,303)]) # Target user test set. # This data is used to test the performance. target.test &lt;- target.data[-idxs,] # Save target.test labels in a separate variable. target.test.y &lt;- target.test$intlabel # Save target.test pixels in a separate variable. target.test.x &lt;- as.matrix(target.test[,-c(301,302,303)]) We also need to normalize the data and reshape it into the actual image format since in their current form, the pixels are stored into \\(1\\)-dimensional arrays. We learn the normalization parameters only from the train set and then we used the normalize.reshape() function (defined in the same script file) to perform the actual normalization and formatting. # Learn min and max values from train set for normalization. maxv &lt;- max(train.x) minv &lt;- min(train.x) # Normalize and reshape. May take some minutes. train.x &lt;- normalize.reshape(train.x, minv, maxv) target.adaptive.x &lt;- normalize.reshape(target.adaptive.x, minv, maxv) target.test.x &lt;- normalize.reshape(target.test.x, minv, maxv) Let’s inspect how the structure of the final datasets looks like. dim(train.x) #&gt; [1] 6399 10 10 3 dim(target.adaptive.x) #&gt; [1] 124 10 10 3 dim(target.test.x) #&gt; [1] 124 10 10 3 Here, we see that the train set has \\(6399\\) instances (images). The adaptive and test sets both have \\(124\\) instances. Now that we are finally done with all the preprocessing, it is time to build the CNN model! This one will be the initial user-independent model and is trained with all the training data train.x, train.y. model &lt;- keras_model_sequential() model %&gt;% layer_conv_2d(name = &quot;conv1&quot;, filters = 8, kernel_size = c(2,2), activation = &#39;relu&#39;, input_shape = c(10,10,3)) %&gt;% layer_conv_2d(name = &quot;conv2&quot;, filters = 16, kernel_size = c(2,2), activation = &#39;relu&#39;) %&gt;% layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;% layer_flatten() %&gt;% layer_dense(name = &quot;hidden1&quot;, units = 32, activation = &#39;relu&#39;) %&gt;% layer_dropout(0.25) %&gt;% layer_dense(units = 6, activation = &#39;softmax&#39;) This CNN has two convolutional layers followed by a max pooling operation, a fully connected layer, and an output layer. One important thing to note is that we have specified a name for each layer with the name parameter. For example, the first convolution’s name is conv1, the second one is conv2, and the fully connected layer was named hidden1. These names must be unique because they will allow us to later select specific layers fo freeze and unfreeze. If we print the model’s summary we can see that in total it has \\(9,054\\) trainable parameters and \\(0\\) non-trainable parameters. This means that all the parameters of the network will be updated during the gradient descent procedure, as usual. # Print summary. summary(model) Figure 9.5: Summary of initial user-independent model. The next code will compile the model and initiate the training phase. # Compile model. model %&gt;% compile( loss = &#39;sparse_categorical_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.01), metrics = c(&quot;accuracy&quot;) ) # Fit the user-independent model. history &lt;- model %&gt;% fit( train.x, train.y, epochs = 50, batch_size = 8, validation_split = 0.15, verbose = 1, view_metrics = TRUE ) plot(history) Note that this time the loss was defined as loss = 'sparse_categorical_crossentropy' instead of the usual loss = 'categorical_crossentropy'. Here, the sparse_ suffix was added. You may have noted that in this example we did not one-hot encode the labels but they were only transformed into integers. By adding the sparse_ suffix we are telling Keras that our labels are not one-hot-encoded but encoded as integers starting at \\(0\\) so it will perform the one-hot encoding for us. This is a little trick that saved us some time. We save the model so we can load it later. Let’s also estimate the model’s performance on the target user test set. # Save model. save_model_hdf5(model, &quot;user-independent.hdf5&quot;) # Compute performance (accuracy) on the target user test set. model %&gt;% evaluate(target.test.x, target.test.y) #&gt; loss accuracy #&gt; 2.1888916 0.6129032 The overall accuracy of this user-independent model when tested on the target user was \\(61.2\\%\\) (quite low). Now, we can apply transfer learning and see if we can do better. We will ‘freeze’ the first convolution layer and only update the second convolution layer and the remaining fully connected layers using the target user-adaptive data. The following code loads the previously trained user-independent model. Then all the CNN’s weights are frozen using the freeze_weights() function. The from parameter specifies the first layer (inclusive) from which the parameters are to be frozen. Here, we set it to \\(1\\) so all parameters in the network are ‘frozen’. Then, we can use the unfreeze_weights() function to specify from which layer (inclusive) the parameters should be unfrozen. In this case, we want to retrain from the second convolutional layer so we set it to conv2 which is how we named this layer earlier. adaptive.model &lt;- load_model_hdf5(&quot;user-independent.hdf5&quot;) # Freeze all layers. freeze_weights(adaptive.model, from = 1) # Unfreeze layers from conv2. unfreeze_weights(adaptive.model, from = &quot;conv2&quot;) After defining those changes we need to compile the model so the modifications take effect. # Compile model. We need to compile after freezing/unfreezing weights. adaptive.model %&gt;% compile( loss = &#39;sparse_categorical_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.01), metrics = c(&quot;accuracy&quot;) ) summary(adaptive.model) Figure 9.6: Summary of user-independent model after freezing first convolutional layer. After printing the summary, now you can note that the number of trainable and non-trainable parameters has changed. Now, the non-trainable parameters are \\(104\\) but before they were \\(0\\). These \\(104\\) parameters correspond to the first convolutional layer but this time they will not be updated during the gradient descent training phase. The following code will retrain the model using the adaptive data but keeping the first convolutional layer fixed. # Update model with adaptive data. history &lt;- adaptive.model %&gt;% fit( target.adaptive.x, target.adaptive.y, epochs = 50, batch_size = 8, validation_split = 0, verbose = 1, view_metrics = TRUE ) Note that this time the validation_split was set to \\(0\\). This is because the target user data set is very small so there is not enough data to use as validation set. One possible approach to overcome this is to leave a percentage of users out when building the train set for the user-independent model. Then, use those left-out users to find which are the most appropriate layers to keep frozen. Once you are happy with the results, evaluate the model with the target user. # Compute performance (accuracy) on the target user test set. adaptive.model %&gt;% evaluate(target.test.x, target.test.y) #&gt; loss accuracy #&gt; 0.4652017 0.8548387 If we evaluate the adaptive model performance on the target’s user test set, now we see that the accuracy is \\(85.4\\%\\) which is a considerable increase! (\\(\\approx 24\\%\\) increase). At this point, you may be wondering that this accuracy increase was due to the fact that the model was trained for an additional \\(50\\) epochs. To check this, we can re-train the initial user-independent model for \\(50\\) more epochs. retrained_model &lt;- load_model_hdf5(&quot;user-independent.hdf5&quot;) # Fit the user-independent model for 50 more epochs. history &lt;- retrained_model %&gt;% fit( train.x, train.y, epochs = 50, batch_size = 8, validation_split = 0.15, verbose = 1, view_metrics = TRUE ) # Compute performance (accuracy) on the target user test set. retrained_model %&gt;% evaluate(target.test.x, target.test.y) #&gt; loss accuracy #&gt; 2.7197671 0.6612903 After re-training the user-independent model for \\(50\\) more epochs, its accuracy increased to \\(66.1\\%\\). On the other hand, the adaptive model was trained with much less data and produced a much better result (\\(85.4%\\)) with only \\(124\\) instances as compared to the user-independent model \\(\\approx 5440\\) instances. That is, the \\(6399\\) instances minus \\(15\\%\\) used as the validation set. This also highlights one of the advantages of transfer learning which is a reduction in the needed amount of training data. 9.5 Summary Many real-life scenarios involve multi-user settings. That is, the system heavily depends on the specific behavior of a given final user. This chapter covered different types of models that can be used to evaluate the performance of a system in such a scenario. A multi-user setting is one in which its results depend heavily on the target user. Inter/Intra -user variance are the differences between users and within the same users, respectively. Mixed models are trained without considering unique users (user ids) information. User-independent models are trained without including data from the target user. User-dependent models are trained only with data from the target user. User-adaptive models can be adapted to a particular target user as more data is available. Transfer learning is a method that can be used to adapt a model to a particular user without requiring big quantities of data. References "],
["appendixInstall.html", "A Setup Your Environment A.1 Installing the Datasets A.2 Installing the Examples Source Code A.3 Installing Keras and TensorFlow.", " A Setup Your Environment The examples in this book were tested with R 4.0.2. You can get the latest R version from its official website: www.r-project.org/ As IDE, I use RStudio (https://rstudio.com/) but you can use your favorite one. Most of the code examples in this book rely on datasets. The following two sections describe how to get and install the datasets and source code. If you want to try out the examples, I recommend you to follow the instructions on the following two sections. The last section includes instructions on how to install Keras and TensorFlow which are the required libraries to build and train deep learning models. Deep learning is covered in chapter 8. Before that, you don’t need those libraries. A.1 Installing the Datasets A compressed file with a collection of most of the datasets used in this book can be downloaded here: https://github.com/enriquegit/behavior-datasets. Download the datasets collection file (behavior_book_datasets.zip) and extract it into a local directory, for example, C:/datasets/. This compilation includes most of the datasets. Due to some datasets having large file sizes or license restrictions, not all of them are included in the collection set. But you can download them separately. Even though a dataset may not be included in the compiled set, it will still have a corresponding directory with a README file with instructions on how to get it. The following picture shows how the directory structure looks like on my PC. A.2 Installing the Examples Source Code The examples source code can be downloaded here: https://github.com/enriquegit/behavior-code You can get the code using git or if you are not familiar with it, click on the ‘Code’ button and then ‘Download zip’. Then, extract the file into a local directory of your choice. There is a directory for each chapter and two additional directories: auxiliary_functions/ and install_functions/. The auxiliary_functions/ folder has generic functions that are imported by some other R scripts. In this directory, there is a file called globals.R. Open that file and set the variable datasets_path to your local path where you downloaded the datasets. For example, I set it to: datasets_path &lt;- &quot;C:/datasets&quot; The install_functions/ directory has a single script: install_packages.R. This script can be used to install all the packages used in the examples (except Keras and TensorFlow which is covered in the next section). The script reads the packages listed in listpackages.txt and tries to install them if they are not present. This is just a convenient way to install everything at once but you can always install each package individually with the usual install.packages() method. When running the examples, it is assumed that the working directory is the same as the actual script. For example, if you want to try indoor_classification.R, and that script is located in C:/code/Predicting Behavior with Classification Models/ then, your working directory should be C:/code/Predicting Behavior with Classification Models/. In Windows, and if RStudio is not already opened, when you double-click an R script, RStudio will be launched (if it is set as the default program) and the working directory will be set. You can check your current working directory by typing getwd() and you can set your working directory with setwd(). Alternatively, in RStudio, you can set your working directory in the menu bar ‘Session’ -&gt; ‘Set Working Directory’ -&gt; ‘To Source File Location’. A.3 Installing Keras and TensorFlow. Keras and TensorFlow are used until Chapter 8. It is not necessary to install them if you are not still there. TensorFlow has two main versions. a CPU and a GPU version. The GPU version takes advantage of the capabilities of some video cards to perform faster operations. The examples in this book can be run on the CPU version. The following instructions apply to the CPU version. Installing the GPU version requires some platform-specific details. I recommend you to first install the CPU version and if you want/need to perform faster computations, then, go with the GPU version. Installing Keras with TensorFlow (CPU version) as backend takes four simple steps: If you are on Windows, first you should install Anaconda27. Install the keras R package with install.packages(\"keras\") Load keras with library(keras) Run install_keras(). This function will install TensorFlow as the backend. If you don’t have Anaconda installed, you will be asked if you want to install Miniconda. You can test your installation with: library(tensorflow) tf$constant(&quot;Hello World&quot;) #&gt; tf.Tensor(b&#39;Hello World&#39;, shape=(), dtype=string) The first time in a session that you run TensorFlow related code with the CPU version, you may get warning messages like the following, which you can safely ignore. #&gt; tensorflow/stream_executor/platform/default/dso_loader.cc:55] #&gt; Could not load dynamic library &#39;cudart64_101.dll&#39;; #&gt; dlerror: cudart64_101.dll not found If you want to install the GPU version, first, you need to make sure you have a compatible video card. More information on how to install the GPU version is available here https://keras.rstudio.com/reference/install_keras.html and here https://tensorflow.rstudio.com/installation/gpu/local_gpu/ https://www.anaconda.com↩︎ "],
["appendixDatasets.html", "B Datasets B.1 COMPLEX ACTIVITIES B.2 DEPRESJON B.3 ELECTROMYOGRAPHY B.4 HAND GESTURES B.5 HOME TASKS B.6 HOMICIDE REPORTS B.7 INDOOR LOCATION B.8 SHEEP GOATS B.9 SKELETON ACTIONS B.10 SMARTPHONE ACTIVITIES B.11 SMILES B.12 STUDENTS’ MENTAL HEALTH", " B Datasets This Appendix has a list with a description of all the datasets used in this book. A compressed file with a compilation of most of the datasets can be downloaded here: https://github.com/enriquegit/behavior-datasets. I recommend you to download the datasets compilation file and extract its contents to a local directory. Due to some datasets with large file sizes or license restrictions, not all of them are included in the compiled set. But you can download them separately. Even though a dataset may not be included in the compiled set, it will have a corresponding directory with a README file with instructions on how to get it. Each dataset in the following list, states whether or not it is included in the compiled set. The datasets are ordered alphabetically. B.1 COMPLEX ACTIVITIES Included: Yes. This dataset was collected with a smartphone and contains \\(5\\) complex activities: commuting, working, at home, shopping at the supermarket and exercising. An Android 2.2 application running on a LG Optimus Me cellphone was used to collect the accelerometer data from each of the axes (x,y,z). The sample rate was set at \\(50\\) Hz. The cellphone was placed in the user’s belt. A training and a test set were collected on different days. The duration of the activities varies from about \\(5\\) minutes to a couple of hours. The total recorded data consists of approximately \\(41\\) hours. The data was collected by one user. Each file contains a whole activity. B.2 DEPRESJON Included: Yes. This dataset contains motor activity recordings of \\(23\\) unipolar and bipolar depressed patients and \\(32\\) healthy controls. Motor activity was monitored with an actigraph watch worn at the right wrist (Actiwatch, Cambridge Neurotechnology Ltd, England, model AW4). The sampling frequency was \\(32\\) Hz. The device uses the inertial sensors data to compute an activity count every minute which is stored as an integer value in the memory unit of the actigraph watch. The number of counts is proportional to the intensity of the movement. The dataset also contains some additional information about the patients and the control group. For more details please see Garcia-Ceja, Riegler, Jakobsen, et al. (2018). B.3 ELECTROMYOGRAPHY Included: Yes. This dataset was made available by Kirill Yashuk. The data was collected using an armband device that has \\(8\\) sensors placed on the skin surface that measure electrical activity from the right forearm at a sampling rate of \\(200\\) Hz. A video of the device can be seen here: https://youtu.be/1u5-G6DPtkk. The data contains \\(4\\) different gestures: 0-rock, 1-scissors, 2-paper, 3-OK, and has \\(65\\) columns. The last column is the class label from \\(0\\) to \\(3\\). Each gesture was recorded \\(6\\) times for \\(20\\) seconds. The first \\(64\\) columns are electrical measurements. \\(8\\) consecutive readings for each of the \\(8\\) sensors. For more details, please see Yashuk (2019). B.4 HAND GESTURES Included: Yes. The data was collected using an LG Optimus Me smartphone using its accelerometer sensor. The data was collected by \\(10\\) subjects which performed \\(5\\) repetitions of \\(10\\) different gestures (triangle, square, circle, a, b, c, 1, 2, 3, 4) giving a total of \\(500\\) instances. The sensor is a tri-axial accelerometer which returns values for the x, y, and z axes. The data was collected by \\(10\\) volunteers who performed \\(5\\) repetitions per gesture. The sampling rate was set at \\(50\\) Hz. To record a gesture the user presses the phone screen with his/her thumb, performs the gesture, and stops pressing the screen. For more information, please see Garcia-Ceja, Brena, and Galván-Tejada (2014). B.5 HOME TASKS Included: Yes. The sound and accelerometer data were collected by \\(3\\) volunteers while performing \\(7\\) different home task activities: mop floor, sweep floor, type on computer keyboard, brush teeth, wash hands, eat chips and watch t.v. Each volunteer performed each activity for approximately \\(3\\) min. If the activity lasted less than \\(3\\) min, another session was recorded until completing the \\(3\\) min. The data were collected with a wrist-band (Microsoft Band 2) and a cellphone. The wrist-band was used to collect accelerometer data and was worn by the volunteers in their dominant hand. The accelerometer sensor returns values from the x, y, and z axes, and the sampling rate was set to \\(31\\) Hz. The cellphone was used to record environmental sound with a sampling rate of \\(8000\\) Hz and it was placed on a table in the same room the user was performing the activity. To preserve privacy, the dataset does not contain the raw recordings but extracted features. \\(16\\) features from the accelerometer sensor and \\(12\\) Mel frequency cepstral coefficients from the audio recordings. For more information, please see Garcia-Ceja, Galván-Tejada, and Brena (2018). B.6 HOMICIDE REPORTS Included: Yes. This dataset was compiled and made available by the Murder Accountability Project, founded by Thomas Hargrove28. It contains information about homicides in the United States. This dataset includes the age, race, sex, ethnicity of victims and perpetrators, in addition to the relationship between the victim and perpetrator and weapon used. The original dataset includes the database.csv file. The files processed.csv and transactions.RData were generated with the R scripts included in the examples code to facilitate the analysis. B.7 INDOOR LOCATION Included: Yes. This dataset contains WiFi signal recordings from different locations in a building including the MAC address and signal strength. The data was collected with an Android 2.2 application running on a LG Optimus Me cell phone. To generate a single instance, the device scans and records the MAC address and signal strength of the nearby access points. A delay of \\(500\\) ms is set between scans. For each location, approximately \\(3\\) minutes of data were collected while the user walked around the specific location. The data has four different locations: bedroomA, beadroomB, tv room and the lobby. To preserve privacy, the MAC addresses are encoded as integer numbers. For more information, please, see Garcia and Brena (2012). B.8 SHEEP GOATS Included: No. The dataset was made available by Kamminga et al. (2017) and can be downloaded from https://easy.dans.knaw.nl/ui/datasets/id/easy-dataset:76131. The researchers placed inertial sensors on sheep and goats and tracked their behavior during one day. They also video-recorded the session and annotated the data with different types of behaviors such as grazing, fighting, scratch-biting, etc. The device was placed on the neck with random orientation and it collects acceleration, orientation, magnetic field, temperature, and barometric pressure. In this book, only data from one of the sheep is used (Sheep/S1.csv). B.9 SKELETON ACTIONS Included: No. The authors of this dataset are Chen, Jafari, and Kehtarnavaz (2015). The data was recorded by \\(8\\) subjects with a Kinect camera and an inertial sensor unit and each subject repeated each action \\(4\\) times. The number of actions is \\(27\\) and some of the actions are: right hand wave, two hand front clap, basketball shoot, front boxing, etc. More information about the collection process and pictures can be consulted on the website https://personal.utdallas.edu/~kehtar/UTD-MHAD.html. B.10 SMARTPHONE ACTIVITIES Included: Yes. This dataset is called WISDM29 and was made available by Kwapisz, Weiss, and Moore (2010). The dataset has \\(6\\) different activities: walking, jogging, walking upstairs, walking downstairs, sitting and standing. The data was collected by \\(36\\) volunteers with the accelerometer of an Android phone located in the users’ pants pocket and with a sampling rate of \\(20\\) Hz. B.11 SMILES Included: No. This dataset contains color face images of \\(64 \\times 64\\) pixels and is published here: http://conradsanderson.id.au/lfwcrop/. This is a cropped version (Sanderson and Lovell 2009) of the Labeled Faces in the Wild (LFW) database (Huang et al. 2008). Please, download the color version (lfwcrop_color.zip) and copy all ppm files into the faces/ directory. A subset of the database was labeled by Arigbabu et al. (2016), Arigbabu (2017). The labels are provided as two text files (SMILE_list.txt, NON-SMILE_list.txt), each, containing the list of files that correspond to smiling and non-smiling faces. The smiling set has \\(600\\) pictures and the non-smiling has \\(603\\) pictures. B.12 STUDENTS’ MENTAL HEALTH Included: Yes. This dataset contains \\(268\\) survey responses that include variables related to depression, acculturative stress, social connectedness, and help-seeking behaviors reported by international and domestic students at an international university in Japan. For a detailed description, please see (Nguyen et al. 2019). References "],
["citing-this-book.html", "Citing this Book", " Citing this Book If you found this book useful, you can consider citing it like this: Garcia-Ceja, Enrique. &quot;Behavior Analysis with Machine Learning and R: A Sensors and Data Driven Approach&quot;, 2020. http://behavior.enriquegc.com BibTeX: @book{GarciaCejaBook, title = {Behavior Analysis with Machine Learning and {R}: A Sensors and Data Driven Approach}, author = {Enrique Garcia-Ceja}, year = {2020}, note = {\\url{http://behavior.enriquegc.com}} } "],
["references.html", "References", " References "]
]
