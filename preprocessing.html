<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Preprocessing Behavioral Data | Behavior Analysis with Machine Learning and R</title>
  <meta name="description" content="Chapter 5 Preprocessing Behavioral Data | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Preprocessing Behavioral Data | Behavior Analysis with Machine Learning and R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="Chapter 5 Preprocessing Behavioral Data | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Preprocessing Behavioral Data | Behavior Analysis with Machine Learning and R" />
  
  <meta name="twitter:description" content="Chapter 5 Preprocessing Behavioral Data | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  <meta name="twitter:image" content="images/cover.png" />

<meta name="author" content="Enrique Garcia Ceja" />


<meta name="date" content="2020-10-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="edavis.html"/>
<link rel="next" href="unsupervised.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/d3-4.9.0/d3.min.js"></script>
<script src="libs/d3-tip-0.7.1/index-min.js"></script>
<link href="libs/d3panels-1.4.9/d3panels.min.css" rel="stylesheet" />
<script src="libs/d3panels-1.4.9/d3panels.min.js"></script>
<script src="libs/qtlcharts_iplotCorr-0.11.6/iplotCorr.js"></script>
<script src="libs/qtlcharts_iplotCorr-0.11.6/iplotCorr_noscat.js"></script>
<script src="libs/iplotCorr-binding-0.11.6/iplotCorr.js"></script>
<link href="libs/dygraphs-1.1.1/dygraph.css" rel="stylesheet" />
<script src="libs/dygraphs-1.1.1/dygraph-combined.js"></script>
<script src="libs/dygraphs-1.1.1/shapes.js"></script>
<script src="libs/moment-2.8.4/moment.js"></script>
<script src="libs/moment-timezone-0.2.5/moment-timezone-with-data.js"></script>
<script src="libs/moment-fquarter-1.0.0/moment-fquarter.min.js"></script>
<script src="libs/dygraphs-binding-1.1.1.6/dygraphs.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178679335-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178679335-1', { 'anonymize_ip': true });
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="#">Behavior Analysis with Machine Learning and R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#supplemental-material"><i class="fa fa-check"></i>Supplemental Material</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#conventions"><i class="fa fa-check"></i>Conventions</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#taxonomy"><i class="fa fa-check"></i><b>1.2</b> Types of Machine Learning</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#terminology"><i class="fa fa-check"></i><b>1.3</b> Terminology</a><ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#tables"><i class="fa fa-check"></i><b>1.3.1</b> Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#variable-types"><i class="fa fa-check"></i><b>1.3.2</b> Variable Types</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro.html"><a href="intro.html#predictive-models"><i class="fa fa-check"></i><b>1.3.3</b> Predictive Models</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#pipeline"><i class="fa fa-check"></i><b>1.4</b> Data Analysis Pipeline</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#trainingeval"><i class="fa fa-check"></i><b>1.5</b> Evaluating Predictive Models</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#simple-classification-example"><i class="fa fa-check"></i><b>1.6</b> Simple Classification Example</a><ul>
<li class="chapter" data-level="1.6.1" data-path="intro.html"><a href="intro.html#k-fold-cross-validation-example"><i class="fa fa-check"></i><b>1.6.1</b> K-fold Cross-Validation Example</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#simple-regression-example"><i class="fa fa-check"></i><b>1.7</b> Simple Regression Example</a></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#underfitting-and-overfitting"><i class="fa fa-check"></i><b>1.8</b> Underfitting and Overfitting</a></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a href="intro.html#bias-and-variance"><i class="fa fa-check"></i><b>1.9</b> Bias and Variance</a></li>
<li class="chapter" data-level="1.10" data-path="intro.html"><a href="intro.html#SummaryIntro"><i class="fa fa-check"></i><b>1.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>2</b> Predicting Behavior with Classification Models</a><ul>
<li class="chapter" data-level="2.1" data-path="classification.html"><a href="classification.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>2.1</b> <em>k</em>-nearest Neighbors</a><ul>
<li class="chapter" data-level="2.1.1" data-path="classification.html"><a href="classification.html#indoor-location-with-wi-fi-signals"><i class="fa fa-check"></i><b>2.1.1</b> Indoor Location with Wi-Fi Signals</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="classification.html"><a href="classification.html#performance-metrics"><i class="fa fa-check"></i><b>2.2</b> Performance Metrics</a></li>
<li class="chapter" data-level="2.3" data-path="classification.html"><a href="classification.html#decision-trees"><i class="fa fa-check"></i><b>2.3</b> Decision Trees</a><ul>
<li class="chapter" data-level="2.3.1" data-path="classification.html"><a href="classification.html#activityRecognition"><i class="fa fa-check"></i><b>2.3.1</b> Activity Recognition with Smartphones</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="classification.html"><a href="classification.html#naive-bayes"><i class="fa fa-check"></i><b>2.4</b> Naive Bayes</a><ul>
<li class="chapter" data-level="2.4.1" data-path="classification.html"><a href="classification.html#activity-recognition-with-naive-bayes"><i class="fa fa-check"></i><b>2.4.1</b> Activity Recognition with Naive Bayes</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="classification.html"><a href="classification.html#dynamic-time-warping"><i class="fa fa-check"></i><b>2.5</b> Dynamic Time Warping</a><ul>
<li class="chapter" data-level="2.5.1" data-path="classification.html"><a href="classification.html#sechandgestures"><i class="fa fa-check"></i><b>2.5.1</b> Hand Gesture Recognition</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="classification.html"><a href="classification.html#summaryClassification"><i class="fa fa-check"></i><b>2.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>3</b> Predicting Behavior with Ensemble Learning</a><ul>
<li class="chapter" data-level="3.1" data-path="ensemble.html"><a href="ensemble.html#bagging"><i class="fa fa-check"></i><b>3.1</b> Bagging</a><ul>
<li class="chapter" data-level="3.1.1" data-path="ensemble.html"><a href="ensemble.html#activity-recognition-with-bagging"><i class="fa fa-check"></i><b>3.1.1</b> Activity recognition with Bagging</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ensemble.html"><a href="ensemble.html#random-forest"><i class="fa fa-check"></i><b>3.2</b> Random Forest</a></li>
<li class="chapter" data-level="3.3" data-path="ensemble.html"><a href="ensemble.html#stacked-generalization"><i class="fa fa-check"></i><b>3.3</b> Stacked Generalization</a></li>
<li class="chapter" data-level="3.4" data-path="ensemble.html"><a href="ensemble.html#multiviewhometasks"><i class="fa fa-check"></i><b>3.4</b> Multi-view Stacking for Home Tasks Recognition</a></li>
<li class="chapter" data-level="3.5" data-path="ensemble.html"><a href="ensemble.html#SummaryEnsemble"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="edavis.html"><a href="edavis.html"><i class="fa fa-check"></i><b>4</b> Exploring and Visualizing Behavioral Data</a><ul>
<li class="chapter" data-level="4.1" data-path="edavis.html"><a href="edavis.html#talking-with-field-experts"><i class="fa fa-check"></i><b>4.1</b> Talking with Field Experts</a></li>
<li class="chapter" data-level="4.2" data-path="edavis.html"><a href="edavis.html#summary-statistics"><i class="fa fa-check"></i><b>4.2</b> Summary Statistics</a></li>
<li class="chapter" data-level="4.3" data-path="edavis.html"><a href="edavis.html#class-distributions"><i class="fa fa-check"></i><b>4.3</b> Class Distributions</a></li>
<li class="chapter" data-level="4.4" data-path="edavis.html"><a href="edavis.html#user-class-sparsity-matrix"><i class="fa fa-check"></i><b>4.4</b> User-Class Sparsity Matrix</a></li>
<li class="chapter" data-level="4.5" data-path="edavis.html"><a href="edavis.html#boxplots"><i class="fa fa-check"></i><b>4.5</b> Boxplots</a></li>
<li class="chapter" data-level="4.6" data-path="edavis.html"><a href="edavis.html#correlation-plots"><i class="fa fa-check"></i><b>4.6</b> Correlation Plots</a><ul>
<li class="chapter" data-level="4.6.1" data-path="edavis.html"><a href="edavis.html#interactive-correlation-plots"><i class="fa fa-check"></i><b>4.6.1</b> Interactive Correlation Plots</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="edavis.html"><a href="edavis.html#timeseries"><i class="fa fa-check"></i><b>4.7</b> Timeseries</a><ul>
<li class="chapter" data-level="4.7.1" data-path="edavis.html"><a href="edavis.html#interactive-timeseries"><i class="fa fa-check"></i><b>4.7.1</b> Interactive Timeseries</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="edavis.html"><a href="edavis.html#multidimensional-scaling-mds"><i class="fa fa-check"></i><b>4.8</b> Multidimensional Scaling (MDS)</a></li>
<li class="chapter" data-level="4.9" data-path="edavis.html"><a href="edavis.html#heatmaps"><i class="fa fa-check"></i><b>4.9</b> Heatmaps</a></li>
<li class="chapter" data-level="4.10" data-path="edavis.html"><a href="edavis.html#automated-eda"><i class="fa fa-check"></i><b>4.10</b> Automated EDA</a></li>
<li class="chapter" data-level="4.11" data-path="edavis.html"><a href="edavis.html#SummaryExploratory"><i class="fa fa-check"></i><b>4.11</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="preprocessing.html"><a href="preprocessing.html"><i class="fa fa-check"></i><b>5</b> Preprocessing Behavioral Data</a><ul>
<li class="chapter" data-level="5.1" data-path="preprocessing.html"><a href="preprocessing.html#missing-values"><i class="fa fa-check"></i><b>5.1</b> Missing Values</a><ul>
<li class="chapter" data-level="5.1.1" data-path="preprocessing.html"><a href="preprocessing.html#imputation"><i class="fa fa-check"></i><b>5.1.1</b> Imputation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="preprocessing.html"><a href="preprocessing.html#smoothing"><i class="fa fa-check"></i><b>5.2</b> Smoothing</a></li>
<li class="chapter" data-level="5.3" data-path="preprocessing.html"><a href="preprocessing.html#normalization"><i class="fa fa-check"></i><b>5.3</b> Normalization</a></li>
<li class="chapter" data-level="5.4" data-path="preprocessing.html"><a href="preprocessing.html#imbalanced-classes"><i class="fa fa-check"></i><b>5.4</b> Imbalanced Classes</a><ul>
<li class="chapter" data-level="5.4.1" data-path="preprocessing.html"><a href="preprocessing.html#random-oversampling"><i class="fa fa-check"></i><b>5.4.1</b> Random Oversampling</a></li>
<li class="chapter" data-level="5.4.2" data-path="preprocessing.html"><a href="preprocessing.html#smote"><i class="fa fa-check"></i><b>5.4.2</b> SMOTE</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="preprocessing.html"><a href="preprocessing.html#infoinjection"><i class="fa fa-check"></i><b>5.5</b> Information Injection</a></li>
<li class="chapter" data-level="5.6" data-path="preprocessing.html"><a href="preprocessing.html#one-hot-encoding"><i class="fa fa-check"></i><b>5.6</b> One-hot Encoding</a></li>
<li class="chapter" data-level="5.7" data-path="preprocessing.html"><a href="preprocessing.html#SummaryPreprocessing"><i class="fa fa-check"></i><b>5.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="unsupervised.html"><a href="unsupervised.html"><i class="fa fa-check"></i><b>6</b> Discovering Behaviors with Unsupervised Learning</a><ul>
<li class="chapter" data-level="6.1" data-path="unsupervised.html"><a href="unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>6.1</b> K-means clustering</a><ul>
<li class="chapter" data-level="6.1.1" data-path="unsupervised.html"><a href="unsupervised.html#studentresponses"><i class="fa fa-check"></i><b>6.1.1</b> Grouping Student Responses</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="unsupervised.html"><a href="unsupervised.html#the-silhouette-index"><i class="fa fa-check"></i><b>6.2</b> The Silhouette Index</a></li>
<li class="chapter" data-level="6.3" data-path="unsupervised.html"><a href="unsupervised.html#associationrules"><i class="fa fa-check"></i><b>6.3</b> Mining Association Rules</a><ul>
<li class="chapter" data-level="6.3.1" data-path="unsupervised.html"><a href="unsupervised.html#finding-rules-for-criminal-behavior"><i class="fa fa-check"></i><b>6.3.1</b> Finding Rules for Criminal Behavior</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="unsupervised.html"><a href="unsupervised.html#SummaryUnsupervised"><i class="fa fa-check"></i><b>6.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="representations.html"><a href="representations.html"><i class="fa fa-check"></i><b>7</b> Encoding Behavioral Data</a><ul>
<li class="chapter" data-level="7.1" data-path="representations.html"><a href="representations.html#feature-vectors"><i class="fa fa-check"></i><b>7.1</b> Feature Vectors</a></li>
<li class="chapter" data-level="7.2" data-path="representations.html"><a href="representations.html#sectimeseries"><i class="fa fa-check"></i><b>7.2</b> Timeseries</a></li>
<li class="chapter" data-level="7.3" data-path="representations.html"><a href="representations.html#transactions"><i class="fa fa-check"></i><b>7.3</b> Transactions</a></li>
<li class="chapter" data-level="7.4" data-path="representations.html"><a href="representations.html#images"><i class="fa fa-check"></i><b>7.4</b> Images</a></li>
<li class="chapter" data-level="7.5" data-path="representations.html"><a href="representations.html#recurrence-plots"><i class="fa fa-check"></i><b>7.5</b> Recurrence Plots</a><ul>
<li class="chapter" data-level="7.5.1" data-path="representations.html"><a href="representations.html#computing-recurence-plots"><i class="fa fa-check"></i><b>7.5.1</b> Computing Recurence Plots</a></li>
<li class="chapter" data-level="7.5.2" data-path="representations.html"><a href="representations.html#recurrence-plots-of-hand-gestures"><i class="fa fa-check"></i><b>7.5.2</b> Recurrence Plots of Hand Gestures</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="representations.html"><a href="representations.html#bag-of-words"><i class="fa fa-check"></i><b>7.6</b> Bag-of-Words</a><ul>
<li class="chapter" data-level="7.6.1" data-path="representations.html"><a href="representations.html#bow-for-complex-activities."><i class="fa fa-check"></i><b>7.6.1</b> BoW for Complex Activities.</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="representations.html"><a href="representations.html#graphs"><i class="fa fa-check"></i><b>7.7</b> Graphs</a><ul>
<li class="chapter" data-level="7.7.1" data-path="representations.html"><a href="representations.html#complex-activities-as-graphs"><i class="fa fa-check"></i><b>7.7.1</b> Complex Activities as Graphs</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="representations.html"><a href="representations.html#SummaryRepresentations"><i class="fa fa-check"></i><b>7.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deeplearning.html"><a href="deeplearning.html"><i class="fa fa-check"></i><b>8</b> Predicting Behavior with Deep Learning</a><ul>
<li class="chapter" data-level="8.1" data-path="deeplearning.html"><a href="deeplearning.html#ann"><i class="fa fa-check"></i><b>8.1</b> Introduction to Artificial Neural Networks</a><ul>
<li class="chapter" data-level="8.1.1" data-path="deeplearning.html"><a href="deeplearning.html#sigmoid-and-relu-units"><i class="fa fa-check"></i><b>8.1.1</b> Sigmoid and ReLU Units</a></li>
<li class="chapter" data-level="8.1.2" data-path="deeplearning.html"><a href="deeplearning.html#assembling-units-into-layers"><i class="fa fa-check"></i><b>8.1.2</b> Assembling Units into Layers</a></li>
<li class="chapter" data-level="8.1.3" data-path="deeplearning.html"><a href="deeplearning.html#deep-neural-networks"><i class="fa fa-check"></i><b>8.1.3</b> Deep Neural Networks</a></li>
<li class="chapter" data-level="8.1.4" data-path="deeplearning.html"><a href="deeplearning.html#learning-the-parameters"><i class="fa fa-check"></i><b>8.1.4</b> Learning the Parameters</a></li>
<li class="chapter" data-level="8.1.5" data-path="deeplearning.html"><a href="deeplearning.html#parameter-learning-example-in-r"><i class="fa fa-check"></i><b>8.1.5</b> Parameter Learning Example in R</a></li>
<li class="chapter" data-level="8.1.6" data-path="deeplearning.html"><a href="deeplearning.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>8.1.6</b> Stochastic Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="deeplearning.html"><a href="deeplearning.html#keras-and-tensorflow-with-r"><i class="fa fa-check"></i><b>8.2</b> Keras and TensorFlow with R</a><ul>
<li class="chapter" data-level="8.2.1" data-path="deeplearning.html"><a href="deeplearning.html#keras-example"><i class="fa fa-check"></i><b>8.2.1</b> Keras Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="deeplearning.html"><a href="deeplearning.html#classification-with-neural-networks"><i class="fa fa-check"></i><b>8.3</b> Classification with Neural Networks</a><ul>
<li class="chapter" data-level="8.3.1" data-path="deeplearning.html"><a href="deeplearning.html#classification-of-electromyography-signals"><i class="fa fa-check"></i><b>8.3.1</b> Classification of Electromyography Signals</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="deeplearning.html"><a href="deeplearning.html#overfitting"><i class="fa fa-check"></i><b>8.4</b> Overfitting</a><ul>
<li class="chapter" data-level="8.4.1" data-path="deeplearning.html"><a href="deeplearning.html#early-stopping"><i class="fa fa-check"></i><b>8.4.1</b> Early Stopping</a></li>
<li class="chapter" data-level="8.4.2" data-path="deeplearning.html"><a href="deeplearning.html#dropout"><i class="fa fa-check"></i><b>8.4.2</b> Dropout</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="deeplearning.html"><a href="deeplearning.html#fine-tuning-a-neural-network"><i class="fa fa-check"></i><b>8.5</b> Fine-Tuning a Neural Network</a></li>
<li class="chapter" data-level="8.6" data-path="deeplearning.html"><a href="deeplearning.html#cnns"><i class="fa fa-check"></i><b>8.6</b> Convolutional Neural Networks</a><ul>
<li class="chapter" data-level="8.6.1" data-path="deeplearning.html"><a href="deeplearning.html#convolutions"><i class="fa fa-check"></i><b>8.6.1</b> Convolutions</a></li>
<li class="chapter" data-level="8.6.2" data-path="deeplearning.html"><a href="deeplearning.html#pooling-operations"><i class="fa fa-check"></i><b>8.6.2</b> Pooling Operations</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="deeplearning.html"><a href="deeplearning.html#cnns-with-keras"><i class="fa fa-check"></i><b>8.7</b> CNNs with Keras</a><ul>
<li class="chapter" data-level="8.7.1" data-path="deeplearning.html"><a href="deeplearning.html#example-1"><i class="fa fa-check"></i><b>8.7.1</b> Example 1</a></li>
<li class="chapter" data-level="8.7.2" data-path="deeplearning.html"><a href="deeplearning.html#example-2"><i class="fa fa-check"></i><b>8.7.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="deeplearning.html"><a href="deeplearning.html#cnnSmile"><i class="fa fa-check"></i><b>8.8</b> Smiles Detection with a CNN</a></li>
<li class="chapter" data-level="8.9" data-path="deeplearning.html"><a href="deeplearning.html#SummaryDeepLearning"><i class="fa fa-check"></i><b>8.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multiuser.html"><a href="multiuser.html"><i class="fa fa-check"></i><b>9</b> Multi-User Validation</a><ul>
<li class="chapter" data-level="9.1" data-path="multiuser.html"><a href="multiuser.html#mixed-models"><i class="fa fa-check"></i><b>9.1</b> Mixed Models</a><ul>
<li class="chapter" data-level="9.1.1" data-path="multiuser.html"><a href="multiuser.html#skeleton-action-recognition-with-mixed-models"><i class="fa fa-check"></i><b>9.1.1</b> Skeleton Action Recognition with Mixed Models</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="multiuser.html"><a href="multiuser.html#user-independent-models"><i class="fa fa-check"></i><b>9.2</b> User-Independent Models</a></li>
<li class="chapter" data-level="9.3" data-path="multiuser.html"><a href="multiuser.html#user-dependent-models"><i class="fa fa-check"></i><b>9.3</b> User-Dependent Models</a></li>
<li class="chapter" data-level="9.4" data-path="multiuser.html"><a href="multiuser.html#user-adaptive-models"><i class="fa fa-check"></i><b>9.4</b> User-Adaptive Models</a><ul>
<li class="chapter" data-level="9.4.1" data-path="multiuser.html"><a href="multiuser.html#transfer-learning"><i class="fa fa-check"></i><b>9.4.1</b> Transfer Learning</a></li>
<li class="chapter" data-level="9.4.2" data-path="multiuser.html"><a href="multiuser.html#a-user-adaptive-model-for-activity-recognition"><i class="fa fa-check"></i><b>9.4.2</b> A User-Adaptive Model for Activity Recognition</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="multiuser.html"><a href="multiuser.html#SummaryMultiUser"><i class="fa fa-check"></i><b>9.5</b> Summary</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendixInstall.html"><a href="appendixInstall.html"><i class="fa fa-check"></i><b>A</b> Setup Your Environment</a><ul>
<li class="chapter" data-level="A.1" data-path="appendixInstall.html"><a href="appendixInstall.html#installing-the-datasets"><i class="fa fa-check"></i><b>A.1</b> Installing the Datasets</a></li>
<li class="chapter" data-level="A.2" data-path="appendixInstall.html"><a href="appendixInstall.html#installing-the-examples-source-code"><i class="fa fa-check"></i><b>A.2</b> Installing the Examples Source Code</a></li>
<li class="chapter" data-level="A.3" data-path="appendixInstall.html"><a href="appendixInstall.html#installing-keras-and-tensorflow."><i class="fa fa-check"></i><b>A.3</b> Installing Keras and TensorFlow.</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendixDatasets.html"><a href="appendixDatasets.html"><i class="fa fa-check"></i><b>B</b> Datasets</a><ul>
<li class="chapter" data-level="B.1" data-path="appendixDatasets.html"><a href="appendixDatasets.html#complex-activities"><i class="fa fa-check"></i><b>B.1</b> COMPLEX ACTIVITIES</a></li>
<li class="chapter" data-level="B.2" data-path="appendixDatasets.html"><a href="appendixDatasets.html#depresjon"><i class="fa fa-check"></i><b>B.2</b> DEPRESJON</a></li>
<li class="chapter" data-level="B.3" data-path="appendixDatasets.html"><a href="appendixDatasets.html#electromyography"><i class="fa fa-check"></i><b>B.3</b> ELECTROMYOGRAPHY</a></li>
<li class="chapter" data-level="B.4" data-path="appendixDatasets.html"><a href="appendixDatasets.html#hand-gestures"><i class="fa fa-check"></i><b>B.4</b> HAND GESTURES</a></li>
<li class="chapter" data-level="B.5" data-path="appendixDatasets.html"><a href="appendixDatasets.html#home-tasks"><i class="fa fa-check"></i><b>B.5</b> HOME TASKS</a></li>
<li class="chapter" data-level="B.6" data-path="appendixDatasets.html"><a href="appendixDatasets.html#homicide-reports"><i class="fa fa-check"></i><b>B.6</b> HOMICIDE REPORTS</a></li>
<li class="chapter" data-level="B.7" data-path="appendixDatasets.html"><a href="appendixDatasets.html#indoor-location"><i class="fa fa-check"></i><b>B.7</b> INDOOR LOCATION</a></li>
<li class="chapter" data-level="B.8" data-path="appendixDatasets.html"><a href="appendixDatasets.html#sheep-goats"><i class="fa fa-check"></i><b>B.8</b> SHEEP GOATS</a></li>
<li class="chapter" data-level="B.9" data-path="appendixDatasets.html"><a href="appendixDatasets.html#skeleton-actions"><i class="fa fa-check"></i><b>B.9</b> SKELETON ACTIONS</a></li>
<li class="chapter" data-level="B.10" data-path="appendixDatasets.html"><a href="appendixDatasets.html#smartphone-activities"><i class="fa fa-check"></i><b>B.10</b> SMARTPHONE ACTIVITIES</a></li>
<li class="chapter" data-level="B.11" data-path="appendixDatasets.html"><a href="appendixDatasets.html#smiles"><i class="fa fa-check"></i><b>B.11</b> SMILES</a></li>
<li class="chapter" data-level="B.12" data-path="appendixDatasets.html"><a href="appendixDatasets.html#students-mental-health"><i class="fa fa-check"></i><b>B.12</b> STUDENTS’ MENTAL HEALTH</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="citing-this-book.html"><a href="citing-this-book.html"><i class="fa fa-check"></i>Citing this Book</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Behavior Analysis with Machine Learning and R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="preprocessing" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Preprocessing Behavioral Data</h1>

<div class="rmdfolder">
<code>preprocessing.R</code>
</div>

<p>Sensor-based behavioral data comes in many flavors and forms, but when training predictive models, the data needs to be in a particular format. Some of the sources of variation when collecting data are:</p>
<ul>
<li><p><strong>Sensors’ format.</strong> Each type of sensor and manufacturer stores data in a different format. For example, .csv files, binary files, images, proprietary formats, etc.</p></li>
<li><p><strong>Sampling rate.</strong> The sampling rate indicates how many measurements are taken per unit of time. For example, a heart rate sensor may return a single value every second, thus, the sampling rate is <span class="math inline">\(1\)</span> Hz. An accelerometer that captures <span class="math inline">\(50\)</span> values per second has a sampling rate of <span class="math inline">\(50\)</span> Hz.</p></li>
<li><p><strong>Scales and ranges.</strong> Some sensors may return values in <em>degrees</em> (e.g., a temperature sensor) while others may return values in some other scale, for example, in <em>centimeters</em> for a proximity sensor. Furthermore, ranges can also vary. That is, a sensor may capture values in the range of <span class="math inline">\(0\)</span>-<span class="math inline">\(1000\)</span>, for example.</p></li>
</ul>
<p>During the data exploration step (chapter <a href="edavis.html#edavis">4</a>) we may also find that there are missing values, inconsistent values, noise, and so on, thus, we also need to take care of that.</p>
<p>This chapter provides an overview of some common methods used to clean and preprocess the data before one can start training reliable models.</p>

<div class="rmdcaution">
Several of the methods presented here can lead to <em>information injection</em> which can cause overfitting. That is, transferring information from the train set to the test set. This is something that we do not want because both sets need to be independent to accurately estimate the generalization performance. You can find more details about information injection in section <a href="preprocessing.html#infoinjection">5.5</a> of this chapter.
</div>

<div id="missing-values" class="section level2">
<h2><span class="header-section-number">5.1</span> Missing Values</h2>
<p>Many datasets will have missing values and we need ways to identify and deal with them. The reasons for having missing data could be due to faulty sensors, processing errors, unavailable information, and so on. In this section, I present some tools that ease the process of missing values identification. Later, some imputation methods that can be used to fill the missing values are listed.</p>
<p>To demonstrate some of these concepts, the <em>SHEEP GOATS</em> dataset <span class="citation">(Kamminga et al. <a href="#ref-kamminga2017" role="doc-biblioref">2017</a>)</span> will be used. Due to its big size, the files of this dataset are not included with the accompanying book files but they can be downloaded from <a href="https://easy.dans.knaw.nl/ui/datasets/id/easy-dataset:76131" class="uri">https://easy.dans.knaw.nl/ui/datasets/id/easy-dataset:76131</a>. The data was released as part of a study about animal behaviors. The researchers placed inertial sensors on sheep and goats and tracked their behavior during one day. They also video-recorded the session and annotated the data with different types of behaviors such as <em>grazing</em>, <em>fighting</em>, <em>scratch-biting</em>, etc. The device was placed on the neck with random orientation and it collects acceleration, orientation, magnetic field, temperature, and barometric pressure. Figure <a href="preprocessing.html#fig:sheepsensor">5.1</a> shows a schematic view of the setting.</p>
<div class="figure" style="text-align: center"><span id="fig:sheepsensor"></span>
<img src="images/sheep.png" alt="The device was placed on the neck at a random position." width="50%" />
<p class="caption">
Figure 5.1: The device was placed on the neck at a random position.
</p>
</div>
<p>We will start by loading a .csv file that corresponds to one of the sheep and check if there are missing values. The <code>naniar</code> package <span class="citation">(Tierney et al. <a href="#ref-naniar" role="doc-biblioref">2019</a>)</span> offers a set of different functions to explore and deal with missing values. The <code>gg_miss_var()</code> function allows you to quickly check which variables have missing values and how many. The following code loads the data and then plots the number of missing values in each variable.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="preprocessing.html#cb93-1"></a><span class="kw">library</span>(naniar)</span>
<span id="cb93-2"><a href="preprocessing.html#cb93-2"></a></span>
<span id="cb93-3"><a href="preprocessing.html#cb93-3"></a><span class="co"># Path to S1.csv file.</span></span>
<span id="cb93-4"><a href="preprocessing.html#cb93-4"></a>datapath &lt;-<span class="st"> </span><span class="kw">file.path</span>(datasets_path,</span>
<span id="cb93-5"><a href="preprocessing.html#cb93-5"></a>                      <span class="st">&quot;sheep_goats&quot;</span>,<span class="st">&quot;S1.csv&quot;</span>)</span>
<span id="cb93-6"><a href="preprocessing.html#cb93-6"></a></span>
<span id="cb93-7"><a href="preprocessing.html#cb93-7"></a><span class="co"># Can take some seconds to load since the file is big.</span></span>
<span id="cb93-8"><a href="preprocessing.html#cb93-8"></a>df &lt;-<span class="st"> </span><span class="kw">read.csv</span>(datapath, <span class="dt">stringsAsFactors =</span> <span class="ot">TRUE</span>)</span>
<span id="cb93-9"><a href="preprocessing.html#cb93-9"></a></span>
<span id="cb93-10"><a href="preprocessing.html#cb93-10"></a><span class="co"># Plot missing values.</span></span>
<span id="cb93-11"><a href="preprocessing.html#cb93-11"></a><span class="kw">gg_miss_var</span>(df)</span></code></pre></div>
<p>Figure <a href="preprocessing.html#fig:ggmissvar">5.2</a> shows the resulting output. This plot shows that there are missing values in four variables: <em>pressure</em>, <em>cz</em>, <em>cy</em> and <em>cx</em>. The last three correspond to the compass (magnetometer). For <em>pressure</em>, the number of missing values is more than <span class="math inline">\(2\)</span> million! For the rest, it is a bit less (more than <span class="math inline">\(1\)</span> million).</p>
<div class="figure" style="text-align: center"><span id="fig:ggmissvar"></span>
<img src="images/ggmissvar.png" alt="Missing values counts." width="100%" />
<p class="caption">
Figure 5.2: Missing values counts.
</p>
</div>
<p>To further explore this issue, we can plot each observation in a row with the function <code>vis_miss()</code>.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="preprocessing.html#cb94-1"></a><span class="co"># Select first 1000 rows.</span></span>
<span id="cb94-2"><a href="preprocessing.html#cb94-2"></a><span class="co"># It can take some time to plot bigger data frames.</span></span>
<span id="cb94-3"><a href="preprocessing.html#cb94-3"></a><span class="kw">vis_miss</span>(df[<span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>,])</span></code></pre></div>
<p>Figure <a href="preprocessing.html#fig:vismiss">5.3</a> shows every observation per row and missing values are black colored (if any). From this image, it can be seen that missing values seem to be systematic. It looks like there is a clear stripes pattern, especially for the compass variables. Based on these observations, we could guess that this doesn’t look like random sensor failures or random noise.</p>
<div class="figure" style="text-align: center"><span id="fig:vismiss"></span>
<img src="images/vismiss.png" alt="Rows with missing values." width="100%" />
<p class="caption">
Figure 5.3: Rows with missing values.
</p>
</div>
<p>If we explore the data frame’s values, for example with the RStudio viewer (Figure <a href="preprocessing.html#fig:missvaluesdf">5.4</a>), two things can be noted. First, for the compass values, there is a missing value for each present value. Thus, it looks like <span class="math inline">\(50\%\)</span> of compass values are missing. For <em>pressure</em>, it seems that there are <span class="math inline">\(7\)</span> missing values for each available value.</p>
<div class="figure" style="text-align: center"><span id="fig:missvaluesdf"></span>
<img src="images/missingvalues.png" alt="Displaying the data frame in RStudio." width="100%" />
<p class="caption">
Figure 5.4: Displaying the data frame in RStudio.
</p>
</div>
<p>So, what could be the root cause of those missing values? Remember that at the beginning of this chapter it was mentioned that <strong>one of the sources of variation is sampling rate</strong>. If we look at the data set documentation all sensors have a sampling rate of <span class="math inline">\(200\)</span> Hz except for the compass and the pressure sensor. The compass has a sampling rate of <span class="math inline">\(100\)</span> Hz. That is half compared to the other sensors! This explains why <span class="math inline">\(50\%\)</span> of the rows are missing. Similarly, the pressure sensor has a sampling rate of <span class="math inline">\(25\)</span> Hz. By visualizing and then inspecting the missing data, we have just found out that the missing values are not caused by random noise or sensor failures but because the sensors are not as fast as the others!</p>
<p>Now that we know there are missing values we need to decide what to do with them. The following subsection lists some ways to deal with missing values.</p>
<div id="imputation" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Imputation</h3>
<p>Imputation is the process of filling in missing values. One of the reasons for imputing missing values is that some predictive models cannot deal with missing data. Another reason is that it may help in increasing the predictions’ performance, for example, if we are trying to predict the sheep behavior from a discrete set of categories based on the inertial data. There are different ways to handle missing values:</p>
<ul>
<li><p><strong>Discard rows.</strong> If the rows with missing values are not too many, they can simply be discarded.</p></li>
<li><p><strong>Mean value.</strong> Fill the missing values with the mean value of the corresponding variable. This method is simple and can be effective. One of the problems with this method is that it is sensitive to outliers (as it is the arithmetic mean).</p></li>
<li><p><strong>Median value.</strong> The median is robust against outliers, thus, it can be used instead of the arithmetic mean to fill the gaps.</p></li>
<li><p><strong>Replace with the closest value.</strong> For timeseries data, as is the case of the sheep readings, one could also replace missing values with the closest known value.</p></li>
<li><p><strong>Predict the missing values.</strong> Use the other variables to predict the missing one. This can be done by training a predictive model. A regressor if the variable is numeric or a classifier if the variable is categorical.</p></li>
</ul>
<p>Another problem with the mean and median values is that they can be correlated with other variables, for example, with the class that we want to predict. One way to avoid this, is to compute the mean (or median) for each class but still, some hidden correlations may bias the estimates.</p>
<p>In R, the <code>simputation</code> package <span class="citation">(van der Loo <a href="#ref-simputation" role="doc-biblioref">2019</a>)</span> has implemented various imputation techniques including: group-wise median imputation, model-based with linear regression, random forests, etc. The following code snippet (complete code is in <code>preprocessing.R</code>) uses the <code>impute_lm()</code> method to impute the missing values in the sheep data using linear regression.</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="preprocessing.html#cb95-1"></a><span class="kw">library</span>(simputation)</span>
<span id="cb95-2"><a href="preprocessing.html#cb95-2"></a></span>
<span id="cb95-3"><a href="preprocessing.html#cb95-3"></a><span class="co"># Replace NaN with NAs.</span></span>
<span id="cb95-4"><a href="preprocessing.html#cb95-4"></a><span class="co"># Since missing values are represented as NaN,</span></span>
<span id="cb95-5"><a href="preprocessing.html#cb95-5"></a><span class="co"># first we need to replace them with NAs.</span></span>
<span id="cb95-6"><a href="preprocessing.html#cb95-6"></a></span>
<span id="cb95-7"><a href="preprocessing.html#cb95-7"></a><span class="co"># Code to replace NaN with NA was taken from Hong Ooi:</span></span>
<span id="cb95-8"><a href="preprocessing.html#cb95-8"></a><span class="co"># https://stackoverflow.com/questions/18142117/#</span></span>
<span id="cb95-9"><a href="preprocessing.html#cb95-9"></a><span class="co"># how-to-replace-nan-value-with-zero-in-a-huge-data-frame/18143097</span></span>
<span id="cb95-10"><a href="preprocessing.html#cb95-10"></a>is.nan.data.frame &lt;-<span class="st"> </span><span class="cf">function</span>(x)<span class="kw">do.call</span>(cbind, <span class="kw">lapply</span>(x, is.nan))</span>
<span id="cb95-11"><a href="preprocessing.html#cb95-11"></a>df[<span class="kw">is.nan</span>(df)] &lt;-<span class="st"> </span><span class="ot">NA</span></span>
<span id="cb95-12"><a href="preprocessing.html#cb95-12"></a></span>
<span id="cb95-13"><a href="preprocessing.html#cb95-13"></a><span class="co"># Use simputation package to impute values.</span></span>
<span id="cb95-14"><a href="preprocessing.html#cb95-14"></a><span class="co"># The first 4 columns are removed since we </span></span>
<span id="cb95-15"><a href="preprocessing.html#cb95-15"></a><span class="co"># do not want to use them as predictor variables.</span></span>
<span id="cb95-16"><a href="preprocessing.html#cb95-16"></a>imp_df &lt;-<span class="st"> </span><span class="kw">impute_lm</span>(df[,<span class="op">-</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>)],</span>
<span id="cb95-17"><a href="preprocessing.html#cb95-17"></a>                    cx <span class="op">+</span><span class="st"> </span>cy <span class="op">+</span><span class="st"> </span>cz <span class="op">+</span><span class="st"> </span>pressure <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>cx <span class="op">-</span><span class="st"> </span>cy <span class="op">-</span><span class="st"> </span>cz <span class="op">-</span><span class="st"> </span>pressure)</span>
<span id="cb95-18"><a href="preprocessing.html#cb95-18"></a><span class="co"># Print summary.</span></span>
<span id="cb95-19"><a href="preprocessing.html#cb95-19"></a><span class="kw">summary</span>(imp_df)</span></code></pre></div>
<p>Originally, the missing values are encoded as <code>NaN</code> but in order to use the <code>simputation</code> package functions, we need them as <code>NA</code>. First, <code>NaNs</code> are replaced with <code>NA</code>. The first argument of <code>impute_lm()</code> is a data frame and the second argument is a formula. We discard the first <span class="math inline">\(4\)</span> variables of the data frame since we do not want to use them as predictors. The left-hand side of the formula (everything before the ~ symbol) specifies the variables we want to impute. The right-hand side specifies the variables used to build the linear models. The ‘.’ indicates that we want to use all variables while the ‘-’ is used to specify variables that we do not want to include. The vignettes<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> of the package contain more detailed examples.</p>

<div class="rmdcaution">
The mean, median, etc. and the predictive models to infer missing values should be trained using data only from the train set to avoid information injection.
</div>

</div>
</div>
<div id="smoothing" class="section level2">
<h2><span class="header-section-number">5.2</span> Smoothing</h2>
<p>Smoothing comprises a set of algorithms with the aim of highlighting patterns in the data or as a preprocessing step to clean the data and remove noise. These methods are widely used on timeseries data but also with spatio-temporal data such as images. With timeseries data, they are often used to emphasize long-term patterns and reduce short-term signal artifacts. For example, in Figure <a href="preprocessing.html#fig:smoothingStock">5.5</a><a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> a stock chart was smoothed using two methods: moving average and exponential moving average. The smoothed versions make it easier to spot the overall trend rather than focusing on short-term variations.</p>
<div class="figure" style="text-align: center"><span id="fig:smoothingStock"></span>
<img src="images/smoothing_stock.png" alt="Stock chart with two smoothed versions. One with moving average and the other one with an exponential moving average. Source: wikipedia." width="90%" />
<p class="caption">
Figure 5.5: Stock chart with two smoothed versions. One with moving average and the other one with an exponential moving average. Source: wikipedia.
</p>
</div>
<p>The most common smoothing method for timeseries is the <strong>simple moving average</strong>. With this method, the first element of the resulting smoothed series is computed by taking the average of the elements within a window of predefined size. The window’s position starts at the first element of the original series. The second element is computed in the same way but after moving the window one position to the right. Figure <a href="preprocessing.html#fig:movavgsteps">5.6</a> shows this procedure on a series with <span class="math inline">\(5\)</span> elements and a window size of size <span class="math inline">\(3\)</span>. After the third iteration, it is not possible to move the window one more step to the right while covering <span class="math inline">\(3\)</span> elements since the end of the time series has been reached. Because of this, the smoothed series will have some missing values at the end. Specifically, it will have <span class="math inline">\(w-1\)</span> fewer elements where <span class="math inline">\(w\)</span> is the window size. A simple solution is to compute the average of the elements covered by the window even if they are less than the window size.</p>
<div class="figure" style="text-align: center"><span id="fig:movavgsteps"></span>
<img src="images/movingavgsteps.png" alt="Simple moving average step by step with window size = 3." width="90%" />
<p class="caption">
Figure 5.6: Simple moving average step by step with window size = 3.
</p>
</div>
<p>In the previous example the average is taken from the elements to the right of the pointer. There is a variation called <em>centered moving average</em> in which the center point of the window has the same elements to the left and right (Figure <a href="preprocessing.html#fig:centeredmovavg">5.7</a>). Note that with this version of moving average some values at the beginning and at the end will be empty. Also note that the window size should be odd. In practice, both versions produce very similar results.</p>
<div class="figure" style="text-align: center"><span id="fig:centeredmovavg"></span>
<img src="images/centeredmovavg.png" alt="Centered moving average step by step with window size = 3." width="90%" />
<p class="caption">
Figure 5.7: Centered moving average step by step with window size = 3.
</p>
</div>
<p>In the <code>preprocessing.R</code> script the function <code>movingAvg()</code> implements the simple moving average procedure. In the following code note that the output vector will have the same size as the original one but the last elements will contain <code>NA</code> values when the window can not be moved any longer to the right.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="preprocessing.html#cb96-1"></a>movingAvg &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">w =</span> <span class="dv">5</span>){</span>
<span id="cb96-2"><a href="preprocessing.html#cb96-2"></a>  <span class="co"># Applies moving average to x with a window of size w.</span></span>
<span id="cb96-3"><a href="preprocessing.html#cb96-3"></a>  n &lt;-<span class="st"> </span><span class="kw">length</span>(x) <span class="co"># Total number of points.</span></span>
<span id="cb96-4"><a href="preprocessing.html#cb96-4"></a>  smoothedX &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, n)</span>
<span id="cb96-5"><a href="preprocessing.html#cb96-5"></a>  </span>
<span id="cb96-6"><a href="preprocessing.html#cb96-6"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(n<span class="op">-</span>w<span class="op">+</span><span class="dv">1</span>)){</span>
<span id="cb96-7"><a href="preprocessing.html#cb96-7"></a>    smoothedX[i] &lt;-<span class="st"> </span><span class="kw">mean</span>(x[i<span class="op">:</span>(i<span class="dv">-1</span><span class="op">+</span>w)])</span>
<span id="cb96-8"><a href="preprocessing.html#cb96-8"></a>  }</span>
<span id="cb96-9"><a href="preprocessing.html#cb96-9"></a>  </span>
<span id="cb96-10"><a href="preprocessing.html#cb96-10"></a>  <span class="kw">return</span>(smoothedX)</span>
<span id="cb96-11"><a href="preprocessing.html#cb96-11"></a>}</span></code></pre></div>
<p>We can apply this function to a segment of accelerometer data from the <em>SHEEP AND GOATS</em> data set.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="preprocessing.html#cb97-1"></a>datapath &lt;-<span class="st"> &quot;../Sheep/S1.csv&quot;</span></span>
<span id="cb97-2"><a href="preprocessing.html#cb97-2"></a>df &lt;-<span class="st"> </span><span class="kw">read.csv</span>(datapath)</span>
<span id="cb97-3"><a href="preprocessing.html#cb97-3"></a></span>
<span id="cb97-4"><a href="preprocessing.html#cb97-4"></a><span class="co"># Only select a subset of the whole series.</span></span>
<span id="cb97-5"><a href="preprocessing.html#cb97-5"></a>dfsegment &lt;-<span class="st"> </span>df[df<span class="op">$</span>timestamp_ms <span class="op">&lt;</span><span class="st"> </span><span class="dv">6000</span>,]</span>
<span id="cb97-6"><a href="preprocessing.html#cb97-6"></a>x &lt;-<span class="st"> </span>dfsegment<span class="op">$</span>ax</span>
<span id="cb97-7"><a href="preprocessing.html#cb97-7"></a></span>
<span id="cb97-8"><a href="preprocessing.html#cb97-8"></a><span class="co"># Compute simple moving average with a window of size 21.</span></span>
<span id="cb97-9"><a href="preprocessing.html#cb97-9"></a>smoothed &lt;-<span class="st"> </span><span class="kw">movingAvg</span>(x, <span class="dt">w =</span> <span class="dv">21</span>)</span></code></pre></div>
<p>Figure <a href="preprocessing.html#fig:smoothingExample">5.8</a> shows the result after plotting the original vector and the smoothed one. Here, we can see that many of the small peaks are not present anymore in the smoothed version. The window size is a parameter that needs to be defined by the user. If it is set too large some important information may be lost from the signal.</p>
<div class="figure" style="text-align: center"><span id="fig:smoothingExample"></span>
<img src="images/smoothingExample.png" alt="Original time series and smoothed version using a moving average window of size 21" width="90%" />
<p class="caption">
Figure 5.8: Original time series and smoothed version using a moving average window of size 21
</p>
</div>
<p>One of the disadvantages of this method is that the arithmetic mean is sensitive to noise. Instead of computing the mean, one can use the median which is more robust against outlier values. There also exist other derived methods (not covered here) such as weighted moving average and exponential moving average<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> which assign more importance to data points closer to the central point in the window. Smoothing a signal before feature extraction is a common practice and is used to remove some of the unwanted noise.</p>
</div>
<div id="normalization" class="section level2">
<h2><span class="header-section-number">5.3</span> Normalization</h2>
<p>Having variables on different scales can have an impact during learning and at inference time. Consider a study where the data was collected using a wristband that has a light sensor and an accelerometer. The measurement unit of the light sensor is <em>lux</em> whereas the accelerometer’s is <span class="math inline">\(m/s^2\)</span>. After inspecting the dataset, you realize that the <em>min</em> and <em>max</em> values of the light sensor are <span class="math inline">\(0\)</span> and <span class="math inline">\(155\)</span> respectively. The <em>min</em> and <em>max</em> values for the accelerometer are <span class="math inline">\(-0.4\)</span> and <span class="math inline">\(7.45\)</span> respectively. Why is this a problem? Well, several learning methods are based on distances such as <span class="math inline">\(k\)</span>-nn, Nearest centroid, Dynamic Time Warping, etc. thus, distances will be more heavily affected by bigger scales. Furthermore, other methods like neural networks (covered in chapter <a href="deeplearning.html#deeplearning">8</a>) are also affected by different scales. They have a harder time learning their parameters (weights) when data is not normalized. On the other hand, some methods are not affected by different scales, for example, tree-based learners such as decision trees and random forests. Since most of the time you may want to try different methods, it is a good idea to normalize your predictor variables.</p>
<p>A common normalization technique is to scale all the variables between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. Suppose there is a numeric vector <span class="math inline">\(x\)</span> that you want to normalize between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. Let <span class="math inline">\(max(x)\)</span> and <span class="math inline">\(min(x)\)</span> be the maximum and minimum values of <span class="math inline">\(x\)</span>. The following can be used to normalize the <span class="math inline">\(i^{th}\)</span> value of <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[\begin{equation}
  z_i = \frac{x_i - min(x)}{max(x)-min(x)}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(z_i\)</span> is the new normalized <span class="math inline">\(i^{th}\)</span> value, thus, the previous formula is applied to every value in <span class="math inline">\(x\)</span>. The <span class="math inline">\(max(x)\)</span> and <span class="math inline">\(min(x)\)</span> values are parameters learned from the data. Notice that if you will split your data into training and test sets the <em>max</em> and <em>min</em> values (the parameters) are learned only from the train set and then used to normalize both the train and test set. This is to avoid information injection (section <a href="preprocessing.html#infoinjection">5.5</a>). Be also aware that after the parameters are learned from the train set and once the model is deployed in production, it is likely that some input values will be ‘out of range’. If the train set is not very representative of what you will find in real life, some values will probably be smaller than the learned <span class="math inline">\(min(x)\)</span> and some others will be greater than the learned <span class="math inline">\(max(x)\)</span>. Even if the train set is representative of the real-life phenomenon, there is nothing that will prevent some values to be out of range. A simple way to handle this is to truncate the values. In some cases, we do know what are the true minimum and maximum values. For example in image processing, images are usually represented as color intensities between <span class="math inline">\(0\)</span> and <span class="math inline">\(255\)</span>. Here, we know that the <em>min</em> value will always be <span class="math inline">\(0\)</span> and the <em>max</em> value will always be <span class="math inline">\(255\)</span>.</p>
<p>Let’s see an example using the <em>HOME TASKS</em> dataset. The following code first loads the dataset and prints a summary of the first <span class="math inline">\(4\)</span> variables.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="preprocessing.html#cb98-1"></a><span class="co"># Load home activities dataset.</span></span>
<span id="cb98-2"><a href="preprocessing.html#cb98-2"></a>dataset &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="kw">file.path</span>(datasets_path,</span>
<span id="cb98-3"><a href="preprocessing.html#cb98-3"></a>                              <span class="st">&quot;home_tasks&quot;</span>,</span>
<span id="cb98-4"><a href="preprocessing.html#cb98-4"></a>                              <span class="st">&quot;sound_acc.csv&quot;</span>),</span>
<span id="cb98-5"><a href="preprocessing.html#cb98-5"></a>                    <span class="dt">stringsAsFactors =</span> T)</span>
<span id="cb98-6"><a href="preprocessing.html#cb98-6"></a></span>
<span id="cb98-7"><a href="preprocessing.html#cb98-7"></a><span class="co"># Check first 4 variables&#39; min and max values.</span></span>
<span id="cb98-8"><a href="preprocessing.html#cb98-8"></a><span class="kw">summary</span>(dataset[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>])</span>
<span id="cb98-9"><a href="preprocessing.html#cb98-9"></a></span>
<span id="cb98-10"><a href="preprocessing.html#cb98-10"></a><span class="co">#&gt;                label        v1_mfcc1      v1_mfcc2         v1_mfcc3     </span></span>
<span id="cb98-11"><a href="preprocessing.html#cb98-11"></a><span class="co">#&gt; brush_teeth     :180   Min.   :103   Min.   :-17.20   Min.   :-20.90  </span></span>
<span id="cb98-12"><a href="preprocessing.html#cb98-12"></a><span class="co">#&gt; eat_chips       :282   1st Qu.:115   1st Qu.: -8.14   1st Qu.: -7.95  </span></span>
<span id="cb98-13"><a href="preprocessing.html#cb98-13"></a><span class="co">#&gt; mop_floor       :181   Median :120   Median : -3.97   Median : -4.83  </span></span>
<span id="cb98-14"><a href="preprocessing.html#cb98-14"></a><span class="co">#&gt; sweep           :178   Mean   :121   Mean   : -4.50   Mean   : -5.79  </span></span>
<span id="cb98-15"><a href="preprocessing.html#cb98-15"></a><span class="co">#&gt; type_on_keyboard:179   3rd Qu.:126   3rd Qu.: -1.30   3rd Qu.: -3.09  </span></span>
<span id="cb98-16"><a href="preprocessing.html#cb98-16"></a><span class="co">#&gt; wash_hands      :180   Max.   :141   Max.   :  8.98   Max.   :  3.27  </span></span>
<span id="cb98-17"><a href="preprocessing.html#cb98-17"></a><span class="co">#&gt; watch_tv        :206    </span></span></code></pre></div>
<p>Since <em>label</em> is a categorical variable the class counts are printed. For the three remaining variables, we get some statistics including their <em>min</em> and <em>max</em> values. As we can see, the min value of <em>v1_mfcc1</em> is very different from the <em>min</em> value of <em>v1_mfcc2</em> and the same is true for the maximum values, thus, we want to have all variables between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> if we want to use classification methods sensitive to different scales. Let’s assume we want to train a classifier with this data so we divide it into train and test sets:</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="preprocessing.html#cb99-1"></a><span class="co"># Divide into 50/50% train and test set.</span></span>
<span id="cb99-2"><a href="preprocessing.html#cb99-2"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb99-3"><a href="preprocessing.html#cb99-3"></a>folds &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">2</span>, <span class="kw">nrow</span>(dataset), <span class="dt">replace =</span> T)</span>
<span id="cb99-4"><a href="preprocessing.html#cb99-4"></a>trainset &lt;-<span class="st"> </span>dataset[folds <span class="op">==</span><span class="st"> </span><span class="dv">1</span>,]</span>
<span id="cb99-5"><a href="preprocessing.html#cb99-5"></a>testset &lt;-<span class="st"> </span>dataset[folds <span class="op">==</span><span class="st"> </span><span class="dv">2</span>,]</span></code></pre></div>
<p>Now we can define a function that normalizes every numeric or integer variable. If the variable is not numeric or integer it will skip them. The function will take as input a train set and a test set. The parameters (<em>max</em> and <em>min</em>) are learned from the train set and used to normalize both, the train and test sets.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="preprocessing.html#cb100-1"></a><span class="co"># Define a function to normalize the train and test set</span></span>
<span id="cb100-2"><a href="preprocessing.html#cb100-2"></a><span class="co"># based on the parameters learned from the train set.</span></span>
<span id="cb100-3"><a href="preprocessing.html#cb100-3"></a>normalize &lt;-<span class="st"> </span><span class="cf">function</span>(trainset, testset){</span>
<span id="cb100-4"><a href="preprocessing.html#cb100-4"></a>  </span>
<span id="cb100-5"><a href="preprocessing.html#cb100-5"></a>  <span class="co"># Iterate columns</span></span>
<span id="cb100-6"><a href="preprocessing.html#cb100-6"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(trainset)){</span>
<span id="cb100-7"><a href="preprocessing.html#cb100-7"></a>    </span>
<span id="cb100-8"><a href="preprocessing.html#cb100-8"></a>    c &lt;-<span class="st"> </span>trainset[,i] <span class="co"># trainset column</span></span>
<span id="cb100-9"><a href="preprocessing.html#cb100-9"></a>    c2 &lt;-<span class="st"> </span>testset[,i] <span class="co"># testset column</span></span>
<span id="cb100-10"><a href="preprocessing.html#cb100-10"></a>    </span>
<span id="cb100-11"><a href="preprocessing.html#cb100-11"></a>    <span class="co"># Skip if the variable is not numeric or integer.</span></span>
<span id="cb100-12"><a href="preprocessing.html#cb100-12"></a>    <span class="cf">if</span>(<span class="kw">class</span>(c) <span class="op">!=</span><span class="st"> &quot;numeric&quot;</span> <span class="op">&amp;&amp;</span><span class="st"> </span><span class="kw">class</span>(c) <span class="op">!=</span><span class="st"> &quot;integer&quot;</span>)<span class="cf">next</span>;</span>
<span id="cb100-13"><a href="preprocessing.html#cb100-13"></a>    </span>
<span id="cb100-14"><a href="preprocessing.html#cb100-14"></a>    <span class="co"># Learn the max value from the trainset&#39;s column.</span></span>
<span id="cb100-15"><a href="preprocessing.html#cb100-15"></a>    max &lt;-<span class="st"> </span><span class="kw">max</span>(c, <span class="dt">na.rm =</span> T)</span>
<span id="cb100-16"><a href="preprocessing.html#cb100-16"></a>    <span class="co"># Learn the min value from the trainset&#39;s column.</span></span>
<span id="cb100-17"><a href="preprocessing.html#cb100-17"></a>    min &lt;-<span class="st"> </span><span class="kw">min</span>(c, <span class="dt">na.rm =</span> T)</span>
<span id="cb100-18"><a href="preprocessing.html#cb100-18"></a>    </span>
<span id="cb100-19"><a href="preprocessing.html#cb100-19"></a>    <span class="co"># If all values are the same set it to max.</span></span>
<span id="cb100-20"><a href="preprocessing.html#cb100-20"></a>    <span class="cf">if</span>(max<span class="op">==</span>min){</span>
<span id="cb100-21"><a href="preprocessing.html#cb100-21"></a>      trainset[,i] &lt;-<span class="st"> </span>max</span>
<span id="cb100-22"><a href="preprocessing.html#cb100-22"></a>      testset[,i] &lt;-<span class="st"> </span>max</span>
<span id="cb100-23"><a href="preprocessing.html#cb100-23"></a>    }</span>
<span id="cb100-24"><a href="preprocessing.html#cb100-24"></a>    <span class="cf">else</span>{</span>
<span id="cb100-25"><a href="preprocessing.html#cb100-25"></a>      </span>
<span id="cb100-26"><a href="preprocessing.html#cb100-26"></a>      <span class="co"># Normalize trainset&#39;s column.</span></span>
<span id="cb100-27"><a href="preprocessing.html#cb100-27"></a>      trainset[,i] &lt;-<span class="st"> </span>(c <span class="op">-</span><span class="st"> </span>min) <span class="op">/</span><span class="st"> </span>(max <span class="op">-</span><span class="st"> </span>min)</span>
<span id="cb100-28"><a href="preprocessing.html#cb100-28"></a>      </span>
<span id="cb100-29"><a href="preprocessing.html#cb100-29"></a>      <span class="co"># Truncate max values in testset.</span></span>
<span id="cb100-30"><a href="preprocessing.html#cb100-30"></a>      idxs &lt;-<span class="st"> </span><span class="kw">which</span>(c2 <span class="op">&gt;</span><span class="st"> </span>max)</span>
<span id="cb100-31"><a href="preprocessing.html#cb100-31"></a>      <span class="cf">if</span>(<span class="kw">length</span>(idxs) <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>){</span>
<span id="cb100-32"><a href="preprocessing.html#cb100-32"></a>        c2[idxs] &lt;-<span class="st"> </span>max</span>
<span id="cb100-33"><a href="preprocessing.html#cb100-33"></a>      }</span>
<span id="cb100-34"><a href="preprocessing.html#cb100-34"></a>      </span>
<span id="cb100-35"><a href="preprocessing.html#cb100-35"></a>      <span class="co"># Truncate min values in testset.</span></span>
<span id="cb100-36"><a href="preprocessing.html#cb100-36"></a>      idxs &lt;-<span class="st"> </span><span class="kw">which</span>(c2 <span class="op">&lt;</span><span class="st"> </span>min)</span>
<span id="cb100-37"><a href="preprocessing.html#cb100-37"></a>      <span class="cf">if</span>(<span class="kw">length</span>(idxs) <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>){</span>
<span id="cb100-38"><a href="preprocessing.html#cb100-38"></a>        c2[idxs] &lt;-<span class="st"> </span>min</span>
<span id="cb100-39"><a href="preprocessing.html#cb100-39"></a>      }</span>
<span id="cb100-40"><a href="preprocessing.html#cb100-40"></a>      </span>
<span id="cb100-41"><a href="preprocessing.html#cb100-41"></a>      <span class="co"># Normalize testset&#39;s column.</span></span>
<span id="cb100-42"><a href="preprocessing.html#cb100-42"></a>      testset[,i] &lt;-<span class="st"> </span>(c2 <span class="op">-</span><span class="st"> </span>min) <span class="op">/</span><span class="st"> </span>(max <span class="op">-</span><span class="st"> </span>min)</span>
<span id="cb100-43"><a href="preprocessing.html#cb100-43"></a>    }</span>
<span id="cb100-44"><a href="preprocessing.html#cb100-44"></a>  }</span>
<span id="cb100-45"><a href="preprocessing.html#cb100-45"></a>  </span>
<span id="cb100-46"><a href="preprocessing.html#cb100-46"></a>  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">train=</span>trainset, <span class="dt">test=</span>testset))</span>
<span id="cb100-47"><a href="preprocessing.html#cb100-47"></a>}</span></code></pre></div>
<p>Now we can use the previous function to normalize the train and test sets. The function returns a list of two elements: a normalized train and test sets.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="preprocessing.html#cb101-1"></a><span class="co"># Call our function to normalize each set.</span></span>
<span id="cb101-2"><a href="preprocessing.html#cb101-2"></a>normalizedData &lt;-<span class="st"> </span><span class="kw">normalize</span>(trainset, testset)</span>
<span id="cb101-3"><a href="preprocessing.html#cb101-3"></a></span>
<span id="cb101-4"><a href="preprocessing.html#cb101-4"></a><span class="co"># Inspect the normalized train set.</span></span>
<span id="cb101-5"><a href="preprocessing.html#cb101-5"></a><span class="kw">summary</span>(normalizedData<span class="op">$</span>train[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>])</span>
<span id="cb101-6"><a href="preprocessing.html#cb101-6"></a></span>
<span id="cb101-7"><a href="preprocessing.html#cb101-7"></a><span class="co">#&gt;                label        v1_mfcc1        v1_mfcc2        v1_mfcc3    </span></span>
<span id="cb101-8"><a href="preprocessing.html#cb101-8"></a><span class="co">#&gt; brush_teeth     : 88   Min.   :0.000   Min.   :0.000   Min.   :0.000  </span></span>
<span id="cb101-9"><a href="preprocessing.html#cb101-9"></a><span class="co">#&gt; eat_chips       :139   1st Qu.:0.350   1st Qu.:0.403   1st Qu.:0.527  </span></span>
<span id="cb101-10"><a href="preprocessing.html#cb101-10"></a><span class="co">#&gt; mop_floor       : 91   Median :0.464   Median :0.590   Median :0.661  </span></span>
<span id="cb101-11"><a href="preprocessing.html#cb101-11"></a><span class="co">#&gt; sweep           : 84   Mean   :0.474   Mean   :0.568   Mean   :0.616  </span></span>
<span id="cb101-12"><a href="preprocessing.html#cb101-12"></a><span class="co">#&gt; type_on_keyboard: 94   3rd Qu.:0.613   3rd Qu.:0.721   3rd Qu.:0.730  </span></span>
<span id="cb101-13"><a href="preprocessing.html#cb101-13"></a><span class="co">#&gt; wash_hands      :102   Max.   :1.000   Max.   :1.000   Max.   :1.000  </span></span>
<span id="cb101-14"><a href="preprocessing.html#cb101-14"></a><span class="co">#&gt; watch_tv        : 99</span></span>
<span id="cb101-15"><a href="preprocessing.html#cb101-15"></a></span>
<span id="cb101-16"><a href="preprocessing.html#cb101-16"></a><span class="co"># Inspect the normalized test set.</span></span>
<span id="cb101-17"><a href="preprocessing.html#cb101-17"></a><span class="kw">summary</span>(normalizedData<span class="op">$</span>test[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>])</span>
<span id="cb101-18"><a href="preprocessing.html#cb101-18"></a><span class="co">#&gt;                label        v1_mfcc1         v1_mfcc2        v1_mfcc3    </span></span>
<span id="cb101-19"><a href="preprocessing.html#cb101-19"></a><span class="co">#&gt; brush_teeth     : 92   Min.   :0.0046   Min.   :0.000   Min.   :0.000  </span></span>
<span id="cb101-20"><a href="preprocessing.html#cb101-20"></a><span class="co">#&gt; eat_chips       :143   1st Qu.:0.3160   1st Qu.:0.421   1st Qu.:0.500  </span></span>
<span id="cb101-21"><a href="preprocessing.html#cb101-21"></a><span class="co">#&gt; mop_floor       : 90   Median :0.4421   Median :0.606   Median :0.644  </span></span>
<span id="cb101-22"><a href="preprocessing.html#cb101-22"></a><span class="co">#&gt; sweep           : 94   Mean   :0.4569   Mean   :0.582   Mean   :0.603  </span></span>
<span id="cb101-23"><a href="preprocessing.html#cb101-23"></a><span class="co">#&gt; type_on_keyboard: 85   3rd Qu.:0.5967   3rd Qu.:0.728   3rd Qu.:0.724  </span></span>
<span id="cb101-24"><a href="preprocessing.html#cb101-24"></a><span class="co">#&gt; wash_hands      : 78   Max.   :0.9801   Max.   :1.000   Max.   :1.000  </span></span>
<span id="cb101-25"><a href="preprocessing.html#cb101-25"></a><span class="co">#&gt; watch_tv        :107</span></span></code></pre></div>
<p>Now the variables on the train set are exactly between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> for all numeric variables. For the test set, not all <em>min</em> values will be exactly <span class="math inline">\(0\)</span> but a bit higher. Conversely, some <em>max</em> values will be lower than <span class="math inline">\(1\)</span>. This is because the test set may have a <em>min</em> value that is greater than the <em>min</em> value of the train set and a <em>max</em> value that is smaller than the <em>max</em> value of the train set. However, after normalization, all values are guaranteed to be within <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</p>
</div>
<div id="imbalanced-classes" class="section level2">
<h2><span class="header-section-number">5.4</span> Imbalanced Classes</h2>
<p>Ideally, classes will be uniformly distributed, that is, there is approximately the same number of instances per class. In real-life (as always), this is not the case. And in many situations (more often than you may think), <strong>class counts are heavily skewed</strong>. When this happens the dataset is said to be imbalanced. Take as an example, bank transactions. Most of them will be normal whereas a small percent will be fraudulent. In the medical field this is very common. It is easier to collect samples from healthy individuals compared to samples from individuals with some rare conditions. For example, a database may have thousands of images from healthy tissue but just a dozen with signs of cancer. Of course, having just a few cases with diseases is a good thing for the world! but not for machine learning methods. This is because predictive models will try to learn their parameters such that the error is reduced and most of the time this error is based on accuracy, thus, the models will be biased towards making correct predictions for the majority classes (the ones with higher counts) while paying little attention to minority classes. This can be a problem because for some applications we may be more interested in detecting the minority classes (illegal transactions, cancer cases, etc.).</p>
<p>Suppose a given database has <span class="math inline">\(998\)</span> instances with class <em>‘no cancer’</em> and only <span class="math inline">\(2\)</span> instances with class <em>‘cancer’</em>. A trivial classifier that always predicts <em>‘no cancer’</em> will have an accuracy of <span class="math inline">\(98.8\%\)</span> but will not be able to detect any of the <em>‘cancer’</em> cases! So, what can we do?</p>
<ul>
<li><p><strong>Collect more data from the minority class.</strong> In practice, this can be difficult, expensive, etc. or just impossible because the study was conducted a long time ago and it is no longer possible to replicate the context.</p></li>
<li><p><strong>Delete data from the majority class.</strong> Randomly discard instances from the majority class. In the previous example, we could discard <span class="math inline">\(996\)</span> instances of type <em>‘no cancer’</em>. The problem with this is that we end up having small datasets and it will not be enough to learn good predictive models. If you have a huge dataset then this can be an option but in practice, this is rarely the case and you have the risk of having underrepresented samples.</p></li>
<li><p><strong>Create synthetic data.</strong> One of the most common solutions is to create synthetic data from the minority classes. In the following sections two methods that do that will be discussed: <strong>random oversampling</strong> and <strong>SMOTE</strong>.</p></li>
<li><p><strong>Adapt your learning algorithm.</strong> Another option is to use an algorithm that takes into account class counts and weights them accordingly. This is called <strong>cost-sensitive classification</strong>. For example, the <code>rpart()</code> method to train decision trees has a <code>weight</code> parameter which can be used to assign more weight to minority classes. When training neural networks it is also possible to assign different weights to different classes.</p></li>
</ul>
<p>The following two subsections cover two techniques to create synthetic data.</p>
<div id="random-oversampling" class="section level3">
<h3><span class="header-section-number">5.4.1</span> Random Oversampling</h3>

<div class="rmdfolder">
<code>random-oversampling.Rmd</code>
</div>

<p>This method consists of duplicating data points from the minority class. The following code will create an imbalanced dataset with <span class="math inline">\(200\)</span> instances of class <em>‘class1’</em> and only <span class="math inline">\(15\)</span> instances of class <em>‘class2’</em>.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="preprocessing.html#cb102-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb102-2"><a href="preprocessing.html#cb102-2"></a></span>
<span id="cb102-3"><a href="preprocessing.html#cb102-3"></a><span class="co"># Create random data</span></span>
<span id="cb102-4"><a href="preprocessing.html#cb102-4"></a>n1 &lt;-<span class="st"> </span><span class="dv">200</span> <span class="co"># Number of points of majority class.</span></span>
<span id="cb102-5"><a href="preprocessing.html#cb102-5"></a>n2 &lt;-<span class="st"> </span><span class="dv">15</span> <span class="co"># Number of points of minority class.</span></span>
<span id="cb102-6"><a href="preprocessing.html#cb102-6"></a></span>
<span id="cb102-7"><a href="preprocessing.html#cb102-7"></a><span class="co"># Generate random values for class1.</span></span>
<span id="cb102-8"><a href="preprocessing.html#cb102-8"></a>x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="fl">0.5</span>, <span class="dt">n =</span> n1)</span>
<span id="cb102-9"><a href="preprocessing.html#cb102-9"></a>y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>, <span class="dt">n =</span> n1)</span>
<span id="cb102-10"><a href="preprocessing.html#cb102-10"></a>df1 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">label=</span><span class="kw">rep</span>(<span class="st">&quot;class1&quot;</span>, n1),</span>
<span id="cb102-11"><a href="preprocessing.html#cb102-11"></a>                  <span class="dt">x=</span>x, <span class="dt">y=</span>y, <span class="dt">stringsAsFactors =</span> T)</span>
<span id="cb102-12"><a href="preprocessing.html#cb102-12"></a></span>
<span id="cb102-13"><a href="preprocessing.html#cb102-13"></a><span class="co"># Generate random values for class2.</span></span>
<span id="cb102-14"><a href="preprocessing.html#cb102-14"></a>x2 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">mean =</span> <span class="fl">1.5</span>, <span class="dt">sd =</span> <span class="fl">0.5</span>, <span class="dt">n =</span> n2)</span>
<span id="cb102-15"><a href="preprocessing.html#cb102-15"></a>y2 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">mean =</span> <span class="fl">1.5</span>, <span class="dt">sd =</span> <span class="dv">1</span>, <span class="dt">n =</span> n2)</span>
<span id="cb102-16"><a href="preprocessing.html#cb102-16"></a>df2 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">label=</span><span class="kw">rep</span>(<span class="st">&quot;class2&quot;</span>, n2),</span>
<span id="cb102-17"><a href="preprocessing.html#cb102-17"></a>                  <span class="dt">x=</span>x2, <span class="dt">y=</span>y2, <span class="dt">stringsAsFactors =</span> T)</span>
<span id="cb102-18"><a href="preprocessing.html#cb102-18"></a></span>
<span id="cb102-19"><a href="preprocessing.html#cb102-19"></a><span class="co"># This is our imbalanced dataset.</span></span>
<span id="cb102-20"><a href="preprocessing.html#cb102-20"></a>imbalancedDf &lt;-<span class="st"> </span><span class="kw">rbind</span>(df1, df2)</span>
<span id="cb102-21"><a href="preprocessing.html#cb102-21"></a></span>
<span id="cb102-22"><a href="preprocessing.html#cb102-22"></a><span class="co"># Print class counts.</span></span>
<span id="cb102-23"><a href="preprocessing.html#cb102-23"></a><span class="kw">summary</span>(imbalancedDf<span class="op">$</span>label)</span>
<span id="cb102-24"><a href="preprocessing.html#cb102-24"></a><span class="co">#&gt; class1 class2 </span></span>
<span id="cb102-25"><a href="preprocessing.html#cb102-25"></a><span class="co">#&lt;    200     15 </span></span></code></pre></div>
<p>If we want to exactly balance the class counts we will need <span class="math inline">\(185\)</span> instances of type <em>‘class2’</em>. We can use our well known <code>sample()</code> function to pick <span class="math inline">\(185\)</span> points from data frame <code>df2</code> (which contains only instances of class <em>‘class2’</em>) and stored them in <code>new.points</code>. Notice the <code>replace = T</code> parameter. This instructs the function to be allowed to pick repeated elements. Then, the new data points are appended to the imbalanced data set which now becomes balanced.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="preprocessing.html#cb103-1"></a><span class="co"># Generate new points from the minority class.</span></span>
<span id="cb103-2"><a href="preprocessing.html#cb103-2"></a>new.points &lt;-<span class="st"> </span>df2[<span class="kw">sample</span>(<span class="kw">nrow</span>(df2), <span class="dt">size =</span> <span class="dv">185</span>, <span class="dt">replace =</span> T),]</span>
<span id="cb103-3"><a href="preprocessing.html#cb103-3"></a></span>
<span id="cb103-4"><a href="preprocessing.html#cb103-4"></a><span class="co"># Add new points to the imbalanced dataset and save the</span></span>
<span id="cb103-5"><a href="preprocessing.html#cb103-5"></a><span class="co"># result in balancedDf.</span></span>
<span id="cb103-6"><a href="preprocessing.html#cb103-6"></a>balancedDf &lt;-<span class="st"> </span><span class="kw">rbind</span>(imbalancedDf, new.points)</span>
<span id="cb103-7"><a href="preprocessing.html#cb103-7"></a></span>
<span id="cb103-8"><a href="preprocessing.html#cb103-8"></a><span class="co"># Print class counts.</span></span>
<span id="cb103-9"><a href="preprocessing.html#cb103-9"></a><span class="kw">summary</span>(balancedDf<span class="op">$</span>label)</span>
<span id="cb103-10"><a href="preprocessing.html#cb103-10"></a><span class="co">#&gt; class1 class2 </span></span>
<span id="cb103-11"><a href="preprocessing.html#cb103-11"></a><span class="co">#&gt;   200    200 </span></span></code></pre></div>
<p>The code associated with this chapter includes a shiny app<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> <code>random-oversampling.Rmd</code>. Shiny apps are interactive web applications. This shiny app graphically demonstrates how random oversampling works. Figure <a href="preprocessing.html#fig:shinyOversampling">5.9</a> shows the shiny app. The user can move the slider to generate new data points. Please note that the boundaries do not change as the number of instances increases (or decreases). This is because the new points are just duplicates so they overlap with existing ones.</p>
<div class="figure" style="text-align: center"><span id="fig:shinyOversampling"></span>
<img src="images/shiny_oversampling.png" alt="Shiny app with random oversampling example." width="100%" />
<p class="caption">
Figure 5.9: Shiny app with random oversampling example.
</p>
</div>

<div class="rmdcaution">
It is a common mistake to generate synthetic data on the entire dataset before splitting into train and test sets. This will cause your model to be highly overfitted since several duplicate data points can end up in both sets. Create synthetic data <em>only</em> from the <em>train set</em>.
</div>

<p>Random oversampling is simple and effective in many cases. A potential problem is that the models can overfit since there are many duplicate data points. To overcome this, the following method (SMOTE) creates entirely new instances instead of duplicating them.</p>
</div>
<div id="smote" class="section level3">
<h3><span class="header-section-number">5.4.2</span> SMOTE</h3>

<div class="rmdfolder">
<code>auxiliary_functions/functions.R</code>, <code>smote-oversampling.Rmd</code>
</div>

<p>The Synthetic Minority Oversampling Technique (SMOTE) <span class="citation">(Chawla et al. <a href="#ref-chawla2002smote" role="doc-biblioref">2002</a>)</span> can also be used to generate more data points from the minority class. One of the limitations of random oversampling is that it creates duplicates. This has the effect of having fixed boundaries and the classifiers can overspecialize. To avoid this, SMOTE creates entirely new data points.</p>
<p>SMOTE operates on the feature space (on the predictor variables). To generate a new point, take the difference between a given point <span class="math inline">\(a\)</span> (taken from the minority class) and one of its randomly selected nearest neighbors <span class="math inline">\(b\)</span>. The difference is multiplied by a random number between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> and added to <span class="math inline">\(a\)</span>. This has the effect of selecting a point along the line between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. Figure <a href="preprocessing.html#fig:newpoint">5.10</a> illustrates the procedure of generating a new point in two dimensions.</p>
<div class="figure" style="text-align: center"><span id="fig:newpoint"></span>
<img src="images/smote_newpoint.png" alt="Synthetic point generation." width="50%" />
<p class="caption">
Figure 5.10: Synthetic point generation.
</p>
</div>
<p>The number of nearest neighbors <span class="math inline">\(k\)</span> is a parameter defined by the user. In their original work <span class="citation">(Chawla et al. <a href="#ref-chawla2002smote" role="doc-biblioref">2002</a>)</span>, the authors set <span class="math inline">\(k=5\)</span>. Depending on how many new samples need to be generated, <span class="math inline">\(k&#39;\)</span> neighbors are randomly selected from the original <span class="math inline">\(k\)</span> nearest neighbors. For example, if <span class="math inline">\(200\%\)</span> oversampling is needed, <span class="math inline">\(k&#39;=2\)</span> neighbors are selected at random out of the <span class="math inline">\(k=5\)</span> and one data point is generated with each of them. This is performed for each data point in the minority class.</p>
<p>An implementation of SMOTE is provided in <code>auxiliary_functions/functions.R</code>. An example of how to apply it can be found in <code>preprocessing.R</code> in the corresponding directory of this chapter’s code. The <code>smote.class(completeDf, targetClass, N, k)</code> function has several arguments. The first one is the data frame that contains the minority and majority class, that is, the complete dataset. The second argument is the minority class label. The third argument <code>N</code> is the percent of smote and the last one (<code>k</code>) is the number of nearest neighbors to consider.</p>
<p>The following code shows how the function <code>smote.class()</code> can be used to generate new points from the imbalanced dataset that was introduced in the previous section ‘Random Oversampling’. Recall that it has <span class="math inline">\(200\)</span> points of class <em>‘class1’</em> and <span class="math inline">\(15\)</span> points of class <em>‘class2’</em>.</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="preprocessing.html#cb104-1"></a><span class="co"># To balance the dataset, we need to oversample 1200%.</span></span>
<span id="cb104-2"><a href="preprocessing.html#cb104-2"></a><span class="co"># This means that the method will create 12 * 15 new points.</span></span>
<span id="cb104-3"><a href="preprocessing.html#cb104-3"></a><span class="kw">ceiling</span>(<span class="dv">180</span> <span class="op">/</span><span class="st"> </span><span class="dv">15</span>) <span class="op">*</span><span class="st"> </span><span class="dv">100</span></span>
<span id="cb104-4"><a href="preprocessing.html#cb104-4"></a><span class="co">#&gt; [1] 1200</span></span>
<span id="cb104-5"><a href="preprocessing.html#cb104-5"></a></span>
<span id="cb104-6"><a href="preprocessing.html#cb104-6"></a><span class="co"># Percent to oversample.</span></span>
<span id="cb104-7"><a href="preprocessing.html#cb104-7"></a>N &lt;-<span class="st"> </span><span class="dv">1200</span></span>
<span id="cb104-8"><a href="preprocessing.html#cb104-8"></a></span>
<span id="cb104-9"><a href="preprocessing.html#cb104-9"></a><span class="co"># Generate new data points.</span></span>
<span id="cb104-10"><a href="preprocessing.html#cb104-10"></a>synthetic.points &lt;-<span class="st"> </span><span class="kw">smote.class</span>(imbalancedDf,</span>
<span id="cb104-11"><a href="preprocessing.html#cb104-11"></a>                                <span class="dt">targetClass =</span> <span class="st">&quot;class2&quot;</span>,</span>
<span id="cb104-12"><a href="preprocessing.html#cb104-12"></a>                                <span class="dt">N =</span> N,</span>
<span id="cb104-13"><a href="preprocessing.html#cb104-13"></a>                                <span class="dt">k =</span> <span class="dv">5</span>)<span class="op">$</span>synthetic</span>
<span id="cb104-14"><a href="preprocessing.html#cb104-14"></a></span>
<span id="cb104-15"><a href="preprocessing.html#cb104-15"></a><span class="co"># Append the new points to the original dataset.</span></span>
<span id="cb104-16"><a href="preprocessing.html#cb104-16"></a>smote.balancedDf &lt;-<span class="st"> </span><span class="kw">rbind</span>(imbalancedDf,</span>
<span id="cb104-17"><a href="preprocessing.html#cb104-17"></a>                          synthetic.points)</span>
<span id="cb104-18"><a href="preprocessing.html#cb104-18"></a></span>
<span id="cb104-19"><a href="preprocessing.html#cb104-19"></a><span class="co"># Print class counts.</span></span>
<span id="cb104-20"><a href="preprocessing.html#cb104-20"></a><span class="kw">summary</span>(smote.balancedDf<span class="op">$</span>label)</span>
<span id="cb104-21"><a href="preprocessing.html#cb104-21"></a><span class="co">#&gt; class1 class2 </span></span>
<span id="cb104-22"><a href="preprocessing.html#cb104-22"></a><span class="co">#&gt;    200    195 </span></span></code></pre></div>
<p>The parameter <code>N</code> is set to <span class="math inline">\(1200\)</span>. This will create <span class="math inline">\(12\)</span> new data points for every minority class instance (<span class="math inline">\(15\)</span>). Thus, the method will return <span class="math inline">\(180\)</span> instances. In this case, <span class="math inline">\(k\)</span> is set to <span class="math inline">\(5\)</span>. Finally, the new points are appended to the imbalanced dataset having a total of <span class="math inline">\(195\)</span> samples of class ‘class2’.</p>
<p>Again, a shiny app is included with this chapter’s code. The shiny app shows an example of applying SMOTE. Figure <a href="preprocessing.html#fig:shinySMOTE">5.11</a> shows the distribution of points before SMOTE and after applying SMOTE. Note how the boundary of <em>‘class2’</em> changes after SMOTE. It slightly spans in all directions. This is particularly visible in the lower right corner. This boundary expansion is what allows the classifiers to generalize better as compared to training them using random oversampled data.</p>
<div class="figure" style="text-align: center"><span id="fig:shinySMOTE"></span>
<img src="images/shiny_smote.png" alt="Shiny app with SMOTE example." width="100%" />
<p class="caption">
Figure 5.11: Shiny app with SMOTE example.
</p>
</div>
</div>
</div>
<div id="infoinjection" class="section level2">
<h2><span class="header-section-number">5.5</span> Information Injection</h2>
<p>The purpose of dividing the data into train/validation/test sets is to be able to accurately estimate the generalization performance of a predictive model when it is presented with previously unseen data points. So, it is advisable to construct such set splits in a way that they are as independent as possible. Often, before training a model and generating predictions out of it, the data needs to be preprocessed. Preprocessing operations may include imputing missing values, normalizing, and so on. During those operations, some information can be inadvertently transferred from the train to the test set thus, violating the assumption that they are independent.</p>

<div class="rmdinfo">
Information injection occurs when information from the train set is transferred to the test set. When having train/validation/test sets information injection occurs when information from the train set leaks into the validation and/or test set. It also happens when information from the validation set is transferred to the test set.
</div>

<p>Suppose that as one of the preprocessing steps, you need to subtract the mean value for each feature for a given instance. For now, suppose a dataset has a single feature <span class="math inline">\(x\)</span> of numeric type and a categorical response variable <span class="math inline">\(y\)</span>. The dataset has <span class="math inline">\(n\)</span> rows. As a preprocessing step, you decide that you need to subtract the mean of <span class="math inline">\(x\)</span> from each data point. Since you want to predict <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span>, you train a classifier by splitting your data into train and test sets as usual. So you proceed with the steps depicted in Figure <a href="preprocessing.html#fig:injection1">5.12</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:injection1"></span>
<img src="images/info_injection_1.png" alt="Information injection example." width="90%" />
<p class="caption">
Figure 5.12: Information injection example.
</p>
</div>
<p>First (a), you compute the <span class="math inline">\(mean\)</span> value of the of variable <span class="math inline">\(x\)</span> from the entire dataset. This <span class="math inline">\(mean\)</span> is known as the parameter. In this case, there is only one parameter but there could be several. For example, we could additionally need to know the standard deviation. Once we know the mean value, the dataset is divided into train and test sets (b). Finally, the <span class="math inline">\(mean\)</span> is subtracted from each element in the train and test sets (c). Without realizing, we have transferred information from the train set to the test set! But, how did this happen? Well, the <em>mean</em> parameter was computed using information from the <em>entire</em> dataset. Then, that <span class="math inline">\(mean\)</span> parameter was used on the test set but it was calculated using data points that also belong to that same test set!.</p>
<p>Figure <a href="preprocessing.html#fig:injection2">5.13</a> shows how to correctly do the preprocessing to avoid information injection. The dataset is first split (a). Then, the <span class="math inline">\(mean\)</span> parameter is calculated only with data points from the train set. Finally, the <em>mean</em> parameter is subtracted from both sets. Here, the mean contains information only from the train set.</p>
<div class="figure" style="text-align: center"><span id="fig:injection2"></span>
<img src="images/info_injection_2.png" alt="No information injection example." width="90%" />
<p class="caption">
Figure 5.13: No information injection example.
</p>
</div>
<p>In the previous example, we assumed that the dataset was split into one train and one test set only once. The same idea applies when performing <span class="math inline">\(k\)</span>-fold cross-validation. In each of the <span class="math inline">\(k\)</span> iterations, the preprocessing parameters need to be learned only from the train split.</p>
</div>
<div id="one-hot-encoding" class="section level2">
<h2><span class="header-section-number">5.6</span> One-hot Encoding</h2>
<p>Several algorithms need some or all input variables to be in numeric format -either the response and/or predictor variables. In R, for most classification algorithms, the class is usually encoded as a factor but some implementations may require it to be in numeric format. Sometimes we may have some categorical variables as predictors such as gender (<em>‘male’</em>, <em>‘female’</em>). Some algorithms need those to be in numeric format because they, for example, are based on distance computations such as <span class="math inline">\(k\)</span>-nn or Nearest neighbors. Some other models need to perform arithmetic operations on the predictor variables like neural networks.</p>
<p>One way to convert categorical variables into numeric ones is called <strong>one-hot encoding</strong> and it works by creating new variables sometimes called <strong>dummy variables</strong> which are boolean, one for each possible category. Suppose a dataset has a categorical variable <em>Job</em> (Figure <a href="preprocessing.html#fig:onehotenc">5.14</a>) with three possible values: <em>programmer</em>, <em>teacher</em> and <em>dentist</em>. This variable can be one-hot encoded by creating <span class="math inline">\(3\)</span> new boolean dummy variables and setting them to <span class="math inline">\(1\)</span> for the corresponding category and <span class="math inline">\(0\)</span> for the rest.</p>
<div class="figure" style="text-align: center"><span id="fig:onehotenc"></span>
<img src="images/onehotenc.png" alt="One-hot encoding example" width="100%" />
<p class="caption">
Figure 5.14: One-hot encoding example
</p>
</div>

<div class="rmdcaution">
You should be aware of the dummy variable trap which means that one variable can be predicted from the others. For example, if the possible values are just <em>male</em> and <em>female</em>, then if the dummy variable for male is <span class="math inline">\(1\)</span> we know that the dummy variable for female must be <span class="math inline">\(0\)</span>. The solution to this is to drop one of the newly created variables. Which one? Any variable can be dropped, it does not matter which one. This trap only applies when the variable is a predictor. If it is a response variable, nothing should be dropped.
</div>

<p>Figure <a href="preprocessing.html#fig:variableConversion">5.15</a> shows a guideline about how to convert non-numeric variables into numeric ones for classification tasks. This is only a guideline and the process will depend on each application.</p>
<div class="figure" style="text-align: center"><span id="fig:variableConversion"></span>
<img src="images/variable_conversion.png" alt="Variable conversion." width="100%" />
<p class="caption">
Figure 5.15: Variable conversion.
</p>
</div>
<p>The <code>caret</code> package has a function <code>dummyVars()</code> that can be used to one-hot encode the categorical variables of a data frame. Since the <em>STUDENTS’ MENTAL HEALTH</em> dataset <span class="citation">(Nguyen et al. <a href="#ref-Minh2019" role="doc-biblioref">2019</a>)</span> has several categorical variables, it can be used to demonstrate how to apply <code>dummyVars()</code>. This dataset collected at a University in Japan contains survey responses from students about their mental health and help-seeking behaviors. We begin by loading the data.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="preprocessing.html#cb105-1"></a><span class="co"># Load students mental health behavior dataset.</span></span>
<span id="cb105-2"><a href="preprocessing.html#cb105-2"></a><span class="co"># stringsAsFactors is set to F since the function</span></span>
<span id="cb105-3"><a href="preprocessing.html#cb105-3"></a><span class="co"># that we will use to one-hot encode expects characters.</span></span>
<span id="cb105-4"><a href="preprocessing.html#cb105-4"></a>dataset &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="kw">file.path</span>(datasets_path,</span>
<span id="cb105-5"><a href="preprocessing.html#cb105-5"></a>                              <span class="st">&quot;students_mental_health&quot;</span>,</span>
<span id="cb105-6"><a href="preprocessing.html#cb105-6"></a>                              <span class="st">&quot;data.csv&quot;</span>),</span>
<span id="cb105-7"><a href="preprocessing.html#cb105-7"></a>                              <span class="dt">stringsAsFactors =</span> F)</span></code></pre></div>
<p>Note that the <code>stringsAsFactors</code> parameter is set to <code>FALSE</code>. We need this because <code>dummyVars()</code> needs characters to work properly. Before one-hot encoding the variables we need to do some preprocessing to clean the dataset. This dataset contains several fields with empty characters ‘""’. Thus, we will replace them with <code>NA</code> with the <code>replace_with_na_all()</code> function from the <code>naniar</code> package. This package was first described in the missing values section of this chapter but that function was not discussed. The function takes as first argument the dataset and the second argument is a formula that includes a condition.</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="preprocessing.html#cb106-1"></a><span class="co"># The dataset contains several empty strings.</span></span>
<span id="cb106-2"><a href="preprocessing.html#cb106-2"></a><span class="co"># Replace those empty strings with NAs so the following</span></span>
<span id="cb106-3"><a href="preprocessing.html#cb106-3"></a><span class="co"># methods will work properly.</span></span>
<span id="cb106-4"><a href="preprocessing.html#cb106-4"></a><span class="co"># We can use the replace_with_na_all() function</span></span>
<span id="cb106-5"><a href="preprocessing.html#cb106-5"></a><span class="co"># from naniar package to do the replacement.</span></span>
<span id="cb106-6"><a href="preprocessing.html#cb106-6"></a><span class="kw">library</span>(naniar)</span>
<span id="cb106-7"><a href="preprocessing.html#cb106-7"></a>dataset &lt;-<span class="st"> </span><span class="kw">replace_with_na_all</span>(dataset,</span>
<span id="cb106-8"><a href="preprocessing.html#cb106-8"></a>                               <span class="op">~</span>.x <span class="op">%in%</span><span class="st"> </span>common_na_strings)</span></code></pre></div>
<p>In this case, the condition is <code>~.x %in% common_na_strings</code> which means: replace all fields that contain one of the characters in <code>common_na_strings</code>. The variable <code>common_na_strings</code> contains a set of common strings that can be regarded as missing values for example ‘“NA”’, ‘“na”’, ‘“NULL”’, the empty string ‘""’ and so on. Now, we can use the <code>vis_miss()</code> function described in the missing values section to get a visual idea of the missing values.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="preprocessing.html#cb107-1"></a><span class="co"># Visualize missing values.</span></span>
<span id="cb107-2"><a href="preprocessing.html#cb107-2"></a><span class="kw">vis_miss</span>(dataset, <span class="dt">warn_large_data =</span> F)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:mentalmissing"></span>
<img src="images/mentalmissing.png" alt="Missing values in the students mental health dataset." width="90%" />
<p class="caption">
Figure 5.16: Missing values in the students mental health dataset.
</p>
</div>
<p>Figure <a href="preprocessing.html#fig:mentalmissing">5.16</a> shows the output plot. We can see that the last rows contain many missing values so we will discard them and only keep the first rows (<span class="math inline">\(1-268\)</span>).</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="preprocessing.html#cb108-1"></a><span class="co"># Since the last rows starting at 269</span></span>
<span id="cb108-2"><a href="preprocessing.html#cb108-2"></a><span class="co"># are full of missing values we will discard them.</span></span>
<span id="cb108-3"><a href="preprocessing.html#cb108-3"></a>dataset &lt;-<span class="st"> </span>dataset[<span class="dv">1</span><span class="op">:</span><span class="dv">268</span>,]</span></code></pre></div>
<p>As an example, we will one-hot encode the <em>Stay_Cate</em> variable which represents how long a student has been at the university: 1 year (Short), 2–3 years (Medium), or at least 4 years (Long). The <code>dummyVars()</code> function takes a formula as its first argument. Here, we specify that we only want to convert <code>Stay_Cate</code>. This function does not do the actual encoding but returns an object that is used with <code>predict()</code> to obtain the encoded variable(s) as a new data frame.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="preprocessing.html#cb109-1"></a><span class="co"># One-hot encode the Stay_Cate variable.</span></span>
<span id="cb109-2"><a href="preprocessing.html#cb109-2"></a><span class="co"># This variable Stay_Cate has three possible</span></span>
<span id="cb109-3"><a href="preprocessing.html#cb109-3"></a><span class="co"># values: Long, Short and Medium.</span></span>
<span id="cb109-4"><a href="preprocessing.html#cb109-4"></a><span class="co"># First, create a dummyVars object with the dummyVars()</span></span>
<span id="cb109-5"><a href="preprocessing.html#cb109-5"></a><span class="co">#function from caret package.</span></span>
<span id="cb109-6"><a href="preprocessing.html#cb109-6"></a><span class="kw">library</span>(caret)</span>
<span id="cb109-7"><a href="preprocessing.html#cb109-7"></a></span>
<span id="cb109-8"><a href="preprocessing.html#cb109-8"></a>dummyObj &lt;-<span class="st"> </span><span class="kw">dummyVars</span>( <span class="op">~</span><span class="st"> </span>Stay_Cate, <span class="dt">data =</span> dataset)</span>
<span id="cb109-9"><a href="preprocessing.html#cb109-9"></a></span>
<span id="cb109-10"><a href="preprocessing.html#cb109-10"></a><span class="co"># Perform the actual encoding using predict()</span></span>
<span id="cb109-11"><a href="preprocessing.html#cb109-11"></a>encodedVars &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">predict</span>(dummyObj,</span>
<span id="cb109-12"><a href="preprocessing.html#cb109-12"></a>                                  <span class="dt">newdata =</span> dataset))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:stayCate1"></span>
<img src="images/stay_cate_1.png" alt="One-hot encoded Stay Cate." width="50%" />
<p class="caption">
Figure 5.17: One-hot encoded Stay Cate.
</p>
</div>
<p>If we inspect the resulting data frame we see that it has <span class="math inline">\(3\)</span> variables, one for each possible value: Long, Medium and Short. If this variable is going to be used as a predictor variable, we should delete one of its columns to avoid the dummy variable trap. We can do this by setting the parameter <code>fullRank = TRUE</code>.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="preprocessing.html#cb110-1"></a>dummyObj &lt;-<span class="st"> </span><span class="kw">dummyVars</span>( <span class="op">~</span><span class="st"> </span>Stay_Cate, <span class="dt">data =</span> dataset, <span class="dt">fullRank =</span> <span class="ot">TRUE</span>)</span>
<span id="cb110-2"><a href="preprocessing.html#cb110-2"></a>encodedVars &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">predict</span>(dummyObj, <span class="dt">newdata =</span> dataset))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:stayCate2"></span>
<img src="images/stay_cate_2.png" alt="One-hot encoded Stay Cate dropping one of the columns." width="40%" />
<p class="caption">
Figure 5.18: One-hot encoded Stay Cate dropping one of the columns.
</p>
</div>
<p>In this situation, the column with ‘Long’ was discarded. If you want to one-hot encode all variables at once you can use <code>~ .</code> as the formula. But be aware that the dataset may have some categories encoded as numeric and thus will not be transformed. For example, the <em>Age_cate</em> encodes age categories but the categories are represented as integers from <span class="math inline">\(1\)</span> to <span class="math inline">\(5\)</span>. In this case, it may be ok not to encode this variable since lower integer numbers also imply smaller ages and bigger integer numbers represent older ages. If you still want to encode this variable you could first convert it to character by appending a letter at the beginning. Sometimes you should encode a variable, for example, if it represents colors. In that situation, it does not make sense to leave it as numeric since there is not semantic order between colors.</p>

<div class="rmdinfo">
Actually, in some very rare situations, it would make sense to leave color categories as integers. For example, if they represent a gradient like white, light blue, blue, dark blue and black in which case this could be treated as an ordinal variable.
</div>

</div>
<div id="SummaryPreprocessing" class="section level2">
<h2><span class="header-section-number">5.7</span> Summary</h2>
<p>Programming functions that train predictive models expect the data to be in a particular format. Furthermore, some methods make assumptions about the data like having no missing values, having all variables in the same scale and so on. This chapter presented several commonly used methods to preprocess our datasets before using them to train models.</p>
<ul>
<li>When collecting data from different sensors, we can face several sources of variation like <strong>sensor’ format</strong>, <strong>different sampling rates</strong>, <strong>different scales</strong>, and so on.</li>
<li>Some preprocessing methods can lead to <strong>information injection</strong>. This happens when some information from the train set is leaked to the test set.</li>
<li><strong>Missing values</strong> is a common problem in many data analysis tasks. In R, the <code>naniar</code> package can be used to spot missing values.</li>
<li><strong>Imputation</strong> is the process of inferring the values of missing values. The <code>simputation</code> package can be used to impute missing values in datasets.</li>
<li><strong>Normalization</strong> is the process of transforming a set of variables to a common scale. For example from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>.</li>
<li>An <strong>imbalanced dataset</strong> has a disproportionate number of classes of a certain type with respect to the others. Some methods like <strong>random over/under sampling</strong> and <strong>SMOTE</strong> can be used to balance a dataset.</li>
<li><strong>One-hot-encoding</strong> is a method that converts categorical variables into numeric ones.</li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-chawla2002smote">
<p>Chawla, Nitesh V, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. 2002. “SMOTE: Synthetic Minority over-Sampling Technique.” <em>Journal of Artificial Intelligence Research</em> 16: 321–57.</p>
</div>
<div id="ref-kamminga2017">
<p>Kamminga, Jacob W, Helena C Bisby, Duc V Le, Nirvana Meratnia, and Paul JM Havinga. 2017. “Generic Online Animal Activity Recognition on Collar Tags.” In <em>Proceedings of the 2017 Acm International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 Acm International Symposium on Wearable Computers</em>, 597–606.</p>
</div>
<div id="ref-Minh2019">
<p>Nguyen, Minh-Hoang, Manh-Toan Ho, Quynh-Yen T. Nguyen, and Quan-Hoang Vuong. 2019. “A Dataset of Students’ Mental Health and Help-Seeking Behaviors in a Multicultural Environment.” <em>Data</em> 4 (3). <a href="https://doi.org/10.3390/data4030124">https://doi.org/10.3390/data4030124</a>.</p>
</div>
<div id="ref-naniar">
<p>Tierney, Nicholas, Di Cook, Miles McBain, and Colin Fay. 2019. <em>Naniar: Data Structures, Summaries, and Visualisations for Missing Data</em>. <a href="https://CRAN.R-project.org/package=naniar">https://CRAN.R-project.org/package=naniar</a>.</p>
</div>
<div id="ref-simputation">
<p>van der Loo, Mark. 2019. <em>Simputation: Simple Imputation</em>. <a href="https://CRAN.R-project.org/package=simputation">https://CRAN.R-project.org/package=simputation</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="8">
<li id="fn8"><p><a href="https://cran.r-project.org/web/packages/simputation/vignettes/intro.html" class="uri">https://cran.r-project.org/web/packages/simputation/vignettes/intro.html</a><a href="preprocessing.html#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p><a href="https://commons.wikimedia.org/wiki/File:Moving_Average_Types_comparison_-_Simple_and_Exponential.png" class="uri">https://commons.wikimedia.org/wiki/File:Moving_Average_Types_comparison_-_Simple_and_Exponential.png</a><a href="preprocessing.html#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p><a href="https://en.wikiversity.org/wiki/Moving_Average" class="uri">https://en.wikiversity.org/wiki/Moving_Average</a><a href="preprocessing.html#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p><a href="https://shiny.rstudio.com/" class="uri">https://shiny.rstudio.com/</a><a href="preprocessing.html#fnref11" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="edavis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="unsupervised.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
