<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.3 Decision Trees | Behavior Analysis with Machine Learning and R</title>
  <meta name="description" content="2.3 Decision Trees | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors or stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="2.3 Decision Trees | Behavior Analysis with Machine Learning and R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="2.3 Decision Trees | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors or stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.3 Decision Trees | Behavior Analysis with Machine Learning and R" />
  
  <meta name="twitter:description" content="2.3 Decision Trees | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors or stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  <meta name="twitter:image" content="images/cover.png" />

<meta name="author" content="Enrique Garcia Ceja" />


<meta name="date" content="2020-09-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="performance-metrics.html"/>
<link rel="next" href="dynamic-time-warping.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/d3-4.9.0/d3.min.js"></script>
<script src="libs/d3-tip-0.7.1/index-min.js"></script>
<link href="libs/d3panels-1.4.9/d3panels.min.css" rel="stylesheet" />
<script src="libs/d3panels-1.4.9/d3panels.min.js"></script>
<script src="libs/qtlcharts_iplotCorr-0.11.6/iplotCorr.js"></script>
<script src="libs/qtlcharts_iplotCorr-0.11.6/iplotCorr_noscat.js"></script>
<script src="libs/iplotCorr-binding-0.11.6/iplotCorr.js"></script>
<link href="libs/dygraphs-1.1.1/dygraph.css" rel="stylesheet" />
<script src="libs/dygraphs-1.1.1/dygraph-combined.js"></script>
<script src="libs/dygraphs-1.1.1/shapes.js"></script>
<script src="libs/moment-2.8.4/moment.js"></script>
<script src="libs/moment-timezone-0.2.5/moment-timezone-with-data.js"></script>
<script src="libs/moment-fquarter-1.0.0/moment-fquarter.min.js"></script>
<script src="libs/dygraphs-binding-1.1.1.6/dygraphs.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178679335-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178679335-1', { 'anonymize_ip': true });
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="#">Behavior Analysis with Machine Learning and R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#supplemental-material"><i class="fa fa-check"></i>Supplemental Material</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#conventions"><i class="fa fa-check"></i>Conventions</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.2" data-path="taxonomy.html"><a href="taxonomy.html"><i class="fa fa-check"></i><b>1.2</b> Types of Machine Learning</a></li>
<li class="chapter" data-level="1.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>1.3</b> Terminology</a><ul>
<li class="chapter" data-level="1.3.1" data-path="terminology.html"><a href="terminology.html#tables"><i class="fa fa-check"></i><b>1.3.1</b> Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="terminology.html"><a href="terminology.html#variable-types"><i class="fa fa-check"></i><b>1.3.2</b> Variable types</a></li>
<li class="chapter" data-level="1.3.3" data-path="terminology.html"><a href="terminology.html#predictive-models"><i class="fa fa-check"></i><b>1.3.3</b> Predictive Models</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="pipeline.html"><a href="pipeline.html"><i class="fa fa-check"></i><b>1.4</b> Data Analysis Pipeline</a></li>
<li class="chapter" data-level="1.5" data-path="trainingeval.html"><a href="trainingeval.html"><i class="fa fa-check"></i><b>1.5</b> Evaluating Predictive Models</a></li>
<li class="chapter" data-level="1.6" data-path="simple-classification-example.html"><a href="simple-classification-example.html"><i class="fa fa-check"></i><b>1.6</b> Simple Classification Example</a><ul>
<li class="chapter" data-level="1.6.1" data-path="simple-classification-example.html"><a href="simple-classification-example.html#k-fold-cross-validation-example"><i class="fa fa-check"></i><b>1.6.1</b> K-fold Cross-Validation Example</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="simple-regression-example.html"><a href="simple-regression-example.html"><i class="fa fa-check"></i><b>1.7</b> Simple Regression Example</a></li>
<li class="chapter" data-level="1.8" data-path="underfitting-and-overfitting.html"><a href="underfitting-and-overfitting.html"><i class="fa fa-check"></i><b>1.8</b> Underfitting and Overfitting</a></li>
<li class="chapter" data-level="1.9" data-path="bias-and-variance.html"><a href="bias-and-variance.html"><i class="fa fa-check"></i><b>1.9</b> Bias and Variance</a></li>
<li class="chapter" data-level="1.10" data-path="SummaryIntro.html"><a href="SummaryIntro.html"><i class="fa fa-check"></i><b>1.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>2</b> Predicting Behavior with Classification Models</a><ul>
<li class="chapter" data-level="2.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html"><i class="fa fa-check"></i><b>2.1</b> <em>k</em>-nearest Neighbors</a><ul>
<li class="chapter" data-level="2.1.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#indoor-location-with-wi-fi-signals"><i class="fa fa-check"></i><b>2.1.1</b> Indoor Location with Wi-Fi Signals</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="performance-metrics.html"><a href="performance-metrics.html"><i class="fa fa-check"></i><b>2.2</b> Performance Metrics</a></li>
<li class="chapter" data-level="2.3" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>2.3</b> Decision Trees</a><ul>
<li class="chapter" data-level="2.3.1" data-path="decision-trees.html"><a href="decision-trees.html#activityRecognition"><i class="fa fa-check"></i><b>2.3.1</b> Activity Recognition with Smartphones</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="dynamic-time-warping.html"><a href="dynamic-time-warping.html"><i class="fa fa-check"></i><b>2.4</b> Dynamic Time Warping</a><ul>
<li class="chapter" data-level="2.4.1" data-path="dynamic-time-warping.html"><a href="dynamic-time-warping.html#sechandgestures"><i class="fa fa-check"></i><b>2.4.1</b> Hand Gesture Recognition</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="summaryClassification.html"><a href="summaryClassification.html"><i class="fa fa-check"></i><b>2.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>3</b> Predicting Behavior with Ensemble Learning</a><ul>
<li class="chapter" data-level="3.1" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>3.1</b> Bagging</a><ul>
<li class="chapter" data-level="3.1.1" data-path="bagging.html"><a href="bagging.html#activity-recognition-with-bagging"><i class="fa fa-check"></i><b>3.1.1</b> Activity recognition with Bagging</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="random-forest.html"><a href="random-forest.html"><i class="fa fa-check"></i><b>3.2</b> Random Forest</a></li>
<li class="chapter" data-level="3.3" data-path="stacked-generalization.html"><a href="stacked-generalization.html"><i class="fa fa-check"></i><b>3.3</b> Stacked Generalization</a></li>
<li class="chapter" data-level="3.4" data-path="multiviewhometasks.html"><a href="multiviewhometasks.html"><i class="fa fa-check"></i><b>3.4</b> Multi-view Stacking for Home Tasks Recognition</a></li>
<li class="chapter" data-level="3.5" data-path="SummaryEnsemble.html"><a href="SummaryEnsemble.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="edavis.html"><a href="edavis.html"><i class="fa fa-check"></i><b>4</b> Exploring and Visualizing Behavioral Data</a><ul>
<li class="chapter" data-level="4.1" data-path="talking-with-field-experts.html"><a href="talking-with-field-experts.html"><i class="fa fa-check"></i><b>4.1</b> Talking with field experts</a></li>
<li class="chapter" data-level="4.2" data-path="summary-statistics.html"><a href="summary-statistics.html"><i class="fa fa-check"></i><b>4.2</b> Summary Statistics</a></li>
<li class="chapter" data-level="4.3" data-path="class-distributions.html"><a href="class-distributions.html"><i class="fa fa-check"></i><b>4.3</b> Class Distributions</a></li>
<li class="chapter" data-level="4.4" data-path="user-class-sparsity-matrix.html"><a href="user-class-sparsity-matrix.html"><i class="fa fa-check"></i><b>4.4</b> User-Class Sparsity Matrix</a></li>
<li class="chapter" data-level="4.5" data-path="boxplots.html"><a href="boxplots.html"><i class="fa fa-check"></i><b>4.5</b> Boxplots</a></li>
<li class="chapter" data-level="4.6" data-path="correlation-plots.html"><a href="correlation-plots.html"><i class="fa fa-check"></i><b>4.6</b> Correlation Plots</a><ul>
<li class="chapter" data-level="4.6.1" data-path="correlation-plots.html"><a href="correlation-plots.html#interactive-correlation-plots"><i class="fa fa-check"></i><b>4.6.1</b> Interactive Correlation Plots</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="timeseries.html"><a href="timeseries.html"><i class="fa fa-check"></i><b>4.7</b> Timeseries</a><ul>
<li class="chapter" data-level="4.7.1" data-path="timeseries.html"><a href="timeseries.html#interactive-timeseries"><i class="fa fa-check"></i><b>4.7.1</b> Interactive Timeseries</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="multidimensional-scaling-mds.html"><a href="multidimensional-scaling-mds.html"><i class="fa fa-check"></i><b>4.8</b> Multidimensional Scaling (MDS)</a></li>
<li class="chapter" data-level="4.9" data-path="heatmaps.html"><a href="heatmaps.html"><i class="fa fa-check"></i><b>4.9</b> Heatmaps</a></li>
<li class="chapter" data-level="4.10" data-path="automated-eda.html"><a href="automated-eda.html"><i class="fa fa-check"></i><b>4.10</b> Automated EDA</a></li>
<li class="chapter" data-level="4.11" data-path="SummaryExploratory.html"><a href="SummaryExploratory.html"><i class="fa fa-check"></i><b>4.11</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="preprocessing.html"><a href="preprocessing.html"><i class="fa fa-check"></i><b>5</b> Preprocessing Behavioral Data</a><ul>
<li class="chapter" data-level="5.1" data-path="missing-values.html"><a href="missing-values.html"><i class="fa fa-check"></i><b>5.1</b> Missing Values</a><ul>
<li class="chapter" data-level="5.1.1" data-path="missing-values.html"><a href="missing-values.html#imputation"><i class="fa fa-check"></i><b>5.1.1</b> Imputation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>5.2</b> Smoothing</a></li>
<li class="chapter" data-level="5.3" data-path="normalization.html"><a href="normalization.html"><i class="fa fa-check"></i><b>5.3</b> Normalization</a></li>
<li class="chapter" data-level="5.4" data-path="imbalanced-classes.html"><a href="imbalanced-classes.html"><i class="fa fa-check"></i><b>5.4</b> Imbalanced Classes</a><ul>
<li class="chapter" data-level="5.4.1" data-path="imbalanced-classes.html"><a href="imbalanced-classes.html#random-oversampling"><i class="fa fa-check"></i><b>5.4.1</b> Random oversampling</a></li>
<li class="chapter" data-level="5.4.2" data-path="imbalanced-classes.html"><a href="imbalanced-classes.html#smote"><i class="fa fa-check"></i><b>5.4.2</b> SMOTE</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="infoinjection.html"><a href="infoinjection.html"><i class="fa fa-check"></i><b>5.5</b> Information Injection</a></li>
<li class="chapter" data-level="5.6" data-path="one-hot-encoding.html"><a href="one-hot-encoding.html"><i class="fa fa-check"></i><b>5.6</b> One-hot Encoding</a></li>
<li class="chapter" data-level="5.7" data-path="SummaryPreprocessing.html"><a href="SummaryPreprocessing.html"><i class="fa fa-check"></i><b>5.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="unsupervised.html"><a href="unsupervised.html"><i class="fa fa-check"></i><b>6</b> Discovering Behaviors with Unsupervised Learning</a><ul>
<li class="chapter" data-level="6.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>6.1</b> K-means clustering</a><ul>
<li class="chapter" data-level="6.1.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#studentresponses"><i class="fa fa-check"></i><b>6.1.1</b> Grouping Student Responses</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="the-silhouette-index.html"><a href="the-silhouette-index.html"><i class="fa fa-check"></i><b>6.2</b> The Silhouette Index</a></li>
<li class="chapter" data-level="6.3" data-path="associationrules.html"><a href="associationrules.html"><i class="fa fa-check"></i><b>6.3</b> Mining Association Rules</a><ul>
<li class="chapter" data-level="6.3.1" data-path="associationrules.html"><a href="associationrules.html#finding-rules-for-criminal-behavior"><i class="fa fa-check"></i><b>6.3.1</b> Finding Rules for Criminal Behavior</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="SummaryUnsupervised.html"><a href="SummaryUnsupervised.html"><i class="fa fa-check"></i><b>6.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="representations.html"><a href="representations.html"><i class="fa fa-check"></i><b>7</b> Encoding Behavioral Data</a><ul>
<li class="chapter" data-level="7.1" data-path="feature-vectors.html"><a href="feature-vectors.html"><i class="fa fa-check"></i><b>7.1</b> Feature Vectors</a></li>
<li class="chapter" data-level="7.2" data-path="sectimeseries.html"><a href="sectimeseries.html"><i class="fa fa-check"></i><b>7.2</b> Timeseries</a></li>
<li class="chapter" data-level="7.3" data-path="transactions.html"><a href="transactions.html"><i class="fa fa-check"></i><b>7.3</b> Transactions</a></li>
<li class="chapter" data-level="7.4" data-path="images.html"><a href="images.html"><i class="fa fa-check"></i><b>7.4</b> Images</a></li>
<li class="chapter" data-level="7.5" data-path="recurrence-plots.html"><a href="recurrence-plots.html"><i class="fa fa-check"></i><b>7.5</b> Recurrence Plots</a><ul>
<li class="chapter" data-level="7.5.1" data-path="recurrence-plots.html"><a href="recurrence-plots.html#computing-recurence-plots"><i class="fa fa-check"></i><b>7.5.1</b> Computing Recurence Plots</a></li>
<li class="chapter" data-level="7.5.2" data-path="recurrence-plots.html"><a href="recurrence-plots.html#recurrence-plots-of-hand-gestures"><i class="fa fa-check"></i><b>7.5.2</b> Recurrence Plots of Hand Gestures</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="bag-of-words.html"><a href="bag-of-words.html"><i class="fa fa-check"></i><b>7.6</b> Bag-of-Words</a><ul>
<li class="chapter" data-level="7.6.1" data-path="bag-of-words.html"><a href="bag-of-words.html#bow-for-complex-activities."><i class="fa fa-check"></i><b>7.6.1</b> BoW for Complex Activities.</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="graphs.html"><a href="graphs.html"><i class="fa fa-check"></i><b>7.7</b> Graphs</a><ul>
<li class="chapter" data-level="7.7.1" data-path="graphs.html"><a href="graphs.html#complex-activities-as-graphs"><i class="fa fa-check"></i><b>7.7.1</b> Complex activities as graphs</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="SummaryRepresentations.html"><a href="SummaryRepresentations.html"><i class="fa fa-check"></i><b>7.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deeplearning.html"><a href="deeplearning.html"><i class="fa fa-check"></i><b>8</b> Predicting Behavior with Deep Learning</a><ul>
<li class="chapter" data-level="8.1" data-path="ann.html"><a href="ann.html"><i class="fa fa-check"></i><b>8.1</b> Introduction to Artificial Neural Networks</a><ul>
<li class="chapter" data-level="8.1.1" data-path="ann.html"><a href="ann.html#sigmoid-and-relu-units"><i class="fa fa-check"></i><b>8.1.1</b> Sigmoid and ReLU units</a></li>
<li class="chapter" data-level="8.1.2" data-path="ann.html"><a href="ann.html#assembling-units-into-layers"><i class="fa fa-check"></i><b>8.1.2</b> Assembling Units into Layers</a></li>
<li class="chapter" data-level="8.1.3" data-path="ann.html"><a href="ann.html#deep-neural-networks"><i class="fa fa-check"></i><b>8.1.3</b> Deep Neural Networks</a></li>
<li class="chapter" data-level="8.1.4" data-path="ann.html"><a href="ann.html#learning-the-parameters"><i class="fa fa-check"></i><b>8.1.4</b> Learning the Parameters</a></li>
<li class="chapter" data-level="8.1.5" data-path="ann.html"><a href="ann.html#parameter-learning-example-in-r"><i class="fa fa-check"></i><b>8.1.5</b> Parameter Learning Example in R</a></li>
<li class="chapter" data-level="8.1.6" data-path="ann.html"><a href="ann.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>8.1.6</b> Stochastic Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="keras-and-tensorflow-with-r.html"><a href="keras-and-tensorflow-with-r.html"><i class="fa fa-check"></i><b>8.2</b> Keras and TensorFlow with R</a><ul>
<li class="chapter" data-level="8.2.1" data-path="keras-and-tensorflow-with-r.html"><a href="keras-and-tensorflow-with-r.html#keras-example"><i class="fa fa-check"></i><b>8.2.1</b> Keras Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="classification-with-neural-networks.html"><a href="classification-with-neural-networks.html"><i class="fa fa-check"></i><b>8.3</b> Classification with Neural Networks</a><ul>
<li class="chapter" data-level="8.3.1" data-path="classification-with-neural-networks.html"><a href="classification-with-neural-networks.html#classification-of-electromyography-signals"><i class="fa fa-check"></i><b>8.3.1</b> Classification of Electromyography Signals</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>8.4</b> Overfitting</a><ul>
<li class="chapter" data-level="8.4.1" data-path="overfitting.html"><a href="overfitting.html#early-stopping"><i class="fa fa-check"></i><b>8.4.1</b> Early Stopping</a></li>
<li class="chapter" data-level="8.4.2" data-path="overfitting.html"><a href="overfitting.html#dropout"><i class="fa fa-check"></i><b>8.4.2</b> Dropout</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="fine-tuning-a-neural-network.html"><a href="fine-tuning-a-neural-network.html"><i class="fa fa-check"></i><b>8.5</b> Fine-Tuning a Neural Network</a></li>
<li class="chapter" data-level="8.6" data-path="cnns.html"><a href="cnns.html"><i class="fa fa-check"></i><b>8.6</b> Convolutional Neural Networks</a><ul>
<li class="chapter" data-level="8.6.1" data-path="cnns.html"><a href="cnns.html#convolutions"><i class="fa fa-check"></i><b>8.6.1</b> Convolutions</a></li>
<li class="chapter" data-level="8.6.2" data-path="cnns.html"><a href="cnns.html#pooling-operations"><i class="fa fa-check"></i><b>8.6.2</b> Pooling Operations</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="cnns-with-keras.html"><a href="cnns-with-keras.html"><i class="fa fa-check"></i><b>8.7</b> CNNs with Keras</a><ul>
<li class="chapter" data-level="8.7.1" data-path="cnns-with-keras.html"><a href="cnns-with-keras.html#example-1"><i class="fa fa-check"></i><b>8.7.1</b> Example 1</a></li>
<li class="chapter" data-level="8.7.2" data-path="cnns-with-keras.html"><a href="cnns-with-keras.html#example-2"><i class="fa fa-check"></i><b>8.7.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="cnnSmile.html"><a href="cnnSmile.html"><i class="fa fa-check"></i><b>8.8</b> Smiles Detection with a CNN</a></li>
<li class="chapter" data-level="8.9" data-path="SummaryDeepLearning.html"><a href="SummaryDeepLearning.html"><i class="fa fa-check"></i><b>8.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multiuser.html"><a href="multiuser.html"><i class="fa fa-check"></i><b>9</b> Multi-User Validation</a><ul>
<li class="chapter" data-level="9.1" data-path="mixed-models.html"><a href="mixed-models.html"><i class="fa fa-check"></i><b>9.1</b> Mixed Models</a><ul>
<li class="chapter" data-level="9.1.1" data-path="mixed-models.html"><a href="mixed-models.html#skeleton-action-recognition-with-mixed-models"><i class="fa fa-check"></i><b>9.1.1</b> Skeleton Action Recognition with Mixed Models</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="user-independent-models.html"><a href="user-independent-models.html"><i class="fa fa-check"></i><b>9.2</b> User-Independent Models</a></li>
<li class="chapter" data-level="9.3" data-path="user-dependent-models.html"><a href="user-dependent-models.html"><i class="fa fa-check"></i><b>9.3</b> User-Dependent Models</a></li>
<li class="chapter" data-level="9.4" data-path="user-adaptive-models.html"><a href="user-adaptive-models.html"><i class="fa fa-check"></i><b>9.4</b> User-Adaptive Models</a><ul>
<li class="chapter" data-level="9.4.1" data-path="user-adaptive-models.html"><a href="user-adaptive-models.html#transfer-learning"><i class="fa fa-check"></i><b>9.4.1</b> Transfer Learning</a></li>
<li class="chapter" data-level="9.4.2" data-path="user-adaptive-models.html"><a href="user-adaptive-models.html#a-user-adaptive-model-for-activity-recognition"><i class="fa fa-check"></i><b>9.4.2</b> A User-Adaptive Model for Activity Recognition</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="SummaryMultiUser.html"><a href="SummaryMultiUser.html"><i class="fa fa-check"></i><b>9.5</b> Summary</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendixInstall.html"><a href="appendixInstall.html"><i class="fa fa-check"></i><b>A</b> Setup Your Environment</a><ul>
<li class="chapter" data-level="A.1" data-path="installing-the-datasets.html"><a href="installing-the-datasets.html"><i class="fa fa-check"></i><b>A.1</b> Installing the Datasets</a></li>
<li class="chapter" data-level="A.2" data-path="installing-the-examples-source-code.html"><a href="installing-the-examples-source-code.html"><i class="fa fa-check"></i><b>A.2</b> Installing the Examples Source Code</a></li>
<li class="chapter" data-level="A.3" data-path="installing-keras-and-tensorflow-.html"><a href="installing-keras-and-tensorflow-.html"><i class="fa fa-check"></i><b>A.3</b> Installing Keras and TensorFlow.</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendixDatasets.html"><a href="appendixDatasets.html"><i class="fa fa-check"></i><b>B</b> Datasets</a><ul>
<li class="chapter" data-level="B.1" data-path="complex-activities.html"><a href="complex-activities.html"><i class="fa fa-check"></i><b>B.1</b> COMPLEX ACTIVITIES</a></li>
<li class="chapter" data-level="B.2" data-path="depresjon.html"><a href="depresjon.html"><i class="fa fa-check"></i><b>B.2</b> DEPRESJON</a></li>
<li class="chapter" data-level="B.3" data-path="electromyography.html"><a href="electromyography.html"><i class="fa fa-check"></i><b>B.3</b> ELECTROMYOGRAPHY</a></li>
<li class="chapter" data-level="B.4" data-path="hand-gestures.html"><a href="hand-gestures.html"><i class="fa fa-check"></i><b>B.4</b> HAND GESTURES</a></li>
<li class="chapter" data-level="B.5" data-path="home-tasks.html"><a href="home-tasks.html"><i class="fa fa-check"></i><b>B.5</b> HOME TASKS</a></li>
<li class="chapter" data-level="B.6" data-path="homicide-reports.html"><a href="homicide-reports.html"><i class="fa fa-check"></i><b>B.6</b> HOMICIDE REPORTS</a></li>
<li class="chapter" data-level="B.7" data-path="indoor-location.html"><a href="indoor-location.html"><i class="fa fa-check"></i><b>B.7</b> INDOOR LOCATION</a></li>
<li class="chapter" data-level="B.8" data-path="sheep-goats.html"><a href="sheep-goats.html"><i class="fa fa-check"></i><b>B.8</b> SHEEP GOATS</a></li>
<li class="chapter" data-level="B.9" data-path="skeleton-actions.html"><a href="skeleton-actions.html"><i class="fa fa-check"></i><b>B.9</b> SKELETON ACTIONS</a></li>
<li class="chapter" data-level="B.10" data-path="smartphone-activities.html"><a href="smartphone-activities.html"><i class="fa fa-check"></i><b>B.10</b> SMARTPHONE ACTIVITIES</a></li>
<li class="chapter" data-level="B.11" data-path="smiles.html"><a href="smiles.html"><i class="fa fa-check"></i><b>B.11</b> SMILES</a></li>
<li class="chapter" data-level="B.12" data-path="students-mental-health.html"><a href="students-mental-health.html"><i class="fa fa-check"></i><b>B.12</b> STUDENTS’ MENTAL HEALTH</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="citing-this-book.html"><a href="citing-this-book.html"><i class="fa fa-check"></i>Citing this Book</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Behavior Analysis with Machine Learning and R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="decision-trees" class="section level2">
<h2><span class="header-section-number">2.3</span> Decision Trees</h2>
<p>Decision trees are powerful predictive models (especially when combining several, see chapter <a href="ensemble.html#ensemble">3</a>) used for classification and regression tasks. Here, the focus will be on classification. Each node in a tree represents partial or final decisions based on a single feature. If a node is a leaf, then it leads to a final decision. A leaf is simply a terminal node, i.e, it has no children nodes. Given a feature vector representing an instance, the predicted class is obtained by testing the feature values and following the tree path until a leaf is reached. Figure <a href="decision-trees.html#fig:treeExample">2.4</a> shows an example decision tree and a query instance with an unknown class. To get the final class, features are evaluated starting at the root. In this case <em>number_wheels</em> is <span class="math inline">\(4\)</span> in the query instance so we take the left path from the root. Now, we need to evaluate <em>weight</em>. This time the test is false since the weight is <span class="math inline">\(2300\)</span> and we take the right path. Since this is a leaf node the final predicted class is <em>‘truck’</em>. Usually, small trees are preferred (small depth) because they are easier to visualize and interpret and are less prone to overfitting. The example tree has a depth of 2. Should the number of wheels had been <span class="math inline">\(2\)</span> instead of <span class="math inline">\(4\)</span>, then testing the <em>weight</em> feature would not have been necessary.</p>
<div class="figure" style="text-align: center"><span id="fig:treeExample"></span>
<img src="images/treeExample.png" alt="Example decision tree. The query instance is classified as truck by this tree." width="90%" />
<p class="caption">
Figure 2.4: Example decision tree. The query instance is classified as truck by this tree.
</p>
</div>
<p>As shown in the example, decision trees are easy to interpret and the explanation of a final result can be obtained by just following the path. Now let’s see how these decision trees are learned from data. Consider the following artificial <em>cinema</em> dataset (Figure <a href="decision-trees.html#fig:cinemaTable">2.5</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:cinemaTable"></span>
<img src="images/cinemaTable.png" alt="Cinema dataset." width="50%" />
<p class="caption">
Figure 2.5: Cinema dataset.
</p>
</div>
<p>The first <span class="math inline">\(4\)</span> variables are features and the last column is the class. The class is the decision whether or not we should go to the movies based on the other variables. In this case, all variables are binary except <em>Price</em> which has three possible values: <em>low</em>, <em>medium</em>, and <em>high</em>.</p>
<ul>
<li><em>Tired:</em> Indicates if the person is tired or not.</li>
<li><em>Rain:</em> Whether it is raining or not.</li>
<li><em>Comedy:</em> Indicates if the genre of the movie is <em>comedy</em>.</li>
<li><em>Price:</em> Ticket price.</li>
<li><em>Go:</em> The decision of whether to go to the movies or not.</li>
</ul>
<p>The main question when building a tree is which feature should be at the root (top). Once you answer this question, you may need to grow the tree by adding another feature (node) as one of the root’s children. To decide which new feature to add you need to answer the same first question: “What feature should be at the root of this subtree?”. This is a recursive definition! The tree keeps growing until you reach a leaf node, there are no more features to select from, or you have reached a predefined maximum depth.</p>
<p>For the <em>cinema</em> dataset we need to find which is the best variable to be placed at the root. Let’s suppose we need to choose between <em>Price</em> and <em>Comedy</em>. Figure <a href="decision-trees.html#fig:treeAlgo1">2.6</a> shows these two possibilities.</p>
<div class="figure" style="text-align: center"><span id="fig:treeAlgo1"></span>
<img src="images/treeAlgo1.png" alt="Two example trees with one variable split by Price (left) and Comedy (right)." width="100%" />
<p class="caption">
Figure 2.6: Two example trees with one variable split by Price (left) and Comedy (right).
</p>
</div>
<p>If we select <em>Price</em>, there are three possible subnodes, one for each value: <em>low</em>, <em>medium</em>, and <em>high</em>. If <em>Price</em> is <em>low</em> then four instances fall into this subtree (the first four from the table). For all of them, the value of <em>Go</em> is <span class="math inline">\(1\)</span>. If <em>Price</em> is <em>high</em>, two instances fall into this category and their <em>Go</em> value is <span class="math inline">\(0\)</span>, thus if the price is high then you should not go to the movies according to this data. There are six instances for which the <em>Price</em> value is <em>medium</em>. From those, two of them have <em>Go=1</em> and the remaining four have <em>Go=0</em>. For cases when the price is <em>low</em> or <em>high</em> we can arrive at a solution. If the price is <em>low</em> then go to the cinema, if the price is <em>high</em> then do not go. However, if the price is <em>medium</em> it is still not clear what to do since this subnode is not <em>pure</em>. That is, the types of the instances are mixed: two with an output of <span class="math inline">\(1\)</span> and four with an output of <span class="math inline">\(0\)</span>. In this case we can try to use another feature to decide and grow the tree but first, let’s look at what happens if we decide to use <em>Comedy</em> as the first feature at the root. In this case, we end up with two subsets with six instances each. And for each subnode, what decision should we take is still not clear because the output is ‘mixed’ (Go: 3, NotGo: 3). At this point we would need to continue growing the tree below each subnode.</p>
<p>Intuitively, it seems like <em>Price</em> is a better feature since its subnodes are more <em>pure</em>. Then we can use another feature to split the instances whose <em>Price</em> is <em>medium</em>. For example, using the <em>Comedy</em> variable. Figure <a href="decision-trees.html#fig:treeAlgo2">2.7</a> shows how this would look like. Since one of the subnodes of <em>Comedy</em> is still not pure we can further split it using the <em>Rain</em> variable, for example. At this point, we can not split any further. Note that the <em>Tired</em> variable was never used.</p>
<div class="figure" style="text-align: center"><span id="fig:treeAlgo2"></span>
<img src="images/treeAlgo2.png" alt="Tree splitting example. Left: tree splits. Right: Highlighted instances when splitting Comedy." width="100%" />
<p class="caption">
Figure 2.7: Tree splitting example. Left: tree splits. Right: Highlighted instances when splitting Comedy.
</p>
</div>
<p>So far, we have chosen the root variable based on which one looks more pure but to automate the process, we need a way to measure this <em>purity</em> in a quantitative manner. One way to do that is by using the <em>entropy</em>. <em>Entropy</em> is a measure of uncertainty from information theory. It is zero when there is no uncertainty and one when there is complete uncertainty. The entropy of a discrete variable <span class="math inline">\(X\)</span> with values <span class="math inline">\(x_1\dots x_n\)</span> and probability mass function <span class="math inline">\(P(X)\)</span> is:</p>
<p><span class="math display" id="eq:entropy">\[\begin{equation}
  H(X) = -\sum_{i=1}^n{P(x_i)log P(x_i)}
  \tag{2.8}
\end{equation}\]</span></p>
<p>Take for example a fair coin with probability of heads and tails = <span class="math inline">\(0.5\)</span> each. The entropy for that coin is:</p>
<p><span class="math display">\[\begin{equation*}
  H(X) = - (0.5)log(0.5) + (0.5)log(0.5) = 1
\end{equation*}\]</span></p>
<p>Since we do not know what will be the result when we drop the coin, the entropy is maximum. Now consider the extreme case when the coin is biased such that the probability of heads is <span class="math inline">\(1\)</span> and the probability of tails is <span class="math inline">\(0\)</span>. The entropy in this case is zero:</p>
<p><span class="math display">\[\begin{equation*}
  H(X) = - (1)log(1) + (0)log(0) = 0
\end{equation*}\]</span></p>
<p>If we know that the result is always going to be heads, then there is no uncertainty when the coin is dropped. The entropy of <span class="math inline">\(p\)</span> positive examples and <span class="math inline">\(n\)</span> negative examples is:</p>
<p><span class="math display" id="eq:binaryentropy">\[\begin{equation}
  H(p, n) = - (\frac{p}{p+n})log(\frac{p}{p+n}) + (\frac{n}{p+n})log(\frac{n}{p+n})
  \tag{2.9}
\end{equation}\]</span></p>
<p>Thus, the entropies for the three possible values of <em>Price</em> are:</p>
<p><span class="math display">\[\begin{equation*}
  H_{price=low}(4, 0) = - (\frac{4}{4+0})log(\frac{4}{4+0}) + (\frac{0}{4+0})log(\frac{0}{4+0}) = 0
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
  H_{price=medium}(2, 4) = - (\frac{2}{2+4})log(\frac{2}{2+4}) + (\frac{4}{2+4})log(\frac{4}{2+4}) = 0.918
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
  H_{price=high}(0, 2) = - (\frac{0}{0+2})log(\frac{0}{0+2}) + (\frac{2}{0+2})log(\frac{2}{0+2}) = 0
\end{equation*}\]</span></p>
<p>The average of those three can be calculated by taking into account the number of corresponding instances for each value and the total number of instances (<span class="math inline">\(12\)</span>):</p>
<p><span class="math display">\[\begin{equation*}
  meanH(price) = (4/12)(0) + (6/12)(0.918) + (2/12)(0) = 0.459
\end{equation*}\]</span></p>
<p>Before deciding to split on <em>Price</em> the entropy of the entire dataset is <span class="math inline">\(1\)</span> since there are six positive and negative examples:</p>
<p><span class="math display">\[\begin{equation*}
  H(6,6) = 1
\end{equation*}\]</span></p>
<p>thus, the information gain for <em>Price</em> is:</p>
<p><span class="math display">\[\begin{equation*}
  infoGain(Price) = 1 - meanH(Price) = 1 - 0.459 = 0.541
\end{equation*}\]</span></p>
<p>For the rest of the variables the information gain is:</p>
<p><span class="math inline">\(infoGain(Tired) = 0\)</span></p>
<p><span class="math inline">\(infoGain(Rain) = 0.020\)</span></p>
<p><span class="math inline">\(infoGain(Comedy) = 0\)</span></p>
<p>The highest information gain is produced by <em>Price</em>, thus, it is selected as the root node. Then, the process continues recursively for each branch but excluding <em>Price</em>. Since branches with values <em>low</em> and <em>high</em> are already done, we only need to further split <em>medium</em>. Sometimes it is not possible to have completely pure nodes like <em>low</em> and <em>high</em>. This can happen for example, when there are no more attributes left or when two or more instances have the same feature values but different labels. In those situations the final prediction is the most common label (majority vote).</p>
<p>There exist many implementations of decision trees. Some implementations compute variable importance using the entropy (as shown here) but others use the Gini index, for example. Each implementation also treats numeric variables in different ways. Pruning the tree using different techniques is also common in order to reduce its size.</p>
<p>Some of the most common implementations are C4.5 trees <span class="citation">(Quinlan <a href="#ref-quinlan2014" role="doc-biblioref">2014</a>)</span> and CART <span class="citation">(Steinberg and Colla <a href="#ref-steinberg2009" role="doc-biblioref">2009</a>)</span>. The later is implemented in the <code>rpart</code> R package <span class="citation">(Therneau and Atkinson <a href="#ref-rpart" role="doc-biblioref">2019</a>)</span> which will be used in the following section to build a model that predicts physical activities from smartphones sensor data.</p>
<div id="activityRecognition" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Activity Recognition with Smartphones</h3>

<div class="rmdfolder">
<code>smartphone_activities.R</code>
</div>

<p>As mentioned in the introduction, behavior can be an observable activity in a human. We can easily infer what <strong>physical activity</strong> someone is doing by looking at her/his body movements. Observing physical activities can provide useful behavioral and contextual information about someone. This can also be used as a proxy to, for example, infer someone’s health condition by detecting deviations in activity patterns.</p>
<p>Nowadays, most smartphones come with a tri-axial accelerometer sensor. This sensor measures gravitational forces from the <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, and <span class="math inline">\(z\)</span> axes. This information can be used to capture movement patterns from the user and automate the process of monitoring the type of physical activity being performed.</p>
<p>In this section, we will use decision trees to automatically classify physical activities from acceleration data. We will use the <em>SMARTPHONE ACTIVITIES</em> dataset that contains acceleration recordings that were collected with a smartphone. This dataset is called WISDM<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> and was made available by <span class="citation">Kwapisz, Weiss, and Moore (<a href="#ref-kwapisz2010" role="doc-biblioref">2010</a>)</span>. The dataset has <span class="math inline">\(6\)</span> different activities: <em>‘walking’</em>, <em>‘jogging’</em>, <em>‘walking upstairs’</em>, <em>‘walking downstairs’</em>, <em>‘sitting’</em> and <em>‘standing’</em>. The data was collected by <span class="math inline">\(36\)</span> volunteers with an Android phone located in their pant’s pocket and with a sampling rate of <span class="math inline">\(20Hz\)</span> (<span class="math inline">\(1\)</span> sample every <span class="math inline">\(50\)</span> milliseconds).</p>
<p>The dataset contains two types of files. One with the raw accelerometer data and the other one after feature extraction. Figure <a href="decision-trees.html#fig:wisdmFirstLines">2.8</a> shows the first <span class="math inline">\(10\)</span> lines of the raw accelerometer values of the first file. The first column is the id of the user that collected the data and the second column is the class. The third column is the timestamp and the remaining columns are the <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, and <span class="math inline">\(z\)</span> accelerometer values, respectively.</p>
<div class="figure" style="text-align: center"><span id="fig:wisdmFirstLines"></span>
<img src="images/wisdmFirstLines.png" alt="First 10 lines of raw accelerometer data." width="50%" />
<p class="caption">
Figure 2.8: First 10 lines of raw accelerometer data.
</p>
</div>
<p>Usually, classification models are not trained with the raw data but with <em>feature vectors</em> extracted from the raw data. Feature vectors have the advantage of being more compact, thus, making the learning phase more efficient. For activity recognition, the feature extraction process consists of defining a moving window of size <span class="math inline">\(w\)</span> that starts at position <span class="math inline">\(i\)</span>. At the beginning, <span class="math inline">\(i\)</span> is the index pointing to the first accelerometer readings. Then, <span class="math inline">\(n\)</span> statistical features are computed on the elements covered by the window such as mean, standard deviation, <span class="math inline">\(0\)</span>-crossings, etc. This will produce a <span class="math inline">\(n\)</span>-dimensional feature vector and the process is repeated by moving the window <span class="math inline">\(s\)</span> steps forward. Typical values of <span class="math inline">\(s\)</span> are such that the overlap between the previous window position and the next one is about <span class="math inline">\(30\%\)</span> to <span class="math inline">\(50\%\)</span>. An overlap of <span class="math inline">\(0\)</span> is also typical, that is, <span class="math inline">\(s = w\)</span>. Figure <a href="decision-trees.html#fig:featureExtraction">2.9</a> depicts the process.</p>
<div class="figure" style="text-align: center"><span id="fig:featureExtraction"></span>
<img src="images/featureExtraction.png" alt="Moving window for feature extraction." width="70%" />
<p class="caption">
Figure 2.9: Moving window for feature extraction.
</p>
</div>
<p>Once we have the set of feature vectors and their associated class labels, we can use them to train a classifier and make predictions on new data.</p>
<p><img src="images/activitiesPrediction.png" width="90%" style="display: block; margin: auto;" /></p>
<p>For this example, we will use the file with features already extracted. The authors used windows of <span class="math inline">\(10\)</span> seconds which is equivalent to <span class="math inline">\(200\)</span> observations given the <span class="math inline">\(20Hz\)</span> sampling rate and they used <span class="math inline">\(0\%\)</span> overlap. From each window, they extracted <span class="math inline">\(43\)</span> features such as the mean, standard deviation, absolute deviations, etc.</p>
<p>Let’s read and print the first rows of the dataset. The script for this section is <code>smartphone_activities.R</code>. The data frame has several columns, but we only print the first five features and the class which is stored in the last column.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="decision-trees.html#cb30-1"></a><span class="co"># Read data.</span></span>
<span id="cb30-2"><a href="decision-trees.html#cb30-2"></a>df &lt;-<span class="st"> </span><span class="kw">read.csv</span>(datapath,<span class="dt">stringsAsFactors =</span> F)</span>
<span id="cb30-3"><a href="decision-trees.html#cb30-3"></a></span>
<span id="cb30-4"><a href="decision-trees.html#cb30-4"></a><span class="co"># Some code to clean the dataset.</span></span>
<span id="cb30-5"><a href="decision-trees.html#cb30-5"></a><span class="co"># (cleaning code not shown here).</span></span>
<span id="cb30-6"><a href="decision-trees.html#cb30-6"></a></span>
<span id="cb30-7"><a href="decision-trees.html#cb30-7"></a><span class="co"># Print first rows of the dataset.</span></span>
<span id="cb30-8"><a href="decision-trees.html#cb30-8"></a><span class="kw">head</span>(df[,<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,<span class="dv">40</span>)])</span></code></pre></div>
<table>
<caption><span id="tab:activitiesTable">Table 2.2: </span>First rows of activities dataset.</caption>
<thead>
<tr class="header">
<th align="right">X0</th>
<th align="right">X1</th>
<th align="right">X2</th>
<th align="right">X3</th>
<th align="right">X4</th>
<th align="left">class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.04</td>
<td align="right">0.09</td>
<td align="right">0.14</td>
<td align="right">0.12</td>
<td align="right">0.11</td>
<td align="left">Jogging</td>
</tr>
<tr class="even">
<td align="right">0.12</td>
<td align="right">0.12</td>
<td align="right">0.06</td>
<td align="right">0.07</td>
<td align="right">0.11</td>
<td align="left">Jogging</td>
</tr>
<tr class="odd">
<td align="right">0.14</td>
<td align="right">0.09</td>
<td align="right">0.11</td>
<td align="right">0.09</td>
<td align="right">0.09</td>
<td align="left">Jogging</td>
</tr>
<tr class="even">
<td align="right">0.06</td>
<td align="right">0.10</td>
<td align="right">0.09</td>
<td align="right">0.09</td>
<td align="right">0.11</td>
<td align="left">Walking</td>
</tr>
<tr class="odd">
<td align="right">0.12</td>
<td align="right">0.11</td>
<td align="right">0.10</td>
<td align="right">0.08</td>
<td align="right">0.10</td>
<td align="left">Walking</td>
</tr>
<tr class="even">
<td align="right">0.09</td>
<td align="right">0.09</td>
<td align="right">0.10</td>
<td align="right">0.12</td>
<td align="right">0.08</td>
<td align="left">Walking</td>
</tr>
<tr class="odd">
<td align="right">0.12</td>
<td align="right">0.12</td>
<td align="right">0.12</td>
<td align="right">0.13</td>
<td align="right">0.15</td>
<td align="left">Upstairs</td>
</tr>
<tr class="even">
<td align="right">0.10</td>
<td align="right">0.10</td>
<td align="right">0.10</td>
<td align="right">0.10</td>
<td align="right">0.11</td>
<td align="left">Upstairs</td>
</tr>
<tr class="odd">
<td align="right">0.08</td>
<td align="right">0.07</td>
<td align="right">0.08</td>
<td align="right">0.08</td>
<td align="right">0.05</td>
<td align="left">Upstairs</td>
</tr>
</tbody>
</table>
<p>Our aim is to predict the class based on all the numeric features. We will use the <code>rpart</code> package <span class="citation">(Therneau and Atkinson <a href="#ref-rpart" role="doc-biblioref">2019</a>)</span> which implements classification and regression trees. We will assess the performance of the decision tree with <span class="math inline">\(10\)</span>-fold cross-validation. We can use the <code>sample()</code> function to generate the folds. This function will sample <span class="math inline">\(n\)</span> integers from <span class="math inline">\(1\)</span> to <span class="math inline">\(k\)</span> where <span class="math inline">\(n\)</span> is the number of rows in the data frame.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="decision-trees.html#cb31-1"></a><span class="co"># Package with implementations of decision trees.</span></span>
<span id="cb31-2"><a href="decision-trees.html#cb31-2"></a><span class="kw">library</span>(rpart)</span>
<span id="cb31-3"><a href="decision-trees.html#cb31-3"></a></span>
<span id="cb31-4"><a href="decision-trees.html#cb31-4"></a><span class="co"># Set seed for reproducibility.</span></span>
<span id="cb31-5"><a href="decision-trees.html#cb31-5"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb31-6"><a href="decision-trees.html#cb31-6"></a></span>
<span id="cb31-7"><a href="decision-trees.html#cb31-7"></a><span class="co"># Define the number of folds.</span></span>
<span id="cb31-8"><a href="decision-trees.html#cb31-8"></a>k &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb31-9"><a href="decision-trees.html#cb31-9"></a></span>
<span id="cb31-10"><a href="decision-trees.html#cb31-10"></a><span class="co"># Generate folds.</span></span>
<span id="cb31-11"><a href="decision-trees.html#cb31-11"></a>folds &lt;-<span class="st"> </span><span class="kw">sample</span>(k, <span class="dt">size =</span> <span class="kw">nrow</span>(df), <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span>
<span id="cb31-12"><a href="decision-trees.html#cb31-12"></a></span>
<span id="cb31-13"><a href="decision-trees.html#cb31-13"></a><span class="co"># Print first 10 values.</span></span>
<span id="cb31-14"><a href="decision-trees.html#cb31-14"></a><span class="kw">head</span>(folds)</span>
<span id="cb31-15"><a href="decision-trees.html#cb31-15"></a><span class="co">#&gt; [1] 10  6  5  9  5  6</span></span></code></pre></div>
<p>The <code>folds</code> variable stores the fold each instance belongs to. For example, the first instance belongs to fold <span class="math inline">\(1\)</span>, the second instance belongs to fold <span class="math inline">\(6\)</span>, and so on. We can now generate our test and train sets. We will iterate <span class="math inline">\(k=10\)</span> times. For each iteration <span class="math inline">\(i\)</span>, the test set is built using the instances that belong to fold <span class="math inline">\(i\)</span> and the train set will be composed of the remaining instances (those that do not belong to fold <span class="math inline">\(i\)</span>). Next, the <code>rpart()</code> function is used to train the decision tree with the train set. By default, <code>rpart()</code> performs <span class="math inline">\(10\)</span>-fold cross-validation internally. To avoid this, we set the parameter <code>xval = 0</code>. Then, we can use the trained model to obtain the predictions on the test set with the generic <code>predict()</code> function. The ground truth classes and the predictions are stored so the performance metrics can be computed.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="decision-trees.html#cb32-1"></a><span class="co"># Variable to store ground truth classes.</span></span>
<span id="cb32-2"><a href="decision-trees.html#cb32-2"></a>groundTruth &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb32-3"><a href="decision-trees.html#cb32-3"></a></span>
<span id="cb32-4"><a href="decision-trees.html#cb32-4"></a><span class="co"># Variable to store the classifier&#39;s predictions.</span></span>
<span id="cb32-5"><a href="decision-trees.html#cb32-5"></a>predictions &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb32-6"><a href="decision-trees.html#cb32-6"></a></span>
<span id="cb32-7"><a href="decision-trees.html#cb32-7"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k){</span>
<span id="cb32-8"><a href="decision-trees.html#cb32-8"></a>  </span>
<span id="cb32-9"><a href="decision-trees.html#cb32-9"></a>  trainSet &lt;-<span class="st"> </span>df[<span class="kw">which</span>(folds <span class="op">!=</span><span class="st"> </span>i), ]</span>
<span id="cb32-10"><a href="decision-trees.html#cb32-10"></a>  testSet &lt;-<span class="st"> </span>df[<span class="kw">which</span>(folds <span class="op">==</span><span class="st"> </span>i), ]</span>
<span id="cb32-11"><a href="decision-trees.html#cb32-11"></a>  </span>
<span id="cb32-12"><a href="decision-trees.html#cb32-12"></a>  <span class="co"># Train the decision tree</span></span>
<span id="cb32-13"><a href="decision-trees.html#cb32-13"></a>  treeClassifier &lt;-<span class="st"> </span><span class="kw">rpart</span>(class <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb32-14"><a href="decision-trees.html#cb32-14"></a>                          trainSet, <span class="dt">xval=</span><span class="dv">0</span>)</span>
<span id="cb32-15"><a href="decision-trees.html#cb32-15"></a>  </span>
<span id="cb32-16"><a href="decision-trees.html#cb32-16"></a>  <span class="co"># Get predictions on the test set.</span></span>
<span id="cb32-17"><a href="decision-trees.html#cb32-17"></a>  foldPredictions &lt;-<span class="st"> </span><span class="kw">predict</span>(treeClassifier,</span>
<span id="cb32-18"><a href="decision-trees.html#cb32-18"></a>                             testSet, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb32-19"><a href="decision-trees.html#cb32-19"></a>  </span>
<span id="cb32-20"><a href="decision-trees.html#cb32-20"></a>  predictions &lt;-<span class="st"> </span><span class="kw">c</span>(predictions,</span>
<span id="cb32-21"><a href="decision-trees.html#cb32-21"></a>                   <span class="kw">as.character</span>(foldPredictions))</span>
<span id="cb32-22"><a href="decision-trees.html#cb32-22"></a>  </span>
<span id="cb32-23"><a href="decision-trees.html#cb32-23"></a>  groundTruth &lt;-<span class="st"> </span><span class="kw">c</span>(groundTruth,</span>
<span id="cb32-24"><a href="decision-trees.html#cb32-24"></a>                   <span class="kw">as.character</span>(testSet<span class="op">$</span>class))</span>
<span id="cb32-25"><a href="decision-trees.html#cb32-25"></a>}</span></code></pre></div>
<p>Now, we use the <code>confusionMatrix()</code> function to compute the performance metrics and the confusion matrix.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="decision-trees.html#cb33-1"></a>cm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="kw">as.factor</span>(predictions),</span>
<span id="cb33-2"><a href="decision-trees.html#cb33-2"></a>                      <span class="kw">as.factor</span>(groundTruth))</span>
<span id="cb33-3"><a href="decision-trees.html#cb33-3"></a></span>
<span id="cb33-4"><a href="decision-trees.html#cb33-4"></a><span class="co"># Print accuracy</span></span>
<span id="cb33-5"><a href="decision-trees.html#cb33-5"></a>cm<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span>
<span id="cb33-6"><a href="decision-trees.html#cb33-6"></a><span class="co">#&gt; Accuracy </span></span>
<span id="cb33-7"><a href="decision-trees.html#cb33-7"></a><span class="co">#&gt; 0.7895903 </span></span>
<span id="cb33-8"><a href="decision-trees.html#cb33-8"></a></span>
<span id="cb33-9"><a href="decision-trees.html#cb33-9"></a><span class="co"># Print performance metrics per class.</span></span>
<span id="cb33-10"><a href="decision-trees.html#cb33-10"></a>cm<span class="op">$</span>byClass[,<span class="kw">c</span>(<span class="st">&quot;Recall&quot;</span>, <span class="st">&quot;Specificity&quot;</span>, <span class="st">&quot;Precision&quot;</span>, <span class="st">&quot;F1&quot;</span>)]</span>
<span id="cb33-11"><a href="decision-trees.html#cb33-11"></a><span class="co">#&gt;                      Recall Specificity Precision        F1</span></span>
<span id="cb33-12"><a href="decision-trees.html#cb33-12"></a><span class="co">#&gt; Class: Downstairs 0.2821970   0.9617587 0.4434524 0.3449074</span></span>
<span id="cb33-13"><a href="decision-trees.html#cb33-13"></a><span class="co">#&gt; Class: Jogging    0.9612308   0.9601898 0.9118506 0.9358898</span></span>
<span id="cb33-14"><a href="decision-trees.html#cb33-14"></a><span class="co">#&gt; Class: Sitting    0.8366013   0.9984351 0.9696970 0.8982456</span></span>
<span id="cb33-15"><a href="decision-trees.html#cb33-15"></a><span class="co">#&gt; Class: Standing   0.8983740   0.9932328 0.8632812 0.8804781</span></span>
<span id="cb33-16"><a href="decision-trees.html#cb33-16"></a><span class="co">#&gt; Class: Upstairs   0.2246835   0.9669870 0.4733333 0.3047210</span></span>
<span id="cb33-17"><a href="decision-trees.html#cb33-17"></a><span class="co">#&gt; Class: Walking    0.9360884   0.8198981 0.7642213 0.8414687</span></span>
<span id="cb33-18"><a href="decision-trees.html#cb33-18"></a></span>
<span id="cb33-19"><a href="decision-trees.html#cb33-19"></a><span class="co"># Print overall metrics across classes.</span></span>
<span id="cb33-20"><a href="decision-trees.html#cb33-20"></a><span class="kw">colMeans</span>(cm<span class="op">$</span>byClass[,<span class="kw">c</span>(<span class="st">&quot;Recall&quot;</span>, <span class="st">&quot;Specificity&quot;</span>,</span>
<span id="cb33-21"><a href="decision-trees.html#cb33-21"></a>                       <span class="st">&quot;Precision&quot;</span>, <span class="st">&quot;F1&quot;</span>)])</span>
<span id="cb33-22"><a href="decision-trees.html#cb33-22"></a><span class="co">#&gt;     Recall Specificity   Precision          F1 </span></span>
<span id="cb33-23"><a href="decision-trees.html#cb33-23"></a><span class="co">#&gt;  0.6898625   0.9500836   0.7376393   0.7009518 </span></span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:activitiesTreeCM"></span>
<img src="images/activitiesTreeCM.png" alt="Confusion matrix for activities' predictions." width="70%" />
<p class="caption">
Figure 2.10: Confusion matrix for activities’ predictions.
</p>
</div>
<p>The overall accuracy was <span class="math inline">\(78\%\)</span> and by looking at the individual performance metrics, some classes had low scores like <em>‘walking downstairs’</em> and <em>‘walking upstairs’</em>. From the confusion matrix, it can be seen that those two activities were often confused with each other but also with the <em>‘walking’</em> activity. Package <code>rpart.plot</code> <span class="citation">(Milborrow <a href="#ref-rpartplot" role="doc-biblioref">2019</a>)</span> can be used to plot the resulting tree.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="decision-trees.html#cb34-1"></a><span class="kw">library</span>(rpart.plot)</span>
<span id="cb34-2"><a href="decision-trees.html#cb34-2"></a><span class="co"># Plot the tree from the last fold.</span></span>
<span id="cb34-3"><a href="decision-trees.html#cb34-3"></a><span class="kw">rpart.plot</span>(treeClassifier, <span class="dt">fallen.leaves =</span> F,</span>
<span id="cb34-4"><a href="decision-trees.html#cb34-4"></a>           <span class="dt">shadow.col =</span> <span class="st">&quot;gray&quot;</span>, <span class="dt">legend.y =</span> <span class="dv">1</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:activitiesTree"></span>
<img src="images/activitiesTree.png" alt="Resulting decision tree." width="100%" />
<p class="caption">
Figure 2.11: Resulting decision tree.
</p>
</div>
<p>The <code>fallen.leaves = F</code> argument prevents the leaves to be plotted at the bottom. This is useful if the tree has many nodes. Each node shows the predicted class, the predicted probability of each class and the percentage of observations in the node. The plot also shows the feature used for each split. We can see that the <em>YABSOLDEV</em> variable is at the root thus, it had the highest information gain with the initial set of instances. At the root of the tree, before looking at any of the features, the predicted class is <em>‘Walking’</em>. This is because its prior probability is the highest one (<span class="math inline">\(\approx 0.39\)</span>), that is, it’s the most common activity present in the dataset. So, if we didn’t have any other information, our best bet would be to predict the most frequent activity.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="decision-trees.html#cb35-1"></a><span class="co"># Prior probabilities.</span></span>
<span id="cb35-2"><a href="decision-trees.html#cb35-2"></a><span class="kw">table</span>(trainSet<span class="op">$</span>class) <span class="op">/</span><span class="st"> </span><span class="kw">nrow</span>(trainSet)</span>
<span id="cb35-3"><a href="decision-trees.html#cb35-3"></a><span class="co">#&gt; Downstairs    Jogging    Sitting   Standing   Upstairs    Walking </span></span>
<span id="cb35-4"><a href="decision-trees.html#cb35-4"></a><span class="co">#&gt; 0.09882885 0.29607561 0.05506472 0.04705157 0.11793713 0.38504212</span></span></code></pre></div>
<p>These results look promising, but they can still be improved. In the next chapter, I will show you how to improve these results with <em>Ensemble Learning</em> which is a method that is used to aggregate many models.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-kwapisz2010">
<p>Kwapisz, Jennifer R., Gary M. Weiss, and Samuel A. Moore. 2010. “Activity Recognition Using Cell Phone Accelerometers.” In <em>Proceedings of the Fourth International Workshop on Knowledge Discovery from Sensor Data (at KDD-10), Washington DC.</em></p>
</div>
<div id="ref-rpartplot">
<p>Milborrow, Stephen. 2019. <em>Rpart.plot: Plot ’Rpart’ Models: An Enhanced Version of ’plot.rpart’</em>. <a href="https://CRAN.R-project.org/package=rpart.plot">https://CRAN.R-project.org/package=rpart.plot</a>.</p>
</div>
<div id="ref-quinlan2014">
<p>Quinlan, J Ross. 2014. <em>C4.5: Programs for Machine Learning</em>. Elsevier.</p>
</div>
<div id="ref-steinberg2009">
<p>Steinberg, Dan, and Phillip Colla. 2009. “CART: Classification and Regression Trees.” <em>The Top Ten Algorithms in Data Mining</em> 9: 179.</p>
</div>
<div id="ref-rpart">
<p>Therneau, Terry, and Beth Atkinson. 2019. <em>Rpart: Recursive Partitioning and Regression Trees</em>. <a href="https://CRAN.R-project.org/package=rpart">https://CRAN.R-project.org/package=rpart</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p><a href="http://www.cis.fordham.edu/wisdm/dataset.php" class="uri">http://www.cis.fordham.edu/wisdm/dataset.php</a><a href="decision-trees.html#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="performance-metrics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="dynamic-time-warping.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
