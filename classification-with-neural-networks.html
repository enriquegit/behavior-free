<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8.3 Classification with Neural Networks | Behavior Analysis with Machine Learning and R</title>
  <meta name="description" content="8.3 Classification with Neural Networks | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="8.3 Classification with Neural Networks | Behavior Analysis with Machine Learning and R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="8.3 Classification with Neural Networks | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8.3 Classification with Neural Networks | Behavior Analysis with Machine Learning and R" />
  
  <meta name="twitter:description" content="8.3 Classification with Neural Networks | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  <meta name="twitter:image" content="images/cover.png" />

<meta name="author" content="Enrique Garcia Ceja" />


<meta name="date" content="2020-09-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="keras-and-tensorflow-with-r.html"/>
<link rel="next" href="overfitting.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/d3-4.9.0/d3.min.js"></script>
<script src="libs/d3-tip-0.7.1/index-min.js"></script>
<link href="libs/d3panels-1.4.9/d3panels.min.css" rel="stylesheet" />
<script src="libs/d3panels-1.4.9/d3panels.min.js"></script>
<script src="libs/qtlcharts_iplotCorr-0.11.6/iplotCorr.js"></script>
<script src="libs/qtlcharts_iplotCorr-0.11.6/iplotCorr_noscat.js"></script>
<script src="libs/iplotCorr-binding-0.11.6/iplotCorr.js"></script>
<link href="libs/dygraphs-1.1.1/dygraph.css" rel="stylesheet" />
<script src="libs/dygraphs-1.1.1/dygraph-combined.js"></script>
<script src="libs/dygraphs-1.1.1/shapes.js"></script>
<script src="libs/moment-2.8.4/moment.js"></script>
<script src="libs/moment-timezone-0.2.5/moment-timezone-with-data.js"></script>
<script src="libs/moment-fquarter-1.0.0/moment-fquarter.min.js"></script>
<script src="libs/dygraphs-binding-1.1.1.6/dygraphs.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178679335-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178679335-1', { 'anonymize_ip': true });
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="#">Behavior Analysis with Machine Learning and R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#supplemental-material"><i class="fa fa-check"></i>Supplemental Material</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#conventions"><i class="fa fa-check"></i>Conventions</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.2" data-path="taxonomy.html"><a href="taxonomy.html"><i class="fa fa-check"></i><b>1.2</b> Types of Machine Learning</a></li>
<li class="chapter" data-level="1.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>1.3</b> Terminology</a><ul>
<li class="chapter" data-level="1.3.1" data-path="terminology.html"><a href="terminology.html#tables"><i class="fa fa-check"></i><b>1.3.1</b> Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="terminology.html"><a href="terminology.html#variable-types"><i class="fa fa-check"></i><b>1.3.2</b> Variable Types</a></li>
<li class="chapter" data-level="1.3.3" data-path="terminology.html"><a href="terminology.html#predictive-models"><i class="fa fa-check"></i><b>1.3.3</b> Predictive Models</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="pipeline.html"><a href="pipeline.html"><i class="fa fa-check"></i><b>1.4</b> Data Analysis Pipeline</a></li>
<li class="chapter" data-level="1.5" data-path="trainingeval.html"><a href="trainingeval.html"><i class="fa fa-check"></i><b>1.5</b> Evaluating Predictive Models</a></li>
<li class="chapter" data-level="1.6" data-path="simple-classification-example.html"><a href="simple-classification-example.html"><i class="fa fa-check"></i><b>1.6</b> Simple Classification Example</a><ul>
<li class="chapter" data-level="1.6.1" data-path="simple-classification-example.html"><a href="simple-classification-example.html#k-fold-cross-validation-example"><i class="fa fa-check"></i><b>1.6.1</b> K-fold Cross-Validation Example</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="simple-regression-example.html"><a href="simple-regression-example.html"><i class="fa fa-check"></i><b>1.7</b> Simple Regression Example</a></li>
<li class="chapter" data-level="1.8" data-path="underfitting-and-overfitting.html"><a href="underfitting-and-overfitting.html"><i class="fa fa-check"></i><b>1.8</b> Underfitting and Overfitting</a></li>
<li class="chapter" data-level="1.9" data-path="bias-and-variance.html"><a href="bias-and-variance.html"><i class="fa fa-check"></i><b>1.9</b> Bias and Variance</a></li>
<li class="chapter" data-level="1.10" data-path="SummaryIntro.html"><a href="SummaryIntro.html"><i class="fa fa-check"></i><b>1.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>2</b> Predicting Behavior with Classification Models</a><ul>
<li class="chapter" data-level="2.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html"><i class="fa fa-check"></i><b>2.1</b> <em>k</em>-nearest Neighbors</a><ul>
<li class="chapter" data-level="2.1.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#indoor-location-with-wi-fi-signals"><i class="fa fa-check"></i><b>2.1.1</b> Indoor Location with Wi-Fi Signals</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="performance-metrics.html"><a href="performance-metrics.html"><i class="fa fa-check"></i><b>2.2</b> Performance Metrics</a></li>
<li class="chapter" data-level="2.3" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>2.3</b> Decision Trees</a><ul>
<li class="chapter" data-level="2.3.1" data-path="decision-trees.html"><a href="decision-trees.html#activityRecognition"><i class="fa fa-check"></i><b>2.3.1</b> Activity Recognition with Smartphones</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="dynamic-time-warping.html"><a href="dynamic-time-warping.html"><i class="fa fa-check"></i><b>2.4</b> Dynamic Time Warping</a><ul>
<li class="chapter" data-level="2.4.1" data-path="dynamic-time-warping.html"><a href="dynamic-time-warping.html#sechandgestures"><i class="fa fa-check"></i><b>2.4.1</b> Hand Gesture Recognition</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="summaryClassification.html"><a href="summaryClassification.html"><i class="fa fa-check"></i><b>2.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>3</b> Predicting Behavior with Ensemble Learning</a><ul>
<li class="chapter" data-level="3.1" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>3.1</b> Bagging</a><ul>
<li class="chapter" data-level="3.1.1" data-path="bagging.html"><a href="bagging.html#activity-recognition-with-bagging"><i class="fa fa-check"></i><b>3.1.1</b> Activity recognition with Bagging</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="random-forest.html"><a href="random-forest.html"><i class="fa fa-check"></i><b>3.2</b> Random Forest</a></li>
<li class="chapter" data-level="3.3" data-path="stacked-generalization.html"><a href="stacked-generalization.html"><i class="fa fa-check"></i><b>3.3</b> Stacked Generalization</a></li>
<li class="chapter" data-level="3.4" data-path="multiviewhometasks.html"><a href="multiviewhometasks.html"><i class="fa fa-check"></i><b>3.4</b> Multi-view Stacking for Home Tasks Recognition</a></li>
<li class="chapter" data-level="3.5" data-path="SummaryEnsemble.html"><a href="SummaryEnsemble.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="edavis.html"><a href="edavis.html"><i class="fa fa-check"></i><b>4</b> Exploring and Visualizing Behavioral Data</a><ul>
<li class="chapter" data-level="4.1" data-path="talking-with-field-experts.html"><a href="talking-with-field-experts.html"><i class="fa fa-check"></i><b>4.1</b> Talking with Field Experts</a></li>
<li class="chapter" data-level="4.2" data-path="summary-statistics.html"><a href="summary-statistics.html"><i class="fa fa-check"></i><b>4.2</b> Summary Statistics</a></li>
<li class="chapter" data-level="4.3" data-path="class-distributions.html"><a href="class-distributions.html"><i class="fa fa-check"></i><b>4.3</b> Class Distributions</a></li>
<li class="chapter" data-level="4.4" data-path="user-class-sparsity-matrix.html"><a href="user-class-sparsity-matrix.html"><i class="fa fa-check"></i><b>4.4</b> User-Class Sparsity Matrix</a></li>
<li class="chapter" data-level="4.5" data-path="boxplots.html"><a href="boxplots.html"><i class="fa fa-check"></i><b>4.5</b> Boxplots</a></li>
<li class="chapter" data-level="4.6" data-path="correlation-plots.html"><a href="correlation-plots.html"><i class="fa fa-check"></i><b>4.6</b> Correlation Plots</a><ul>
<li class="chapter" data-level="4.6.1" data-path="correlation-plots.html"><a href="correlation-plots.html#interactive-correlation-plots"><i class="fa fa-check"></i><b>4.6.1</b> Interactive Correlation Plots</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="timeseries.html"><a href="timeseries.html"><i class="fa fa-check"></i><b>4.7</b> Timeseries</a><ul>
<li class="chapter" data-level="4.7.1" data-path="timeseries.html"><a href="timeseries.html#interactive-timeseries"><i class="fa fa-check"></i><b>4.7.1</b> Interactive Timeseries</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="multidimensional-scaling-mds.html"><a href="multidimensional-scaling-mds.html"><i class="fa fa-check"></i><b>4.8</b> Multidimensional Scaling (MDS)</a></li>
<li class="chapter" data-level="4.9" data-path="heatmaps.html"><a href="heatmaps.html"><i class="fa fa-check"></i><b>4.9</b> Heatmaps</a></li>
<li class="chapter" data-level="4.10" data-path="automated-eda.html"><a href="automated-eda.html"><i class="fa fa-check"></i><b>4.10</b> Automated EDA</a></li>
<li class="chapter" data-level="4.11" data-path="SummaryExploratory.html"><a href="SummaryExploratory.html"><i class="fa fa-check"></i><b>4.11</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="preprocessing.html"><a href="preprocessing.html"><i class="fa fa-check"></i><b>5</b> Preprocessing Behavioral Data</a><ul>
<li class="chapter" data-level="5.1" data-path="missing-values.html"><a href="missing-values.html"><i class="fa fa-check"></i><b>5.1</b> Missing Values</a><ul>
<li class="chapter" data-level="5.1.1" data-path="missing-values.html"><a href="missing-values.html#imputation"><i class="fa fa-check"></i><b>5.1.1</b> Imputation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>5.2</b> Smoothing</a></li>
<li class="chapter" data-level="5.3" data-path="normalization.html"><a href="normalization.html"><i class="fa fa-check"></i><b>5.3</b> Normalization</a></li>
<li class="chapter" data-level="5.4" data-path="imbalanced-classes.html"><a href="imbalanced-classes.html"><i class="fa fa-check"></i><b>5.4</b> Imbalanced Classes</a><ul>
<li class="chapter" data-level="5.4.1" data-path="imbalanced-classes.html"><a href="imbalanced-classes.html#random-oversampling"><i class="fa fa-check"></i><b>5.4.1</b> Random Oversampling</a></li>
<li class="chapter" data-level="5.4.2" data-path="imbalanced-classes.html"><a href="imbalanced-classes.html#smote"><i class="fa fa-check"></i><b>5.4.2</b> SMOTE</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="infoinjection.html"><a href="infoinjection.html"><i class="fa fa-check"></i><b>5.5</b> Information Injection</a></li>
<li class="chapter" data-level="5.6" data-path="one-hot-encoding.html"><a href="one-hot-encoding.html"><i class="fa fa-check"></i><b>5.6</b> One-hot Encoding</a></li>
<li class="chapter" data-level="5.7" data-path="SummaryPreprocessing.html"><a href="SummaryPreprocessing.html"><i class="fa fa-check"></i><b>5.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="unsupervised.html"><a href="unsupervised.html"><i class="fa fa-check"></i><b>6</b> Discovering Behaviors with Unsupervised Learning</a><ul>
<li class="chapter" data-level="6.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>6.1</b> K-means clustering</a><ul>
<li class="chapter" data-level="6.1.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#studentresponses"><i class="fa fa-check"></i><b>6.1.1</b> Grouping Student Responses</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="the-silhouette-index.html"><a href="the-silhouette-index.html"><i class="fa fa-check"></i><b>6.2</b> The Silhouette Index</a></li>
<li class="chapter" data-level="6.3" data-path="associationrules.html"><a href="associationrules.html"><i class="fa fa-check"></i><b>6.3</b> Mining Association Rules</a><ul>
<li class="chapter" data-level="6.3.1" data-path="associationrules.html"><a href="associationrules.html#finding-rules-for-criminal-behavior"><i class="fa fa-check"></i><b>6.3.1</b> Finding Rules for Criminal Behavior</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="SummaryUnsupervised.html"><a href="SummaryUnsupervised.html"><i class="fa fa-check"></i><b>6.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="representations.html"><a href="representations.html"><i class="fa fa-check"></i><b>7</b> Encoding Behavioral Data</a><ul>
<li class="chapter" data-level="7.1" data-path="feature-vectors.html"><a href="feature-vectors.html"><i class="fa fa-check"></i><b>7.1</b> Feature Vectors</a></li>
<li class="chapter" data-level="7.2" data-path="sectimeseries.html"><a href="sectimeseries.html"><i class="fa fa-check"></i><b>7.2</b> Timeseries</a></li>
<li class="chapter" data-level="7.3" data-path="transactions.html"><a href="transactions.html"><i class="fa fa-check"></i><b>7.3</b> Transactions</a></li>
<li class="chapter" data-level="7.4" data-path="images.html"><a href="images.html"><i class="fa fa-check"></i><b>7.4</b> Images</a></li>
<li class="chapter" data-level="7.5" data-path="recurrence-plots.html"><a href="recurrence-plots.html"><i class="fa fa-check"></i><b>7.5</b> Recurrence Plots</a><ul>
<li class="chapter" data-level="7.5.1" data-path="recurrence-plots.html"><a href="recurrence-plots.html#computing-recurence-plots"><i class="fa fa-check"></i><b>7.5.1</b> Computing Recurence Plots</a></li>
<li class="chapter" data-level="7.5.2" data-path="recurrence-plots.html"><a href="recurrence-plots.html#recurrence-plots-of-hand-gestures"><i class="fa fa-check"></i><b>7.5.2</b> Recurrence Plots of Hand Gestures</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="bag-of-words.html"><a href="bag-of-words.html"><i class="fa fa-check"></i><b>7.6</b> Bag-of-Words</a><ul>
<li class="chapter" data-level="7.6.1" data-path="bag-of-words.html"><a href="bag-of-words.html#bow-for-complex-activities."><i class="fa fa-check"></i><b>7.6.1</b> BoW for Complex Activities.</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="graphs.html"><a href="graphs.html"><i class="fa fa-check"></i><b>7.7</b> Graphs</a><ul>
<li class="chapter" data-level="7.7.1" data-path="graphs.html"><a href="graphs.html#complex-activities-as-graphs"><i class="fa fa-check"></i><b>7.7.1</b> Complex Activities as Graphs</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="SummaryRepresentations.html"><a href="SummaryRepresentations.html"><i class="fa fa-check"></i><b>7.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deeplearning.html"><a href="deeplearning.html"><i class="fa fa-check"></i><b>8</b> Predicting Behavior with Deep Learning</a><ul>
<li class="chapter" data-level="8.1" data-path="ann.html"><a href="ann.html"><i class="fa fa-check"></i><b>8.1</b> Introduction to Artificial Neural Networks</a><ul>
<li class="chapter" data-level="8.1.1" data-path="ann.html"><a href="ann.html#sigmoid-and-relu-units"><i class="fa fa-check"></i><b>8.1.1</b> Sigmoid and ReLU Units</a></li>
<li class="chapter" data-level="8.1.2" data-path="ann.html"><a href="ann.html#assembling-units-into-layers"><i class="fa fa-check"></i><b>8.1.2</b> Assembling Units into Layers</a></li>
<li class="chapter" data-level="8.1.3" data-path="ann.html"><a href="ann.html#deep-neural-networks"><i class="fa fa-check"></i><b>8.1.3</b> Deep Neural Networks</a></li>
<li class="chapter" data-level="8.1.4" data-path="ann.html"><a href="ann.html#learning-the-parameters"><i class="fa fa-check"></i><b>8.1.4</b> Learning the Parameters</a></li>
<li class="chapter" data-level="8.1.5" data-path="ann.html"><a href="ann.html#parameter-learning-example-in-r"><i class="fa fa-check"></i><b>8.1.5</b> Parameter Learning Example in R</a></li>
<li class="chapter" data-level="8.1.6" data-path="ann.html"><a href="ann.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>8.1.6</b> Stochastic Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="keras-and-tensorflow-with-r.html"><a href="keras-and-tensorflow-with-r.html"><i class="fa fa-check"></i><b>8.2</b> Keras and TensorFlow with R</a><ul>
<li class="chapter" data-level="8.2.1" data-path="keras-and-tensorflow-with-r.html"><a href="keras-and-tensorflow-with-r.html#keras-example"><i class="fa fa-check"></i><b>8.2.1</b> Keras Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="classification-with-neural-networks.html"><a href="classification-with-neural-networks.html"><i class="fa fa-check"></i><b>8.3</b> Classification with Neural Networks</a><ul>
<li class="chapter" data-level="8.3.1" data-path="classification-with-neural-networks.html"><a href="classification-with-neural-networks.html#classification-of-electromyography-signals"><i class="fa fa-check"></i><b>8.3.1</b> Classification of Electromyography Signals</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>8.4</b> Overfitting</a><ul>
<li class="chapter" data-level="8.4.1" data-path="overfitting.html"><a href="overfitting.html#early-stopping"><i class="fa fa-check"></i><b>8.4.1</b> Early Stopping</a></li>
<li class="chapter" data-level="8.4.2" data-path="overfitting.html"><a href="overfitting.html#dropout"><i class="fa fa-check"></i><b>8.4.2</b> Dropout</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="fine-tuning-a-neural-network.html"><a href="fine-tuning-a-neural-network.html"><i class="fa fa-check"></i><b>8.5</b> Fine-Tuning a Neural Network</a></li>
<li class="chapter" data-level="8.6" data-path="cnns.html"><a href="cnns.html"><i class="fa fa-check"></i><b>8.6</b> Convolutional Neural Networks</a><ul>
<li class="chapter" data-level="8.6.1" data-path="cnns.html"><a href="cnns.html#convolutions"><i class="fa fa-check"></i><b>8.6.1</b> Convolutions</a></li>
<li class="chapter" data-level="8.6.2" data-path="cnns.html"><a href="cnns.html#pooling-operations"><i class="fa fa-check"></i><b>8.6.2</b> Pooling Operations</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="cnns-with-keras.html"><a href="cnns-with-keras.html"><i class="fa fa-check"></i><b>8.7</b> CNNs with Keras</a><ul>
<li class="chapter" data-level="8.7.1" data-path="cnns-with-keras.html"><a href="cnns-with-keras.html#example-1"><i class="fa fa-check"></i><b>8.7.1</b> Example 1</a></li>
<li class="chapter" data-level="8.7.2" data-path="cnns-with-keras.html"><a href="cnns-with-keras.html#example-2"><i class="fa fa-check"></i><b>8.7.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="cnnSmile.html"><a href="cnnSmile.html"><i class="fa fa-check"></i><b>8.8</b> Smiles Detection with a CNN</a></li>
<li class="chapter" data-level="8.9" data-path="SummaryDeepLearning.html"><a href="SummaryDeepLearning.html"><i class="fa fa-check"></i><b>8.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multiuser.html"><a href="multiuser.html"><i class="fa fa-check"></i><b>9</b> Multi-User Validation</a><ul>
<li class="chapter" data-level="9.1" data-path="mixed-models.html"><a href="mixed-models.html"><i class="fa fa-check"></i><b>9.1</b> Mixed Models</a><ul>
<li class="chapter" data-level="9.1.1" data-path="mixed-models.html"><a href="mixed-models.html#skeleton-action-recognition-with-mixed-models"><i class="fa fa-check"></i><b>9.1.1</b> Skeleton Action Recognition with Mixed Models</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="user-independent-models.html"><a href="user-independent-models.html"><i class="fa fa-check"></i><b>9.2</b> User-Independent Models</a></li>
<li class="chapter" data-level="9.3" data-path="user-dependent-models.html"><a href="user-dependent-models.html"><i class="fa fa-check"></i><b>9.3</b> User-Dependent Models</a></li>
<li class="chapter" data-level="9.4" data-path="user-adaptive-models.html"><a href="user-adaptive-models.html"><i class="fa fa-check"></i><b>9.4</b> User-Adaptive Models</a><ul>
<li class="chapter" data-level="9.4.1" data-path="user-adaptive-models.html"><a href="user-adaptive-models.html#transfer-learning"><i class="fa fa-check"></i><b>9.4.1</b> Transfer Learning</a></li>
<li class="chapter" data-level="9.4.2" data-path="user-adaptive-models.html"><a href="user-adaptive-models.html#a-user-adaptive-model-for-activity-recognition"><i class="fa fa-check"></i><b>9.4.2</b> A User-Adaptive Model for Activity Recognition</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="SummaryMultiUser.html"><a href="SummaryMultiUser.html"><i class="fa fa-check"></i><b>9.5</b> Summary</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendixInstall.html"><a href="appendixInstall.html"><i class="fa fa-check"></i><b>A</b> Setup Your Environment</a><ul>
<li class="chapter" data-level="A.1" data-path="installing-the-datasets.html"><a href="installing-the-datasets.html"><i class="fa fa-check"></i><b>A.1</b> Installing the Datasets</a></li>
<li class="chapter" data-level="A.2" data-path="installing-the-examples-source-code.html"><a href="installing-the-examples-source-code.html"><i class="fa fa-check"></i><b>A.2</b> Installing the Examples Source Code</a></li>
<li class="chapter" data-level="A.3" data-path="installing-keras-and-tensorflow-.html"><a href="installing-keras-and-tensorflow-.html"><i class="fa fa-check"></i><b>A.3</b> Installing Keras and TensorFlow.</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendixDatasets.html"><a href="appendixDatasets.html"><i class="fa fa-check"></i><b>B</b> Datasets</a><ul>
<li class="chapter" data-level="B.1" data-path="complex-activities.html"><a href="complex-activities.html"><i class="fa fa-check"></i><b>B.1</b> COMPLEX ACTIVITIES</a></li>
<li class="chapter" data-level="B.2" data-path="depresjon.html"><a href="depresjon.html"><i class="fa fa-check"></i><b>B.2</b> DEPRESJON</a></li>
<li class="chapter" data-level="B.3" data-path="electromyography.html"><a href="electromyography.html"><i class="fa fa-check"></i><b>B.3</b> ELECTROMYOGRAPHY</a></li>
<li class="chapter" data-level="B.4" data-path="hand-gestures.html"><a href="hand-gestures.html"><i class="fa fa-check"></i><b>B.4</b> HAND GESTURES</a></li>
<li class="chapter" data-level="B.5" data-path="home-tasks.html"><a href="home-tasks.html"><i class="fa fa-check"></i><b>B.5</b> HOME TASKS</a></li>
<li class="chapter" data-level="B.6" data-path="homicide-reports.html"><a href="homicide-reports.html"><i class="fa fa-check"></i><b>B.6</b> HOMICIDE REPORTS</a></li>
<li class="chapter" data-level="B.7" data-path="indoor-location.html"><a href="indoor-location.html"><i class="fa fa-check"></i><b>B.7</b> INDOOR LOCATION</a></li>
<li class="chapter" data-level="B.8" data-path="sheep-goats.html"><a href="sheep-goats.html"><i class="fa fa-check"></i><b>B.8</b> SHEEP GOATS</a></li>
<li class="chapter" data-level="B.9" data-path="skeleton-actions.html"><a href="skeleton-actions.html"><i class="fa fa-check"></i><b>B.9</b> SKELETON ACTIONS</a></li>
<li class="chapter" data-level="B.10" data-path="smartphone-activities.html"><a href="smartphone-activities.html"><i class="fa fa-check"></i><b>B.10</b> SMARTPHONE ACTIVITIES</a></li>
<li class="chapter" data-level="B.11" data-path="smiles.html"><a href="smiles.html"><i class="fa fa-check"></i><b>B.11</b> SMILES</a></li>
<li class="chapter" data-level="B.12" data-path="students-mental-health.html"><a href="students-mental-health.html"><i class="fa fa-check"></i><b>B.12</b> STUDENTS’ MENTAL HEALTH</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="citing-this-book.html"><a href="citing-this-book.html"><i class="fa fa-check"></i>Citing this Book</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Behavior Analysis with Machine Learning and R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification-with-neural-networks" class="section level2">
<h2><span class="header-section-number">8.3</span> Classification with Neural Networks</h2>
<p>Neural networks are trained iteratively by modifying their weights aiming to minimize the loss function. When the network predicts real numbers, the MSE loss function is normally used. For classification problems, the network should predict what is the most likely class out of <span class="math inline">\(k\)</span> possible categories. To make a neural network work for classification problems, we need to introduce new elements to its architecture:</p>
<ol style="list-style-type: decimal">
<li>Add more units to the output layer.</li>
<li>Use a <strong>softmax</strong> activation function in the output layer.</li>
<li>Use <strong>average cross-entropy</strong> as the loss function.</li>
</ol>
<p>Let’s start with point number <span class="math inline">\(1\)</span> (add more units to the output layer). This means that if the number of classes is <span class="math inline">\(k\)</span>, then the last layer needs to have <span class="math inline">\(k\)</span> units, one for each class. That’s it!. Figure <a href="classification-with-neural-networks.html#fig:nnCrossEntropy">8.18</a> shows an example of a neural network with an output layer having <span class="math inline">\(3\)</span> units. Each unit predicts a score for each of the <span class="math inline">\(3\)</span> classes. Let’s call the vector of predicted scores <span class="math inline">\(y&#39;\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:nnCrossEntropy"></span>
<img src="images/nn_cross-entropy.png" alt="Neural network with 3 output scores. Softmax is applied to the scores and the cross-entropy with the true scores is calculated. This gives us an estimate of the similarity between the network's predictions and the true values." width="100%" />
<p class="caption">
Figure 8.18: Neural network with 3 output scores. Softmax is applied to the scores and the cross-entropy with the true scores is calculated. This gives us an estimate of the similarity between the network’s predictions and the true values.
</p>
</div>
<p>Point number <span class="math inline">\(2\)</span> says that a <strong>softmax</strong> activation function should be used in the output layer. When training the network, just as with regression, we need a way to compute the error between the predicted values <span class="math inline">\(y&#39;\)</span> and the true values <span class="math inline">\(y\)</span>. In this case, <span class="math inline">\(y\)</span> is a one-hot encoded vector with a <span class="math inline">\(1\)</span> at the position of the true class and <span class="math inline">\(0s\)</span> elsewhere. If you are not familiar with one-hot encoding, you can check the topic in chapter <a href="preprocessing.html#preprocessing">5</a>. As opposed to other classifiers like decision trees, <span class="math inline">\(k\)</span>-nn, etc., neural networks need the classes to be one-hot encoded.</p>
<p>With regression problems, one way to compare the prediction with the true value is by using the squared difference: <span class="math inline">\((y&#39; - y)^2\)</span>. With classification, <span class="math inline">\(y\)</span> and <span class="math inline">\(y&#39;\)</span> are vectors so we need a way to compare them. The true values <span class="math inline">\(y\)</span> are represented as a vector of probabilities with a <span class="math inline">\(1\)</span> at the position of the true class. The output scores <span class="math inline">\(y&#39;\)</span> do not necessarily sum up to <span class="math inline">\(1\)</span> thus, they are not proper probabilities. Before comparing <span class="math inline">\(y\)</span> and <span class="math inline">\(y&#39;\)</span> we need them both to be probabilities. The <strong>softmax</strong> activation function is used to convert <span class="math inline">\(y&#39;\)</span> into a vector of probabilities. The softmax function is applied individually to each element of a vector:</p>
<p><span class="math display" id="eq:softmax">\[\begin{equation}
  softmax(\boldsymbol{x},i) = \frac{e^{\boldsymbol{x}_i}}{\sum_{j}{e^{\boldsymbol{x}_j}}}
  \tag{8.9}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{x}\)</span> is a vector and <span class="math inline">\(i\)</span> is an index pointing to a particular element in the vector. Thus, to convert <span class="math inline">\(y&#39;\)</span> into a vector of probabilities we need to apply softmax to each of its elements. One thing to note is that this activation function depends on all the values in the vector (the output values of all units). Figure <a href="classification-with-neural-networks.html#fig:nnCrossEntropy">8.18</a> shows the resulting vector of probabilities after applying softmax to each element of <span class="math inline">\(y&#39;\)</span>. In R this can be implemented like the following:</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="classification-with-neural-networks.html#cb135-1"></a><span class="co"># Scores from the figure.</span></span>
<span id="cb135-2"><a href="classification-with-neural-networks.html#cb135-2"></a>scores &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">3.0</span>, <span class="fl">0.03</span>, <span class="fl">1.2</span>)</span>
<span id="cb135-3"><a href="classification-with-neural-networks.html#cb135-3"></a></span>
<span id="cb135-4"><a href="classification-with-neural-networks.html#cb135-4"></a><span class="co"># Softmax function.</span></span>
<span id="cb135-5"><a href="classification-with-neural-networks.html#cb135-5"></a>softmax &lt;-<span class="st"> </span><span class="cf">function</span>(scores){</span>
<span id="cb135-6"><a href="classification-with-neural-networks.html#cb135-6"></a>  <span class="kw">exp</span>(scores) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">exp</span>(scores))</span>
<span id="cb135-7"><a href="classification-with-neural-networks.html#cb135-7"></a>}</span>
<span id="cb135-8"><a href="classification-with-neural-networks.html#cb135-8"></a>probabilities &lt;-<span class="st"> </span><span class="kw">softmax</span>(scores)</span>
<span id="cb135-9"><a href="classification-with-neural-networks.html#cb135-9"></a><span class="kw">print</span>(probabilities)</span>
<span id="cb135-10"><a href="classification-with-neural-networks.html#cb135-10"></a><span class="co">#&gt; [1] 0.82196136 0.04216934 0.13586930</span></span>
<span id="cb135-11"><a href="classification-with-neural-networks.html#cb135-11"></a><span class="kw">print</span>(<span class="kw">sum</span>(probabilities)) <span class="co"># Should sum up to 1.</span></span>
<span id="cb135-12"><a href="classification-with-neural-networks.html#cb135-12"></a><span class="co">#&gt; [1] 1</span></span></code></pre></div>
<p>We used R vectorization capabilities to compute the final vector of probabilities within the same function without having to iterate through each element. When using Keras, these operations are efficiently computed by the backend (for example, TensorFlow).</p>
<p>Finally, point <span class="math inline">\(3\)</span> states that we need to use <strong>average cross-entropy</strong> as the <strong>loss function</strong>. Now that we have converted <span class="math inline">\(y&#39;\)</span> into probabilities, we can compute its dissimilarity with <span class="math inline">\(y\)</span>. The distance (dissimilarity) between two vectors (<span class="math inline">\(A\)</span>,<span class="math inline">\(B\)</span>) of probabilities can be computed using the <strong>cross-entropy</strong>:</p>
<p><span class="math display" id="eq:crossentropy">\[\begin{equation}
  CE(A,B) = - \sum_{i}{B_i log(A_i)}
  \tag{8.10}
\end{equation}\]</span></p>
<p>Thus, to get the dissimilarity between <span class="math inline">\(y&#39;\)</span> and <span class="math inline">\(y\)</span> first we apply softmax to <span class="math inline">\(y&#39;\)</span> (to transform it into proper probabilities) and then, we can compute the cross entropy between the resulting vector of probabilities and <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[\begin{equation}
  CE(softmax(y&#39;),y).
\end{equation}\]</span></p>
<p>In R this can be implemented with the following:</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="classification-with-neural-networks.html#cb136-1"></a><span class="co"># Cross-entropy</span></span>
<span id="cb136-2"><a href="classification-with-neural-networks.html#cb136-2"></a>CE &lt;-<span class="st"> </span><span class="cf">function</span>(A,B){</span>
<span id="cb136-3"><a href="classification-with-neural-networks.html#cb136-3"></a> <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(B <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(A))</span>
<span id="cb136-4"><a href="classification-with-neural-networks.html#cb136-4"></a>}</span>
<span id="cb136-5"><a href="classification-with-neural-networks.html#cb136-5"></a></span>
<span id="cb136-6"><a href="classification-with-neural-networks.html#cb136-6"></a>y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb136-7"><a href="classification-with-neural-networks.html#cb136-7"></a></span>
<span id="cb136-8"><a href="classification-with-neural-networks.html#cb136-8"></a><span class="kw">print</span>(<span class="kw">CE</span>(<span class="kw">softmax</span>(scores), y))</span>
<span id="cb136-9"><a href="classification-with-neural-networks.html#cb136-9"></a><span class="co">#&gt; [1] 0.1960619</span></span></code></pre></div>

<div class="rmdcaution">
Be aware that when computing the cross-entropy with equation <a href="classification-with-neural-networks.html#eq:crossentropy">(8.10)</a> <strong>order matters</strong>. The first element should be the predicted scores <span class="math inline">\(y&#39;\)</span> and the second element should be the true one-hot encoded vector <span class="math inline">\(y\)</span>. We don’t want to apply a log function to a vector with values of <span class="math inline">\(0\)</span>. Most of the time, the predicted scores <span class="math inline">\(y&#39;\)</span> will be different from <span class="math inline">\(0\)</span> that’s why we prefer to apply the log function to them. In the very rare case when the predicted scores have zeros, we can add a very small number. In practice, this is taken care of by the backend (e.g., Tensorflow).
</div>

<p>Now we know how to compute the cross-entropy for each training instance. The total loss function is then, the <strong>average cross-entropy across the training points</strong>. The next section shows how to build a neural network for classification using Keras.</p>
<div id="classification-of-electromyography-signals" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Classification of Electromyography Signals</h3>

<div class="rmdfolder">
<code>keras_electromyography.R</code>
</div>

<p>In this example, we will train a neural network with Keras to classify hand gestures based on muscle electrical activity. The <em>ELECTROYMYOGRAPHY</em> dataset will be used here. The electrical activity was recorded with an electromyography (EMG) sensor worn as an armband. The data were collected and made available by <span class="citation">Yashuk (<a href="#ref-kirill" role="doc-biblioref">2019</a>)</span>. The armband device has <span class="math inline">\(8\)</span> sensors which are placed on the skin surface and measure electrical activity from the right forearm at a sampling rate of <span class="math inline">\(200\)</span> Hz. A video of the device can be found here: <a href="https://youtu.be/1u5-G6DPtkk" class="uri">https://youtu.be/1u5-G6DPtkk</a></p>
<p>The data contains <span class="math inline">\(4\)</span> different gestures: 0-rock, 1-scissors, 2-paper, 3-OK, and has <span class="math inline">\(65\)</span> columns. The last column is the class label from <span class="math inline">\(0\)</span> to <span class="math inline">\(3\)</span>. The first <span class="math inline">\(64\)</span> columns are electrical measurements. <span class="math inline">\(8\)</span> consecutive readings for each of the <span class="math inline">\(8\)</span> sensors. The objective is to use the first <span class="math inline">\(64\)</span> variables to predict the class.</p>
<p>The script <code>keras_electromyography.R</code> has the full code. We start by splitting the <code>dataset</code> into train (<span class="math inline">\(60\%\)</span>), validation (<span class="math inline">\(10\%\)</span>) and test (<span class="math inline">\(30\%\)</span>) sets. We will use the validation set to monitor the performance during each epoch. We also need to normalize the three sets but only learning the normalization parameters from the train set. The <code>normalize()</code> function included in the script will do the job.</p>
<p>One last thing we need to do is to format the data as matrices and one-hot encode the class. The following code defines a function that takes as input a data frame and the expected number of classes. It assumes that the first columns are the features and the last column contains the class. First, it converts the features into a matrix and stores them in <code>x</code>. Then, it converts the class into an array and one-hot encodes it using the <code>to_categorical()</code> function from Keras. The classes are stored in <code>y</code> and the function returns a list with the features and one-hot encoded classes. Then, we can call the function with the train, validation, and test sets.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="classification-with-neural-networks.html#cb137-1"></a><span class="co"># Define a function to format features and one-hot encode the class.</span></span>
<span id="cb137-2"><a href="classification-with-neural-networks.html#cb137-2"></a>format.to.array &lt;-<span class="st"> </span><span class="cf">function</span>(data, <span class="dt">numclasses =</span> <span class="dv">4</span>){</span>
<span id="cb137-3"><a href="classification-with-neural-networks.html#cb137-3"></a>  x &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(data[, <span class="dv">1</span><span class="op">:</span>(<span class="kw">ncol</span>(data)<span class="op">-</span><span class="dv">1</span>)])</span>
<span id="cb137-4"><a href="classification-with-neural-networks.html#cb137-4"></a>  y &lt;-<span class="st"> </span><span class="kw">as.array</span>(data[, <span class="kw">ncol</span>(data)])</span>
<span id="cb137-5"><a href="classification-with-neural-networks.html#cb137-5"></a>  y &lt;-<span class="st"> </span><span class="kw">to_categorical</span>(y, <span class="dt">num_classes =</span> numclasses)</span>
<span id="cb137-6"><a href="classification-with-neural-networks.html#cb137-6"></a>  l &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y)</span>
<span id="cb137-7"><a href="classification-with-neural-networks.html#cb137-7"></a>  <span class="kw">return</span>(l)</span>
<span id="cb137-8"><a href="classification-with-neural-networks.html#cb137-8"></a>}</span>
<span id="cb137-9"><a href="classification-with-neural-networks.html#cb137-9"></a></span>
<span id="cb137-10"><a href="classification-with-neural-networks.html#cb137-10"></a><span class="co"># Format data</span></span>
<span id="cb137-11"><a href="classification-with-neural-networks.html#cb137-11"></a>trainset &lt;-<span class="st"> </span><span class="kw">format.to.array</span>(trainset, <span class="dt">numclasses =</span> <span class="dv">4</span>)</span>
<span id="cb137-12"><a href="classification-with-neural-networks.html#cb137-12"></a>valset &lt;-<span class="st"> </span><span class="kw">format.to.array</span>(valset, <span class="dt">numclasses =</span> <span class="dv">4</span>)</span>
<span id="cb137-13"><a href="classification-with-neural-networks.html#cb137-13"></a>testset &lt;-<span class="st"> </span><span class="kw">format.to.array</span>(testset, <span class="dt">numclasses =</span> <span class="dv">4</span>)</span></code></pre></div>
<p>Let’s print the first one-hot encoded classes from the train set:</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="classification-with-neural-networks.html#cb138-1"></a><span class="kw">head</span>(trainset<span class="op">$</span>y)</span>
<span id="cb138-2"><a href="classification-with-neural-networks.html#cb138-2"></a></span>
<span id="cb138-3"><a href="classification-with-neural-networks.html#cb138-3"></a><span class="co">#&gt;        [,1] [,2] [,3] [,4]</span></span>
<span id="cb138-4"><a href="classification-with-neural-networks.html#cb138-4"></a><span class="co">#&gt; [1,]    0    0    1    0</span></span>
<span id="cb138-5"><a href="classification-with-neural-networks.html#cb138-5"></a><span class="co">#&gt; [2,]    0    0    1    0</span></span>
<span id="cb138-6"><a href="classification-with-neural-networks.html#cb138-6"></a><span class="co">#&gt; [3,]    0    0    1    0</span></span>
<span id="cb138-7"><a href="classification-with-neural-networks.html#cb138-7"></a><span class="co">#&gt; [4,]    0    0    0    1</span></span>
<span id="cb138-8"><a href="classification-with-neural-networks.html#cb138-8"></a><span class="co">#&gt; [5,]    1    0    0    0</span></span>
<span id="cb138-9"><a href="classification-with-neural-networks.html#cb138-9"></a><span class="co">#&gt; [6,]    0    0    0    1</span></span></code></pre></div>
<p>The first three instances belong to the class <em>‘paper’</em> because the <span class="math inline">\(1s\)</span> are in the third position. The corresponding integers are 0-rock, 1-scissors, 2-paper, 3-OK. So <em>‘paper’</em> comes in the third position. The fourth instance belongs to the class <em>‘OK’</em>, the fifth to <em>‘rock’</em>, and so on.</p>
<p>Now it’s time to define the neural network architecture! We will do so inside a function:</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="classification-with-neural-networks.html#cb139-1"></a><span class="co"># Define the network&#39;s architecture.</span></span>
<span id="cb139-2"><a href="classification-with-neural-networks.html#cb139-2"></a>get.nn &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">ninputs =</span> <span class="dv">64</span>, <span class="dt">nclasses =</span> <span class="dv">4</span>, <span class="dt">lr =</span> <span class="fl">0.01</span>){</span>
<span id="cb139-3"><a href="classification-with-neural-networks.html#cb139-3"></a>  </span>
<span id="cb139-4"><a href="classification-with-neural-networks.html#cb139-4"></a>  model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>()</span>
<span id="cb139-5"><a href="classification-with-neural-networks.html#cb139-5"></a>  </span>
<span id="cb139-6"><a href="classification-with-neural-networks.html#cb139-6"></a>  model <span class="op">%&gt;%</span></span>
<span id="cb139-7"><a href="classification-with-neural-networks.html#cb139-7"></a><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">32</span>, <span class="dt">activation =</span> <span class="st">&#39;relu&#39;</span>,</span>
<span id="cb139-8"><a href="classification-with-neural-networks.html#cb139-8"></a>                <span class="dt">input_shape =</span> ninputs) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb139-9"><a href="classification-with-neural-networks.html#cb139-9"></a><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">16</span>, <span class="dt">activation =</span> <span class="st">&#39;relu&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb139-10"><a href="classification-with-neural-networks.html#cb139-10"></a><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> nclasses, <span class="dt">activation =</span> <span class="st">&#39;softmax&#39;</span>)</span>
<span id="cb139-11"><a href="classification-with-neural-networks.html#cb139-11"></a>  </span>
<span id="cb139-12"><a href="classification-with-neural-networks.html#cb139-12"></a>  model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(</span>
<span id="cb139-13"><a href="classification-with-neural-networks.html#cb139-13"></a>    <span class="dt">loss =</span> <span class="st">&#39;categorical_crossentropy&#39;</span>,</span>
<span id="cb139-14"><a href="classification-with-neural-networks.html#cb139-14"></a>    <span class="dt">optimizer =</span> <span class="kw">optimizer_sgd</span>(<span class="dt">lr =</span> lr),</span>
<span id="cb139-15"><a href="classification-with-neural-networks.html#cb139-15"></a>    <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb139-16"><a href="classification-with-neural-networks.html#cb139-16"></a>  )</span>
<span id="cb139-17"><a href="classification-with-neural-networks.html#cb139-17"></a>  </span>
<span id="cb139-18"><a href="classification-with-neural-networks.html#cb139-18"></a>  <span class="kw">return</span>(model)</span>
<span id="cb139-19"><a href="classification-with-neural-networks.html#cb139-19"></a>}</span></code></pre></div>
<p>The first argument takes the number of inputs (features), the second argument specifies the number of classes and the last argument is the learning rate <span class="math inline">\(\alpha\)</span>. The first line instantiates an empty keras sequential model. Then we add three layers. The first two are hidden layers and the last one will be the output layer. The input layer is implicitly defined when setting the <code>input_shape</code> parameter in the first layer. The first hidden layer has <span class="math inline">\(32\)</span> units with a ReLU activation function. Since this is the first hidden layer we also need to specify what is the expected input by setting the <code>input_shape</code>. In this case, it is the number of inputs which will be <span class="math inline">\(64\)</span> as it is the number of features. The next hidden layer has <span class="math inline">\(16\)</span> ReLU units. For the output layer, the number of units needs to be equal to the number of classes (<span class="math inline">\(4\)</span> in this case). Since this is a classification problem we also set the activation function to <code>softmax</code>.</p>
<p>Then, the model is compiled and the loss function is set to <code>categorical_crossentropy</code> because this is a classification problem. Stochastic gradient descent is used with a learning rate passed as a parameter. During training, we want to monitor the <em>accuracy</em>. Finally, the function returns the compiled model.</p>
<p>Now we can call our function to create a model. This one will have <span class="math inline">\(64\)</span> inputs, <span class="math inline">\(4\)</span> outputs and we will use a learning rate of <span class="math inline">\(0.01\)</span>. It is always useful to print a summary of the model with the <code>summary()</code> function.</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="classification-with-neural-networks.html#cb140-1"></a>model &lt;-<span class="st"> </span><span class="kw">get.nn</span>(<span class="dv">64</span>, <span class="dv">4</span>, <span class="dt">lr =</span> <span class="fl">0.01</span>)</span>
<span id="cb140-2"><a href="classification-with-neural-networks.html#cb140-2"></a><span class="kw">summary</span>(model)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:nnEMGSummary"></span>
<img src="images/nn_emg_summary.png" alt="Summary of the network." width="90%" />
<p class="caption">
Figure 8.19: Summary of the network.
</p>
</div>
<p>From the summary, we can see that the network has <span class="math inline">\(3\)</span> layers. The second column shows the output shape which in this case corresponds to the number of units in each layer. The last column shows the number of parameters of each layer. For example, the first layer has <span class="math inline">\(2080\)</span> parameters! Those come from the weights and biases. There are <span class="math inline">\(64\)</span> (inputs) * <span class="math inline">\(32\)</span> (units) = <span class="math inline">\(2048\)</span> weights plus the <span class="math inline">\(32\)</span> biases (one for each unit). The biases are included by default on each layer unless otherwise specified.</p>
<p>The second layer receives <span class="math inline">\(32\)</span> inputs on each of its <span class="math inline">\(16\)</span> units. Thus <span class="math inline">\(32\)</span> * <span class="math inline">\(16\)</span> + <span class="math inline">\(16\)</span> (biases) = <span class="math inline">\(528\)</span>. The last layer has <span class="math inline">\(16\)</span> inputs from the previous layer on each of its <span class="math inline">\(4\)</span> units plus <span class="math inline">\(4\)</span> biases giving a total of <span class="math inline">\(68\)</span> parameters. In total, the network has <span class="math inline">\(2676\)</span> parameters. Here, we can see how fast the number of parameters grows when adding more layers and units. Now, we can use the <code>fit()</code> function to train the model.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="classification-with-neural-networks.html#cb141-1"></a>history &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(</span>
<span id="cb141-2"><a href="classification-with-neural-networks.html#cb141-2"></a>  trainset<span class="op">$</span>x, trainset<span class="op">$</span>y,</span>
<span id="cb141-3"><a href="classification-with-neural-networks.html#cb141-3"></a>  <span class="dt">epochs =</span> <span class="dv">300</span>,</span>
<span id="cb141-4"><a href="classification-with-neural-networks.html#cb141-4"></a>  <span class="dt">batch_size =</span> <span class="dv">8</span>,</span>
<span id="cb141-5"><a href="classification-with-neural-networks.html#cb141-5"></a>  <span class="dt">validation_data =</span> <span class="kw">list</span>(valset<span class="op">$</span>x, valset<span class="op">$</span>y),</span>
<span id="cb141-6"><a href="classification-with-neural-networks.html#cb141-6"></a>  <span class="dt">verbose =</span> <span class="dv">1</span>,</span>
<span id="cb141-7"><a href="classification-with-neural-networks.html#cb141-7"></a>  <span class="dt">view_metrics =</span> <span class="ot">TRUE</span></span>
<span id="cb141-8"><a href="classification-with-neural-networks.html#cb141-8"></a>)</span></code></pre></div>
<p>The model is trained for <span class="math inline">\(300\)</span> epochs with a batch size of <span class="math inline">\(8\)</span>. We used the <code>validation_data</code> parameter to specify the validation set to compute the performance on unseen data. The training will take some minutes to complete. Bigger models can take hours or even several days. Thus, it is a good idea to save a model once it is trained.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="classification-with-neural-networks.html#cb142-1"></a><span class="co"># Save model.</span></span>
<span id="cb142-2"><a href="classification-with-neural-networks.html#cb142-2"></a><span class="kw">save_model_hdf5</span>(model, <span class="st">&quot;electromyography.hdf5&quot;</span>)</span></code></pre></div>
<p>We can load a previously saved model with:</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="classification-with-neural-networks.html#cb143-1"></a><span class="co"># Load model.</span></span>
<span id="cb143-2"><a href="classification-with-neural-networks.html#cb143-2"></a>model &lt;-<span class="st"> </span><span class="kw">load_model_hdf5</span>(<span class="st">&quot;electromyography.hdf5&quot;</span>)</span></code></pre></div>
<p>Figure <a href="classification-with-neural-networks.html#fig:nnEMGloss">8.20</a> shows the train and validation loss and accuracy as produced by <code>plot(history)</code>. We can see that both the training and validation loss are decreasing over time. The accuracy increases over time.</p>
<div class="figure" style="text-align: center"><span id="fig:nnEMGloss"></span>
<img src="images/nn_emg_loss.png" alt="Loss and accuracy of the electromyography model." width="100%" />
<p class="caption">
Figure 8.20: Loss and accuracy of the electromyography model.
</p>
</div>
<p>Now, we can evaluate the performance of the trained model with the test set using the <code>evaluate()</code> function.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="classification-with-neural-networks.html#cb144-1"></a><span class="co"># Evaluate model.</span></span>
<span id="cb144-2"><a href="classification-with-neural-networks.html#cb144-2"></a>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">evaluate</span>(testset<span class="op">$</span>x, testset<span class="op">$</span>y)</span>
<span id="cb144-3"><a href="classification-with-neural-networks.html#cb144-3"></a></span>
<span id="cb144-4"><a href="classification-with-neural-networks.html#cb144-4"></a><span class="co">#&gt;      loss  accuracy </span></span>
<span id="cb144-5"><a href="classification-with-neural-networks.html#cb144-5"></a><span class="co">#&gt; 0.4045424 0.8474576</span></span></code></pre></div>
<p>The accuracy was pretty decent (<span class="math inline">\(\approx 84\%\)</span>). If you want to get the actual class predictions you can use the <code>predict_classes()</code> function.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="classification-with-neural-networks.html#cb145-1"></a><span class="co"># Predict classes.</span></span>
<span id="cb145-2"><a href="classification-with-neural-networks.html#cb145-2"></a>classes &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict_classes</span>(testset<span class="op">$</span>x)</span>
<span id="cb145-3"><a href="classification-with-neural-networks.html#cb145-3"></a><span class="kw">head</span>(classes)</span>
<span id="cb145-4"><a href="classification-with-neural-networks.html#cb145-4"></a><span class="co">#&gt; [1] 2 2 1 3 0 1</span></span></code></pre></div>
<p>Note that this function returns the classes with numbers starting with <span class="math inline">\(0\)</span> just as in the original dataset.</p>
<p>Sometimes it is also useful to get the actual predicted scores for each class. This can be done with the <code>predict_on_batch()</code> function.</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="classification-with-neural-networks.html#cb146-1"></a><span class="co"># Make predictions on the test set.</span></span>
<span id="cb146-2"><a href="classification-with-neural-networks.html#cb146-2"></a>predictions &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict_on_batch</span>(testset<span class="op">$</span>x)</span>
<span id="cb146-3"><a href="classification-with-neural-networks.html#cb146-3"></a><span class="kw">head</span>(predictions)</span>
<span id="cb146-4"><a href="classification-with-neural-networks.html#cb146-4"></a><span class="co">#&gt;              [,1]         [,2]         [,3]         [,4]</span></span>
<span id="cb146-5"><a href="classification-with-neural-networks.html#cb146-5"></a><span class="co">#&gt; [1,] 1.957638e-05 8.726048e-02 7.708290e-01 1.418910e-01</span></span>
<span id="cb146-6"><a href="classification-with-neural-networks.html#cb146-6"></a><span class="co">#&gt; [2,] 3.937355e-05 2.571992e-04 9.965665e-01 3.136863e-03</span></span>
<span id="cb146-7"><a href="classification-with-neural-networks.html#cb146-7"></a><span class="co">#&gt; [3,] 4.261451e-03 7.343097e-01 7.226156e-02 1.891673e-01</span></span>
<span id="cb146-8"><a href="classification-with-neural-networks.html#cb146-8"></a><span class="co">#&gt; [4,] 8.669784e-06 2.088269e-04 1.339851e-01 8.657974e-01</span></span>
<span id="cb146-9"><a href="classification-with-neural-networks.html#cb146-9"></a><span class="co">#&gt; [5,] 9.999956e-01 7.354113e-26 1.299388e-08 4.451362e-06</span></span>
<span id="cb146-10"><a href="classification-with-neural-networks.html#cb146-10"></a><span class="co">#&gt; [6,] 2.513005e-05 9.914154e-01 7.252949e-03 1.306421e-03</span></span></code></pre></div>
<p>If we want to get the actual classes from the scores we can get the index of the maximum column. Then we subtract <span class="math inline">\(-1\)</span> so classes start at <span class="math inline">\(0\)</span>.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="classification-with-neural-networks.html#cb147-1"></a>classes &lt;-<span class="st"> </span><span class="kw">max.col</span>(predictions) <span class="op">-</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb147-2"><a href="classification-with-neural-networks.html#cb147-2"></a><span class="kw">head</span>(classes)</span>
<span id="cb147-3"><a href="classification-with-neural-networks.html#cb147-3"></a><span class="co">#&gt; [1] 2 2 1 3 0 1</span></span></code></pre></div>
<p>Since the true classes are also one-hot encoded we need to do the same to get the ground truth.</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="classification-with-neural-networks.html#cb148-1"></a>groundTruth &lt;-<span class="st"> </span><span class="kw">max.col</span>(testset<span class="op">$</span>y) <span class="op">-</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb148-2"><a href="classification-with-neural-networks.html#cb148-2"></a></span>
<span id="cb148-3"><a href="classification-with-neural-networks.html#cb148-3"></a><span class="co"># Compute accuracy.</span></span>
<span id="cb148-4"><a href="classification-with-neural-networks.html#cb148-4"></a><span class="kw">sum</span>(classes <span class="op">==</span><span class="st"> </span>groundTruth) <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(classes)</span>
<span id="cb148-5"><a href="classification-with-neural-networks.html#cb148-5"></a><span class="co">#&gt; [1] 0.8474576</span></span></code></pre></div>
<p>We can convert the integers to class strings by mapping them and then generate a confusion matrix.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="classification-with-neural-networks.html#cb149-1"></a><span class="co"># Convert classes to strings.</span></span>
<span id="cb149-2"><a href="classification-with-neural-networks.html#cb149-2"></a><span class="co"># Class mapping by index: rock 0, scissors 1, paper 2, ok 3.</span></span>
<span id="cb149-3"><a href="classification-with-neural-networks.html#cb149-3"></a>mapping &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;rock&quot;</span>, <span class="st">&quot;scissors&quot;</span>, <span class="st">&quot;paper&quot;</span>, <span class="st">&quot;ok&quot;</span>)</span>
<span id="cb149-4"><a href="classification-with-neural-networks.html#cb149-4"></a><span class="co"># Need to add 1 because indices in R start at 1.</span></span>
<span id="cb149-5"><a href="classification-with-neural-networks.html#cb149-5"></a>str.predictions &lt;-<span class="st"> </span>mapping[classes<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb149-6"><a href="classification-with-neural-networks.html#cb149-6"></a>str.groundTruth &lt;-<span class="st"> </span>mapping[groundTruth<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb149-7"><a href="classification-with-neural-networks.html#cb149-7"></a></span>
<span id="cb149-8"><a href="classification-with-neural-networks.html#cb149-8"></a><span class="kw">library</span>(caret)</span>
<span id="cb149-9"><a href="classification-with-neural-networks.html#cb149-9"></a>cm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="kw">as.factor</span>(str.predictions),</span>
<span id="cb149-10"><a href="classification-with-neural-networks.html#cb149-10"></a>                      <span class="kw">as.factor</span>(str.groundTruth))</span>
<span id="cb149-11"><a href="classification-with-neural-networks.html#cb149-11"></a>cm<span class="op">$</span>table</span>
<span id="cb149-12"><a href="classification-with-neural-networks.html#cb149-12"></a><span class="co">#&gt;           Reference</span></span>
<span id="cb149-13"><a href="classification-with-neural-networks.html#cb149-13"></a><span class="co">#&gt; Prediction  ok paper rock scissors</span></span>
<span id="cb149-14"><a href="classification-with-neural-networks.html#cb149-14"></a><span class="co">#&gt;   ok       681   118   24       27</span></span>
<span id="cb149-15"><a href="classification-with-neural-networks.html#cb149-15"></a><span class="co">#&gt;   paper     54   681   47       12</span></span>
<span id="cb149-16"><a href="classification-with-neural-networks.html#cb149-16"></a><span class="co">#&gt;   rock      29    18  771        1</span></span>
<span id="cb149-17"><a href="classification-with-neural-networks.html#cb149-17"></a><span class="co">#&gt;   scissors 134    68    8      867</span></span></code></pre></div>
<p>Try to modify the network by making it deeper (adding more layers) and fine-tune the hyperparameters like the learning rate, batch size, etc. to increase the performance.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-kirill">
<p>Yashuk, Kirill. 2019. <em>Classify Gestures by Reading Muscle Activity: A Recording of Human Hand Muscle Activity Producing Four Different Hand Gestures</em>. <a href="https://www.kaggle.com/kyr7plus/emg-4">https://www.kaggle.com/kyr7plus/emg-4</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="keras-and-tensorflow-with-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="overfitting.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
