<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8.1 Introduction to Artificial Neural Networks | Behavior Analysis with Machine Learning and R</title>
  <meta name="description" content="8.1 Introduction to Artificial Neural Networks | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="8.1 Introduction to Artificial Neural Networks | Behavior Analysis with Machine Learning and R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="8.1 Introduction to Artificial Neural Networks | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8.1 Introduction to Artificial Neural Networks | Behavior Analysis with Machine Learning and R" />
  
  <meta name="twitter:description" content="8.1 Introduction to Artificial Neural Networks | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  <meta name="twitter:image" content="images/cover.png" />

<meta name="author" content="Enrique Garcia Ceja" />


<meta name="date" content="2020-09-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="deeplearning.html"/>
<link rel="next" href="keras-and-tensorflow-with-r.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/d3-4.9.0/d3.min.js"></script>
<script src="libs/d3-tip-0.7.1/index-min.js"></script>
<link href="libs/d3panels-1.4.9/d3panels.min.css" rel="stylesheet" />
<script src="libs/d3panels-1.4.9/d3panels.min.js"></script>
<script src="libs/qtlcharts_iplotCorr-0.11.6/iplotCorr.js"></script>
<script src="libs/qtlcharts_iplotCorr-0.11.6/iplotCorr_noscat.js"></script>
<script src="libs/iplotCorr-binding-0.11.6/iplotCorr.js"></script>
<link href="libs/dygraphs-1.1.1/dygraph.css" rel="stylesheet" />
<script src="libs/dygraphs-1.1.1/dygraph-combined.js"></script>
<script src="libs/dygraphs-1.1.1/shapes.js"></script>
<script src="libs/moment-2.8.4/moment.js"></script>
<script src="libs/moment-timezone-0.2.5/moment-timezone-with-data.js"></script>
<script src="libs/moment-fquarter-1.0.0/moment-fquarter.min.js"></script>
<script src="libs/dygraphs-binding-1.1.1.6/dygraphs.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178679335-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178679335-1', { 'anonymize_ip': true });
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="#">Behavior Analysis with Machine Learning and R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#supplemental-material"><i class="fa fa-check"></i>Supplemental Material</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#conventions"><i class="fa fa-check"></i>Conventions</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.2" data-path="taxonomy.html"><a href="taxonomy.html"><i class="fa fa-check"></i><b>1.2</b> Types of Machine Learning</a></li>
<li class="chapter" data-level="1.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>1.3</b> Terminology</a><ul>
<li class="chapter" data-level="1.3.1" data-path="terminology.html"><a href="terminology.html#tables"><i class="fa fa-check"></i><b>1.3.1</b> Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="terminology.html"><a href="terminology.html#variable-types"><i class="fa fa-check"></i><b>1.3.2</b> Variable Types</a></li>
<li class="chapter" data-level="1.3.3" data-path="terminology.html"><a href="terminology.html#predictive-models"><i class="fa fa-check"></i><b>1.3.3</b> Predictive Models</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="pipeline.html"><a href="pipeline.html"><i class="fa fa-check"></i><b>1.4</b> Data Analysis Pipeline</a></li>
<li class="chapter" data-level="1.5" data-path="trainingeval.html"><a href="trainingeval.html"><i class="fa fa-check"></i><b>1.5</b> Evaluating Predictive Models</a></li>
<li class="chapter" data-level="1.6" data-path="simple-classification-example.html"><a href="simple-classification-example.html"><i class="fa fa-check"></i><b>1.6</b> Simple Classification Example</a><ul>
<li class="chapter" data-level="1.6.1" data-path="simple-classification-example.html"><a href="simple-classification-example.html#k-fold-cross-validation-example"><i class="fa fa-check"></i><b>1.6.1</b> K-fold Cross-Validation Example</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="simple-regression-example.html"><a href="simple-regression-example.html"><i class="fa fa-check"></i><b>1.7</b> Simple Regression Example</a></li>
<li class="chapter" data-level="1.8" data-path="underfitting-and-overfitting.html"><a href="underfitting-and-overfitting.html"><i class="fa fa-check"></i><b>1.8</b> Underfitting and Overfitting</a></li>
<li class="chapter" data-level="1.9" data-path="bias-and-variance.html"><a href="bias-and-variance.html"><i class="fa fa-check"></i><b>1.9</b> Bias and Variance</a></li>
<li class="chapter" data-level="1.10" data-path="SummaryIntro.html"><a href="SummaryIntro.html"><i class="fa fa-check"></i><b>1.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>2</b> Predicting Behavior with Classification Models</a><ul>
<li class="chapter" data-level="2.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html"><i class="fa fa-check"></i><b>2.1</b> <em>k</em>-nearest Neighbors</a><ul>
<li class="chapter" data-level="2.1.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#indoor-location-with-wi-fi-signals"><i class="fa fa-check"></i><b>2.1.1</b> Indoor Location with Wi-Fi Signals</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="performance-metrics.html"><a href="performance-metrics.html"><i class="fa fa-check"></i><b>2.2</b> Performance Metrics</a></li>
<li class="chapter" data-level="2.3" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>2.3</b> Decision Trees</a><ul>
<li class="chapter" data-level="2.3.1" data-path="decision-trees.html"><a href="decision-trees.html#activityRecognition"><i class="fa fa-check"></i><b>2.3.1</b> Activity Recognition with Smartphones</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="dynamic-time-warping.html"><a href="dynamic-time-warping.html"><i class="fa fa-check"></i><b>2.4</b> Dynamic Time Warping</a><ul>
<li class="chapter" data-level="2.4.1" data-path="dynamic-time-warping.html"><a href="dynamic-time-warping.html#sechandgestures"><i class="fa fa-check"></i><b>2.4.1</b> Hand Gesture Recognition</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="summaryClassification.html"><a href="summaryClassification.html"><i class="fa fa-check"></i><b>2.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>3</b> Predicting Behavior with Ensemble Learning</a><ul>
<li class="chapter" data-level="3.1" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>3.1</b> Bagging</a><ul>
<li class="chapter" data-level="3.1.1" data-path="bagging.html"><a href="bagging.html#activity-recognition-with-bagging"><i class="fa fa-check"></i><b>3.1.1</b> Activity recognition with Bagging</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="random-forest.html"><a href="random-forest.html"><i class="fa fa-check"></i><b>3.2</b> Random Forest</a></li>
<li class="chapter" data-level="3.3" data-path="stacked-generalization.html"><a href="stacked-generalization.html"><i class="fa fa-check"></i><b>3.3</b> Stacked Generalization</a></li>
<li class="chapter" data-level="3.4" data-path="multiviewhometasks.html"><a href="multiviewhometasks.html"><i class="fa fa-check"></i><b>3.4</b> Multi-view Stacking for Home Tasks Recognition</a></li>
<li class="chapter" data-level="3.5" data-path="SummaryEnsemble.html"><a href="SummaryEnsemble.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="edavis.html"><a href="edavis.html"><i class="fa fa-check"></i><b>4</b> Exploring and Visualizing Behavioral Data</a><ul>
<li class="chapter" data-level="4.1" data-path="talking-with-field-experts.html"><a href="talking-with-field-experts.html"><i class="fa fa-check"></i><b>4.1</b> Talking with Field Experts</a></li>
<li class="chapter" data-level="4.2" data-path="summary-statistics.html"><a href="summary-statistics.html"><i class="fa fa-check"></i><b>4.2</b> Summary Statistics</a></li>
<li class="chapter" data-level="4.3" data-path="class-distributions.html"><a href="class-distributions.html"><i class="fa fa-check"></i><b>4.3</b> Class Distributions</a></li>
<li class="chapter" data-level="4.4" data-path="user-class-sparsity-matrix.html"><a href="user-class-sparsity-matrix.html"><i class="fa fa-check"></i><b>4.4</b> User-Class Sparsity Matrix</a></li>
<li class="chapter" data-level="4.5" data-path="boxplots.html"><a href="boxplots.html"><i class="fa fa-check"></i><b>4.5</b> Boxplots</a></li>
<li class="chapter" data-level="4.6" data-path="correlation-plots.html"><a href="correlation-plots.html"><i class="fa fa-check"></i><b>4.6</b> Correlation Plots</a><ul>
<li class="chapter" data-level="4.6.1" data-path="correlation-plots.html"><a href="correlation-plots.html#interactive-correlation-plots"><i class="fa fa-check"></i><b>4.6.1</b> Interactive Correlation Plots</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="timeseries.html"><a href="timeseries.html"><i class="fa fa-check"></i><b>4.7</b> Timeseries</a><ul>
<li class="chapter" data-level="4.7.1" data-path="timeseries.html"><a href="timeseries.html#interactive-timeseries"><i class="fa fa-check"></i><b>4.7.1</b> Interactive Timeseries</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="multidimensional-scaling-mds.html"><a href="multidimensional-scaling-mds.html"><i class="fa fa-check"></i><b>4.8</b> Multidimensional Scaling (MDS)</a></li>
<li class="chapter" data-level="4.9" data-path="heatmaps.html"><a href="heatmaps.html"><i class="fa fa-check"></i><b>4.9</b> Heatmaps</a></li>
<li class="chapter" data-level="4.10" data-path="automated-eda.html"><a href="automated-eda.html"><i class="fa fa-check"></i><b>4.10</b> Automated EDA</a></li>
<li class="chapter" data-level="4.11" data-path="SummaryExploratory.html"><a href="SummaryExploratory.html"><i class="fa fa-check"></i><b>4.11</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="preprocessing.html"><a href="preprocessing.html"><i class="fa fa-check"></i><b>5</b> Preprocessing Behavioral Data</a><ul>
<li class="chapter" data-level="5.1" data-path="missing-values.html"><a href="missing-values.html"><i class="fa fa-check"></i><b>5.1</b> Missing Values</a><ul>
<li class="chapter" data-level="5.1.1" data-path="missing-values.html"><a href="missing-values.html#imputation"><i class="fa fa-check"></i><b>5.1.1</b> Imputation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>5.2</b> Smoothing</a></li>
<li class="chapter" data-level="5.3" data-path="normalization.html"><a href="normalization.html"><i class="fa fa-check"></i><b>5.3</b> Normalization</a></li>
<li class="chapter" data-level="5.4" data-path="imbalanced-classes.html"><a href="imbalanced-classes.html"><i class="fa fa-check"></i><b>5.4</b> Imbalanced Classes</a><ul>
<li class="chapter" data-level="5.4.1" data-path="imbalanced-classes.html"><a href="imbalanced-classes.html#random-oversampling"><i class="fa fa-check"></i><b>5.4.1</b> Random Oversampling</a></li>
<li class="chapter" data-level="5.4.2" data-path="imbalanced-classes.html"><a href="imbalanced-classes.html#smote"><i class="fa fa-check"></i><b>5.4.2</b> SMOTE</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="infoinjection.html"><a href="infoinjection.html"><i class="fa fa-check"></i><b>5.5</b> Information Injection</a></li>
<li class="chapter" data-level="5.6" data-path="one-hot-encoding.html"><a href="one-hot-encoding.html"><i class="fa fa-check"></i><b>5.6</b> One-hot Encoding</a></li>
<li class="chapter" data-level="5.7" data-path="SummaryPreprocessing.html"><a href="SummaryPreprocessing.html"><i class="fa fa-check"></i><b>5.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="unsupervised.html"><a href="unsupervised.html"><i class="fa fa-check"></i><b>6</b> Discovering Behaviors with Unsupervised Learning</a><ul>
<li class="chapter" data-level="6.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>6.1</b> K-means clustering</a><ul>
<li class="chapter" data-level="6.1.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#studentresponses"><i class="fa fa-check"></i><b>6.1.1</b> Grouping Student Responses</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="the-silhouette-index.html"><a href="the-silhouette-index.html"><i class="fa fa-check"></i><b>6.2</b> The Silhouette Index</a></li>
<li class="chapter" data-level="6.3" data-path="associationrules.html"><a href="associationrules.html"><i class="fa fa-check"></i><b>6.3</b> Mining Association Rules</a><ul>
<li class="chapter" data-level="6.3.1" data-path="associationrules.html"><a href="associationrules.html#finding-rules-for-criminal-behavior"><i class="fa fa-check"></i><b>6.3.1</b> Finding Rules for Criminal Behavior</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="SummaryUnsupervised.html"><a href="SummaryUnsupervised.html"><i class="fa fa-check"></i><b>6.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="representations.html"><a href="representations.html"><i class="fa fa-check"></i><b>7</b> Encoding Behavioral Data</a><ul>
<li class="chapter" data-level="7.1" data-path="feature-vectors.html"><a href="feature-vectors.html"><i class="fa fa-check"></i><b>7.1</b> Feature Vectors</a></li>
<li class="chapter" data-level="7.2" data-path="sectimeseries.html"><a href="sectimeseries.html"><i class="fa fa-check"></i><b>7.2</b> Timeseries</a></li>
<li class="chapter" data-level="7.3" data-path="transactions.html"><a href="transactions.html"><i class="fa fa-check"></i><b>7.3</b> Transactions</a></li>
<li class="chapter" data-level="7.4" data-path="images.html"><a href="images.html"><i class="fa fa-check"></i><b>7.4</b> Images</a></li>
<li class="chapter" data-level="7.5" data-path="recurrence-plots.html"><a href="recurrence-plots.html"><i class="fa fa-check"></i><b>7.5</b> Recurrence Plots</a><ul>
<li class="chapter" data-level="7.5.1" data-path="recurrence-plots.html"><a href="recurrence-plots.html#computing-recurence-plots"><i class="fa fa-check"></i><b>7.5.1</b> Computing Recurence Plots</a></li>
<li class="chapter" data-level="7.5.2" data-path="recurrence-plots.html"><a href="recurrence-plots.html#recurrence-plots-of-hand-gestures"><i class="fa fa-check"></i><b>7.5.2</b> Recurrence Plots of Hand Gestures</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="bag-of-words.html"><a href="bag-of-words.html"><i class="fa fa-check"></i><b>7.6</b> Bag-of-Words</a><ul>
<li class="chapter" data-level="7.6.1" data-path="bag-of-words.html"><a href="bag-of-words.html#bow-for-complex-activities."><i class="fa fa-check"></i><b>7.6.1</b> BoW for Complex Activities.</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="graphs.html"><a href="graphs.html"><i class="fa fa-check"></i><b>7.7</b> Graphs</a><ul>
<li class="chapter" data-level="7.7.1" data-path="graphs.html"><a href="graphs.html#complex-activities-as-graphs"><i class="fa fa-check"></i><b>7.7.1</b> Complex Activities as Graphs</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="SummaryRepresentations.html"><a href="SummaryRepresentations.html"><i class="fa fa-check"></i><b>7.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deeplearning.html"><a href="deeplearning.html"><i class="fa fa-check"></i><b>8</b> Predicting Behavior with Deep Learning</a><ul>
<li class="chapter" data-level="8.1" data-path="ann.html"><a href="ann.html"><i class="fa fa-check"></i><b>8.1</b> Introduction to Artificial Neural Networks</a><ul>
<li class="chapter" data-level="8.1.1" data-path="ann.html"><a href="ann.html#sigmoid-and-relu-units"><i class="fa fa-check"></i><b>8.1.1</b> Sigmoid and ReLU Units</a></li>
<li class="chapter" data-level="8.1.2" data-path="ann.html"><a href="ann.html#assembling-units-into-layers"><i class="fa fa-check"></i><b>8.1.2</b> Assembling Units into Layers</a></li>
<li class="chapter" data-level="8.1.3" data-path="ann.html"><a href="ann.html#deep-neural-networks"><i class="fa fa-check"></i><b>8.1.3</b> Deep Neural Networks</a></li>
<li class="chapter" data-level="8.1.4" data-path="ann.html"><a href="ann.html#learning-the-parameters"><i class="fa fa-check"></i><b>8.1.4</b> Learning the Parameters</a></li>
<li class="chapter" data-level="8.1.5" data-path="ann.html"><a href="ann.html#parameter-learning-example-in-r"><i class="fa fa-check"></i><b>8.1.5</b> Parameter Learning Example in R</a></li>
<li class="chapter" data-level="8.1.6" data-path="ann.html"><a href="ann.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>8.1.6</b> Stochastic Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="keras-and-tensorflow-with-r.html"><a href="keras-and-tensorflow-with-r.html"><i class="fa fa-check"></i><b>8.2</b> Keras and TensorFlow with R</a><ul>
<li class="chapter" data-level="8.2.1" data-path="keras-and-tensorflow-with-r.html"><a href="keras-and-tensorflow-with-r.html#keras-example"><i class="fa fa-check"></i><b>8.2.1</b> Keras Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="classification-with-neural-networks.html"><a href="classification-with-neural-networks.html"><i class="fa fa-check"></i><b>8.3</b> Classification with Neural Networks</a><ul>
<li class="chapter" data-level="8.3.1" data-path="classification-with-neural-networks.html"><a href="classification-with-neural-networks.html#classification-of-electromyography-signals"><i class="fa fa-check"></i><b>8.3.1</b> Classification of Electromyography Signals</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>8.4</b> Overfitting</a><ul>
<li class="chapter" data-level="8.4.1" data-path="overfitting.html"><a href="overfitting.html#early-stopping"><i class="fa fa-check"></i><b>8.4.1</b> Early Stopping</a></li>
<li class="chapter" data-level="8.4.2" data-path="overfitting.html"><a href="overfitting.html#dropout"><i class="fa fa-check"></i><b>8.4.2</b> Dropout</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="fine-tuning-a-neural-network.html"><a href="fine-tuning-a-neural-network.html"><i class="fa fa-check"></i><b>8.5</b> Fine-Tuning a Neural Network</a></li>
<li class="chapter" data-level="8.6" data-path="cnns.html"><a href="cnns.html"><i class="fa fa-check"></i><b>8.6</b> Convolutional Neural Networks</a><ul>
<li class="chapter" data-level="8.6.1" data-path="cnns.html"><a href="cnns.html#convolutions"><i class="fa fa-check"></i><b>8.6.1</b> Convolutions</a></li>
<li class="chapter" data-level="8.6.2" data-path="cnns.html"><a href="cnns.html#pooling-operations"><i class="fa fa-check"></i><b>8.6.2</b> Pooling Operations</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="cnns-with-keras.html"><a href="cnns-with-keras.html"><i class="fa fa-check"></i><b>8.7</b> CNNs with Keras</a><ul>
<li class="chapter" data-level="8.7.1" data-path="cnns-with-keras.html"><a href="cnns-with-keras.html#example-1"><i class="fa fa-check"></i><b>8.7.1</b> Example 1</a></li>
<li class="chapter" data-level="8.7.2" data-path="cnns-with-keras.html"><a href="cnns-with-keras.html#example-2"><i class="fa fa-check"></i><b>8.7.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="cnnSmile.html"><a href="cnnSmile.html"><i class="fa fa-check"></i><b>8.8</b> Smiles Detection with a CNN</a></li>
<li class="chapter" data-level="8.9" data-path="SummaryDeepLearning.html"><a href="SummaryDeepLearning.html"><i class="fa fa-check"></i><b>8.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multiuser.html"><a href="multiuser.html"><i class="fa fa-check"></i><b>9</b> Multi-User Validation</a><ul>
<li class="chapter" data-level="9.1" data-path="mixed-models.html"><a href="mixed-models.html"><i class="fa fa-check"></i><b>9.1</b> Mixed Models</a><ul>
<li class="chapter" data-level="9.1.1" data-path="mixed-models.html"><a href="mixed-models.html#skeleton-action-recognition-with-mixed-models"><i class="fa fa-check"></i><b>9.1.1</b> Skeleton Action Recognition with Mixed Models</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="user-independent-models.html"><a href="user-independent-models.html"><i class="fa fa-check"></i><b>9.2</b> User-Independent Models</a></li>
<li class="chapter" data-level="9.3" data-path="user-dependent-models.html"><a href="user-dependent-models.html"><i class="fa fa-check"></i><b>9.3</b> User-Dependent Models</a></li>
<li class="chapter" data-level="9.4" data-path="user-adaptive-models.html"><a href="user-adaptive-models.html"><i class="fa fa-check"></i><b>9.4</b> User-Adaptive Models</a><ul>
<li class="chapter" data-level="9.4.1" data-path="user-adaptive-models.html"><a href="user-adaptive-models.html#transfer-learning"><i class="fa fa-check"></i><b>9.4.1</b> Transfer Learning</a></li>
<li class="chapter" data-level="9.4.2" data-path="user-adaptive-models.html"><a href="user-adaptive-models.html#a-user-adaptive-model-for-activity-recognition"><i class="fa fa-check"></i><b>9.4.2</b> A User-Adaptive Model for Activity Recognition</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="SummaryMultiUser.html"><a href="SummaryMultiUser.html"><i class="fa fa-check"></i><b>9.5</b> Summary</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendixInstall.html"><a href="appendixInstall.html"><i class="fa fa-check"></i><b>A</b> Setup Your Environment</a><ul>
<li class="chapter" data-level="A.1" data-path="installing-the-datasets.html"><a href="installing-the-datasets.html"><i class="fa fa-check"></i><b>A.1</b> Installing the Datasets</a></li>
<li class="chapter" data-level="A.2" data-path="installing-the-examples-source-code.html"><a href="installing-the-examples-source-code.html"><i class="fa fa-check"></i><b>A.2</b> Installing the Examples Source Code</a></li>
<li class="chapter" data-level="A.3" data-path="installing-keras-and-tensorflow-.html"><a href="installing-keras-and-tensorflow-.html"><i class="fa fa-check"></i><b>A.3</b> Installing Keras and TensorFlow.</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendixDatasets.html"><a href="appendixDatasets.html"><i class="fa fa-check"></i><b>B</b> Datasets</a><ul>
<li class="chapter" data-level="B.1" data-path="complex-activities.html"><a href="complex-activities.html"><i class="fa fa-check"></i><b>B.1</b> COMPLEX ACTIVITIES</a></li>
<li class="chapter" data-level="B.2" data-path="depresjon.html"><a href="depresjon.html"><i class="fa fa-check"></i><b>B.2</b> DEPRESJON</a></li>
<li class="chapter" data-level="B.3" data-path="electromyography.html"><a href="electromyography.html"><i class="fa fa-check"></i><b>B.3</b> ELECTROMYOGRAPHY</a></li>
<li class="chapter" data-level="B.4" data-path="hand-gestures.html"><a href="hand-gestures.html"><i class="fa fa-check"></i><b>B.4</b> HAND GESTURES</a></li>
<li class="chapter" data-level="B.5" data-path="home-tasks.html"><a href="home-tasks.html"><i class="fa fa-check"></i><b>B.5</b> HOME TASKS</a></li>
<li class="chapter" data-level="B.6" data-path="homicide-reports.html"><a href="homicide-reports.html"><i class="fa fa-check"></i><b>B.6</b> HOMICIDE REPORTS</a></li>
<li class="chapter" data-level="B.7" data-path="indoor-location.html"><a href="indoor-location.html"><i class="fa fa-check"></i><b>B.7</b> INDOOR LOCATION</a></li>
<li class="chapter" data-level="B.8" data-path="sheep-goats.html"><a href="sheep-goats.html"><i class="fa fa-check"></i><b>B.8</b> SHEEP GOATS</a></li>
<li class="chapter" data-level="B.9" data-path="skeleton-actions.html"><a href="skeleton-actions.html"><i class="fa fa-check"></i><b>B.9</b> SKELETON ACTIONS</a></li>
<li class="chapter" data-level="B.10" data-path="smartphone-activities.html"><a href="smartphone-activities.html"><i class="fa fa-check"></i><b>B.10</b> SMARTPHONE ACTIVITIES</a></li>
<li class="chapter" data-level="B.11" data-path="smiles.html"><a href="smiles.html"><i class="fa fa-check"></i><b>B.11</b> SMILES</a></li>
<li class="chapter" data-level="B.12" data-path="students-mental-health.html"><a href="students-mental-health.html"><i class="fa fa-check"></i><b>B.12</b> STUDENTS’ MENTAL HEALTH</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="citing-this-book.html"><a href="citing-this-book.html"><i class="fa fa-check"></i>Citing this Book</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Behavior Analysis with Machine Learning and R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ann" class="section level2">
<h2><span class="header-section-number">8.1</span> Introduction to Artificial Neural Networks</h2>
<p>Artificial neural networks (ANNs) are mathematical models <em>inspired</em> by the brain. Here, I would like to emphasize the word <em>inspired</em> because ANNs do not model how a biological brain actually works. In fact, there is little knowledge about how the the brain works. ANNs are composed of <strong>units</strong> (also called <strong>neurons</strong> or <strong>nodes</strong>) and connections between units. Each unit can receive inputs from other units. Those inputs are processed inside the unit and produce an output. Typically, units are arranged into layers (as we will see later) and connections between units have an associated weight. Those weights are learned during training and they are the core elements that make a network behave in a certain way.</p>

<div class="rmdinfo">
For the rest of the chapter, I will mostly use the term <strong>units</strong> to refer to neurons/nodes. From time to time, I will use the term <strong>network</strong> to refer to artificial neural networks.
</div>

<p>Before going into details of how multi-layer ANNs work, let’s start with a very simple neural network consisting of a <strong>single unit</strong>. See Figure <a href="ann.html#fig:nnPerceptron">8.1</a>. Even though this network only has one node, it is already composed of several interesting elements which are the basis of more complex networks. First, it has <span class="math inline">\(n\)</span> input variables <span class="math inline">\(x_1 \ldots x_n\)</span> which are real numbers. Second, the unit has a set of <span class="math inline">\(n\)</span> weights <span class="math inline">\(w_1 \ldots w_n\)</span> associated with each input. These weights can take real numbers as values. Finally, there is an output <span class="math inline">\(y&#39;\)</span> which is binary (it can take two values: <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span>).</p>
<div class="figure" style="text-align: center"><span id="fig:nnPerceptron"></span>
<img src="images/nn_perceptron.png" alt="A neural network composed of a single unit (perceptron)." width="50%" />
<p class="caption">
Figure 8.1: A neural network composed of a single unit (perceptron).
</p>
</div>

<div class="rmdinfo">
This simple network consisting of one unit with a binary output is called a <strong>perceptron</strong> and was proposed by <span class="citation">Rosenblatt (<a href="#ref-rosenblatt1958" role="doc-biblioref">1958</a>)</span>.
</div>

<p>This single unit also known as <em>perceptron</em> is capable of making binary decisions based on the input and the weights. To get the final decision <span class="math inline">\(y&#39;\)</span> the inputs are multiplied by their corresponding weights and the results are summed. If the sum is greater than a given threshold, then the output is <span class="math inline">\(1\)</span> and <span class="math inline">\(0\)</span> otherwise. Formally:</p>
<p><span class="math display" id="eq:perceptron">\[\begin{equation}
  y&#39; =
 \begin{cases}
  1 &amp; \textit{if } \sum_{i}{w_i x_i &gt; t}, \\
  0 &amp; \textit{if } \sum_{i}{w_i x_i \leq t}
 \end{cases}
  \tag{8.1}
\end{equation}\]</span></p>
<p>We can use a perceptron to make important decisions in life. For example, suppose you need to decide whether or not to go to the movies. Assume this decision is based on two pieces of information:</p>
<ol style="list-style-type: decimal">
<li>You have money to pay the entrance (or not) and,</li>
<li>it is a horror movie (or not).</li>
</ol>
<p>There are two additional assumptions as well:</p>
<ol style="list-style-type: decimal">
<li>The movie theater only projects <span class="math inline">\(1\)</span> film.</li>
<li>You don’t like horror movies.</li>
</ol>
<p>This decision-making process can be modeled with the perceptron of Figure <a href="ann.html#fig:nnMovies">8.2</a>. This perceptron has two binary input variables: <em>money</em> and <em>horror</em>. Each variable has an associated weight. Suppose there is a decision threshold of <span class="math inline">\(t=3\)</span>. Finally, there is a binary output: <span class="math inline">\(1\)</span> means you should go to the movies and <span class="math inline">\(0\)</span> indicates that you should not go.</p>

<div class="rmdinfo">
In this example, the weights (<span class="math inline">\(5\)</span> and <span class="math inline">\(-3\)</span>) and the threshold <span class="math inline">\(t=3\)</span> were already provided. The weights and the threshold are called the <em>parameters</em> of the network. Later, we will see how the parameters can be learned automatically from data.
</div>

<div class="figure" style="text-align: center"><span id="fig:nnMovies"></span>
<img src="images/nn_movies.png" alt="Perceptron to decide whether or not to go to the movies based on two input variables." width="50%" />
<p class="caption">
Figure 8.2: Perceptron to decide whether or not to go to the movies based on two input variables.
</p>
</div>
<p>Suppose that today was payday and the theater is projecting an action movie. Then, we can set the input variables <span class="math inline">\(money=1\)</span> and <span class="math inline">\(horror=0\)</span>. Now we want to decide if we should go to the movie theater or not. To get the final answer we can use Equation <a href="ann.html#eq:perceptron">(8.1)</a>. This formula tells us that we need to multiply each input variable with their corresponding weights and add them:</p>
<p><span class="math display">\[\begin{align*}
(money)(5) + (horror)(-3)
\end{align*}\]</span></p>
<p>Substituting <em>money</em> and <em>horror</em> with their corresponding values:</p>
<p><span class="math display">\[\begin{align*}
(1)(5) + (0)(-3) = 5
\end{align*}\]</span></p>
<p>Since <span class="math inline">\(5 &gt; t\)</span> (remember the threshold <span class="math inline">\(t=3\)</span>), the final output will be <span class="math inline">\(1\)</span>, thus, the advice is to go to the movies. Let’s try the scenario when you have money but they are projecting a horror movie: <span class="math inline">\(money=1\)</span>, <span class="math inline">\(horror=1\)</span>.</p>
<p><span class="math display">\[\begin{align*}
(1)(5) + (1)(-3) = 2
\end{align*}\]</span></p>
<p>In this case, <span class="math inline">\(2 &lt; t\)</span> and the final output is <span class="math inline">\(0\)</span>. Even if you have money, you should not waste it on a movie that you know you most likely will not like. This process of applying operations to the inputs and obtaining the final result is called <strong>forward propagation</strong> because the inputs are ‘pushed’ all the way through the network (a single perceptron in this case). For bigger networks, the outputs of the current layer become the inputs of the next layer, and so on.</p>
<p>For convenience, a simplified version of Equation <a href="ann.html#eq:perceptron">(8.1)</a> can be used. This alternative representation is useful because it provides flexibility to change the internals of the units (neurons) as we will see. The first simplification consists of representing the inputs and weights as vectors:</p>
<p><span class="math display">\[\begin{equation}
  \sum_{i}{w_i x_i} = \boldsymbol{w} \cdot \boldsymbol{x}
\end{equation}\]</span></p>
<p>The summation becomes a dot product between <span class="math inline">\(\boldsymbol{w}\)</span> and <span class="math inline">\(\boldsymbol{x}\)</span>. Next, the threshold <span class="math inline">\(t\)</span> can be moved to the left and renamed to <span class="math inline">\(b\)</span> which stands for <strong>bias</strong>. This is only notation but you can still think of the <em>bias</em> as a threshold.</p>
<p><span class="math display">\[\begin{equation}
  y&#39; = f(\boldsymbol{x}) =
 \begin{cases}
  1 &amp; \textit{if } \boldsymbol{w} \cdot \boldsymbol{x} + b &gt; 0, \\
  0 &amp; \textit{otherwise}
 \end{cases}
\end{equation}\]</span></p>
<p>The output <span class="math inline">\(y&#39;\)</span> is a function of <span class="math inline">\(\boldsymbol{x}\)</span> with <span class="math inline">\(\boldsymbol{w}\)</span> and <span class="math inline">\(b\)</span> as fixed parameters. One thing to note is that first, we are performing the operation <span class="math inline">\(\boldsymbol{w} \cdot \boldsymbol{x} + b\)</span> and then, another operation is applied to the result. In this case, it is a comparison. If the result is greater than <span class="math inline">\(0\)</span> the final output is <span class="math inline">\(1\)</span>. You can think of this second operation as another function. Call it <span class="math inline">\(g(x)\)</span>.</p>
<p><span class="math display" id="eq:nnUnit">\[\begin{equation}
  f(\boldsymbol{x}) = g(\boldsymbol{w} \cdot \boldsymbol{x} + b)
  \tag{8.2}
\end{equation}\]</span></p>
<p>In neural networks terminology, this <span class="math inline">\(g(x)\)</span> is known as the <strong>activation function</strong>. Its result indicates how much active this unit is based on its inputs. If the result is <span class="math inline">\(1\)</span>, it means that this unit is active. If the result is <span class="math inline">\(0\)</span>, it means the unit is inactive.</p>
<p>This new notation allows us to use different activation functions by substituting <span class="math inline">\(g(x)\)</span> with some other function in Equation <a href="ann.html#eq:nnUnit">(8.2)</a>. In the case of the perceptron, the activation function <span class="math inline">\(g(x)\)</span> is the threshold function, which is known as the <em>step function</em>:</p>
<p><span class="math display" id="eq:stepfunc">\[\begin{equation}
  g(x) = step(x) =
 \begin{cases}
  1 &amp; \textit{if } x &gt; 0 \\
  0 &amp; \textit{if } x \leq 0
 \end{cases}
  \tag{8.3}
\end{equation}\]</span></p>
<p>Figure <a href="ann.html#fig:nnStep">8.3</a> shows the plot of the step function.</p>
<div class="figure" style="text-align: center"><span id="fig:nnStep"></span>
<img src="images/nn_step.png" alt="The step function." width="100%" />
<p class="caption">
Figure 8.3: The step function.
</p>
</div>
<p>It is worth noting that perceptrons have two major limitations:</p>
<ol style="list-style-type: decimal">
<li>The output is binary.</li>
<li>Perceptrons are linear functions.</li>
</ol>
<p>The first limitation imposes some restrictions on its applicability. For example, a perceptron cannot be used to predict real-valued outputs which is a fundamental aspect for regression problems. The second limitation makes the perceptron only capable of solving linear problems. Figure <a href="ann.html#fig:nnLinearity">8.4</a> graphically shows this limitation. In the first case, the outputs of the OR logical operator can be classified (separated) using a line. On the other hand, it is not possible to classify the output of the XOR function using a single line.</p>
<div class="figure" style="text-align: center"><span id="fig:nnLinearity"></span>
<img src="images/nn_linearity.png" alt="OR and the XOR logical operators." width="80%" />
<p class="caption">
Figure 8.4: OR and the XOR logical operators.
</p>
</div>
<p>To overcome those limitations, several modifications to the perceptron were introduced. This allows us to build models capable of solving more complex non-linear problems. One such modification is to change the activation function. Another improvement is to add the ability to have several layers of interconnected units. In the next section, two new types of units will be presented. Then, the following section will introduce neural networks also known as multilayer perceptrons which are more complex models built by connecting many units and arranging them into layers.</p>
<div id="sigmoid-and-relu-units" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Sigmoid and ReLU Units</h3>
<p>As previously mentioned, perceptrons have some limitations that restrict their applicability including the fact that they are linear models. In practice, problems are complex and most of them are non-linear. One way to overcome this limitation is to introduce non-linearities and this can be done by using a different type of activation function. Remember that a unit can be modeled as <span class="math inline">\(f(x) = g(wx+b)\)</span> where <span class="math inline">\(g(x)\)</span> is some activation function. For the perceptron, <span class="math inline">\(g(x)\)</span> is the <em>step function</em>. However, another practical limitation not mentioned before is that the step function can change abruptly from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span> and vice versa. Small changes in <span class="math inline">\(x\)</span>,<span class="math inline">\(w\)</span>, or <span class="math inline">\(b\)</span> can completely change the output. This is a problem during learning and inference time. Instead, we would prefer a smooth version of the step function, for example, the <strong>sigmoid function</strong> which is also known as the <strong>logistic function</strong>:</p>
<p><span class="math display" id="eq:sigmoidfunct">\[\begin{equation}
  s(x) = \frac{1}{1 + e^{-x}}
  \tag{8.4}
\end{equation}\]</span></p>
<p>This function has an ‘S’ shape (Figure <a href="ann.html#fig:nnSigmoid">8.5</a>) and as opposed to a step function, this one is smooth. The range of this function is from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:nnSigmoid"></span>
<img src="images/nn_sigmoid.png" alt="Sigmoid function." width="100%" />
<p class="caption">
Figure 8.5: Sigmoid function.
</p>
</div>
<p>If we substitute the activation function in Equation <a href="ann.html#eq:nnUnit">(8.2)</a> with the sigmoid function we get our <strong>sigmoid unit</strong>:</p>
<p><span class="math display" id="eq:sigmoidunit">\[\begin{equation}
  f(x) = \frac{1}{1 + e^{-(w \cdot x + b)}}
  \tag{8.5}
\end{equation}\]</span></p>
<p>Sigmoid units have been one of the most commonly used types of units when building bigger neural networks. Another advantage is that the outputs are real values that can be interpreted as probabilities. For instance, if we want to make binary decisions we can set a threshold. For example, if the output of the sigmoid unit is <span class="math inline">\(&gt; 0.5\)</span> then return a <span class="math inline">\(1\)</span>. Of course, that threshold would depend on the application. If we need more confidence about the result we can set a higher threshold.</p>
<p>In the last years, another type of unit has been successfully applied to train neural networks, the <strong>rectified linear unit</strong> or <strong>ReLU</strong> for short. The activation function of this unit is the rectifier function:</p>
<p><span class="math display" id="eq:rectifierfunct">\[\begin{equation}
  rectifier(x) =
 \begin{cases}
  0 &amp; \textit{if } x &lt; 0, \\
  x &amp; \textit{if } x \geq 0
 \end{cases}
  \tag{8.6}
\end{equation}\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:nnRectified"></span>
<img src="images/nn_relu.png" alt="Rectifier function." width="100%" />
<p class="caption">
Figure 8.6: Rectifier function.
</p>
</div>
<p>This one is also called the <em>ramp function</em> and is one of the simplest non-linear functions and probably the most common one used in modern big neural networks. These units present several advantages, being among them, efficiency during training and inference time.</p>

<div class="rmdinfo">
In practice, many other activation functions are used but the most common ones are sigmoid and ReLU units. In the following link, you can find an extensive list of activation functions: <a href="https://en.wikipedia.org/wiki/Activation_function" class="uri">https://en.wikipedia.org/wiki/Activation_function</a>
</div>

<p>So far, we have been talking about <strong>single units</strong>. In the next section, we will see how these single units can be assembled to build bigger artificial neural networks.</p>
</div>
<div id="assembling-units-into-layers" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Assembling Units into Layers</h3>
<p>Perceptrons, sigmoid and ReLU units can be thought of as very simple neural networks. By connecting several units, one can build more complex neural networks. For historical reasons, neural networks are also called <strong>multilayer perceptrons</strong> regardless if the units are perceptrons or not. Typically, units are grouped into layers. Figure <a href="ann.html#fig:nnExampleNN">8.7</a> shows an example neural network with <span class="math inline">\(3\)</span> layers. An <strong>input layer</strong> with <span class="math inline">\(3\)</span> nodes, a <strong>hidden layer</strong> with <span class="math inline">\(2\)</span> units and an <strong>output layer</strong> with <span class="math inline">\(1\)</span> unit.</p>
<div class="figure" style="text-align: center"><span id="fig:nnExampleNN"></span>
<img src="images/nn_example_nn.png" alt="Example neural network." width="50%" />
<p class="caption">
Figure 8.7: Example neural network.
</p>
</div>

<div class="rmdcaution">
In this type of diagram, the nodes represent units (perceptrons, sigmoids, ReLUs, etc.) except for the input layer. In the input layer, nodes represent input variables (input features). In the above example, the <span class="math inline">\(3\)</span> nodes in the input layer simply indicate that the network takes as input <span class="math inline">\(3\)</span> variables. In this layer, no operations are performed.
</div>

<p>This network only has one hidden layer. Hidden layers are called like that because they do not have direct contact with the external world. Finally, there is an output layer with a single unit. We could also have an output layer with more than one unit. Most of the time, we will have <strong>fully connected</strong> neural networks. That is, all units have incoming connections from all nodes in the previous layer (as in the previous example).</p>

<div class="rmdinfo">
For each specific problem, we need to define several building blocks for the network. For example, the number of layers, the number of units in each layer, the type of units (sigmoid, ReLU, etc.), and so on. This is known as the <strong>architecture</strong> of the network. Choosing a good architecture for a given problem is not a trivial task. It is advised to start with an architecture that was used to solve a similar problem and then fine-tune it for your specific problem. There exist some automatic ways to optimize the network architecture but those methods are out of the scope of this book.
</div>

<p>We already saw how a unit can produce a result based on the inputs by using <em>forward propagation</em>. For more complex networks the process is the same! Consider the network shown in Figure <a href="ann.html#fig:nnForward">8.8</a>. It consists of two inputs and one output. It also has one hidden layer with <span class="math inline">\(2\)</span> units.</p>
<div class="figure" style="text-align: center"><span id="fig:nnForward"></span>
<img src="images/nn_forward.png" alt="Example of forward propagation." width="50%" />
<p class="caption">
Figure 8.8: Example of forward propagation.
</p>
</div>
<p>Each node is labeled as <span class="math inline">\(n_{l,n}\)</span> where <span class="math inline">\(l\)</span> is the layer and <span class="math inline">\(n\)</span> is the unit number.
The two input values are <span class="math inline">\(1\)</span> and <span class="math inline">\(0.5\)</span>. They could be temperature measurements, for example. Each edge has an associated weight. For simplicity, let’s assume that the activation function of the units is the identity function <span class="math inline">\(g(x)=x\)</span>. The bold underlined number inside the nodes of the hidden and output layers are the biases. Here we assume that the network is already trained (later we will see how those weights and biases are learned). To get the final result, for each node, its inputs are multiplied by their corresponding weights and added. Then, the bias is added. Next, the activation function is applied. In this case, it is just the identify function (returns the same value). The outputs of the nodes in the hidden layer become the inputs of the next layer and so on.</p>
<p>In this example, first we need to compute the outputs of nodes <span class="math inline">\(n_{2,1}\)</span> and <span class="math inline">\(n_{2,2}\)</span>:</p>
<p>output of <span class="math inline">\(n_{2,1} = (1)(2) + (0.5)(1) + 1 = 3.5\)</span></p>
<p>output of <span class="math inline">\(n_{2,2} = (1)(-3) + (0.5)(5) + 0 = -0.5\)</span></p>
<p>Finally, we can compute the output of the last node using the outputs of the previous nodes:</p>
<p>output of <span class="math inline">\(n_{3,1} = (3.5)(1) + (-0.5)(-1) + 3 = 7\)</span>.</p>
</div>
<div id="deep-neural-networks" class="section level3">
<h3><span class="header-section-number">8.1.3</span> Deep Neural Networks</h3>
<p>By increasing the number of layers and the number of units in each layer, one can build more complex networks. But what is a deep neural network (DNN)? It is not a strict rule but some people say that a network with more than <span class="math inline">\(2\)</span> hidden layers is a deep network. Yes, that’s all it takes to build a DNN! Figure <a href="ann.html#fig:nnDNN">8.9</a> shows an example of a deep neural network.</p>
<div class="figure" style="text-align: center"><span id="fig:nnDNN"></span>
<img src="images/nn_dnn.png" alt="Example of a deep neural network." width="50%" />
<p class="caption">
Figure 8.9: Example of a deep neural network.
</p>
</div>
<p>A DNN has nothing special compared to a traditional neural network except that it has many layers. One of the reasons why they became so popular until recent years is because before it was not possible to efficiently train them. With the advent of specialized hardware like graphics processing units (GPUs), it is now possible to efficiently train big DNNs. The introduction of ReLU units was also a key factor that allowed the training of even bigger networks. The availability of big quantities of data was another key factor that allowed the development of deep learning technologies. Note that deep learning is not limited to DNNs but it also encompasses other types of architectures like convolutional networks and recurrent neural networks, to name a few. Convolutional layers will be covered later in this chapter.</p>
</div>
<div id="learning-the-parameters" class="section level3">
<h3><span class="header-section-number">8.1.4</span> Learning the Parameters</h3>
<p>We have seen how <em>forward propagation</em> can be used at inference time to compute the output of the network based on the input values. In the previous examples, we assumed that the network’s parameters (weights and biases) were already learned. In practice, you most likely will be using libraries and frameworks to build and train neural networks. Later in this chapter, I will show you how to use TensorFlow and Keras within R. Before that, I will show you how networks’ parameters are learned and how we can code and train a very simple network from scratch.</p>
<p>Back to the problem, the objective is to find the parameters’ values based on training data such that the predicted result for any input data point is as close as possible as the true value. Put in other words, we want to find the parameters’ values that reduce the network´s prediction error.</p>
<p>One way to estimate the network’s error is by computing the squared difference between the prediction <span class="math inline">\(y&#39;\)</span> and the real value <span class="math inline">\(y\)</span>: <span class="math inline">\(error = (y&#39; - y)^2\)</span>. This is how the error can be computed for a single training data point. The error function is typically called the <strong>loss function</strong> and denoted by <span class="math inline">\(L(\theta)\)</span> where <span class="math inline">\(\theta\)</span> represents the parameters of the network (weights and biases). In this example the loss function is <span class="math inline">\(L(\theta)=(y&#39;- y)^2\)</span>.</p>
<p>If there is more than one training data point (which is often the case), the loss function is just the average of the individual squared differences which is known as the <strong>mean squared error (MSE)</strong>:</p>
<p><span class="math display" id="eq:lossMSE">\[\begin{equation}
  L(\theta) = \frac{1}{N} \sum_{n=1}^N{(y&#39;_n - y_n)^2}
  \tag{8.7}
\end{equation}\]</span></p>

<div class="rmdinfo">
The mean squared error (MSE) loss function is commonly used for regression problems. For classification problems, the average cross-entropy loss function is usually preferred (covered later in this chapter).
</div>

<p>The problem of finding the best parameters can be formulated as an optimization problem, that is, find the optimal parameters such that the loss function is minimized. This is the learning/training phase of a neural network. Formally, this can be stated as:</p>
<p><span class="math display" id="eq:minLoss">\[\begin{equation}
  \operatorname*{arg min}_{\theta} L(\theta)
  \tag{8.8}
\end{equation}\]</span></p>
<p>This notation means: find and return the weights and biases that make the loss function be as small as possible.</p>
<p>The most common method to train neural networks is called <strong>gradient descent</strong>. The algorithm updates the parameters in an iterative fashion based on the loss. This algorithm is suitable for complex functions with millions of parameters.</p>
<p>Suppose there is a network with only <span class="math inline">\(1\)</span> weight and no bias with MSE as loss function (Equation <a href="ann.html#eq:lossMSE">(8.7)</a>. Figure <a href="ann.html#fig:nnGD">8.10</a> shows a plot of the loss function. This is a quadratic function that only depends on the value of <span class="math inline">\(w\)</span>. The task is to find the <span class="math inline">\(w\)</span> where the function is at its minimum.</p>
<div class="figure" style="text-align: center"><span id="fig:nnGD"></span>
<img src="images/nn_gd.png" alt="Gradient descent in action." width="50%" />
<p class="caption">
Figure 8.10: Gradient descent in action.
</p>
</div>
<p>Gradient descent starts by assigning <span class="math inline">\(w\)</span> a random value. Then, at each step and based on the error, <span class="math inline">\(w\)</span> is updated in the direction that minimizes the loss function. In the previous figure, the <strong>global minimum</strong> is found after <span class="math inline">\(5\)</span> iterations. In practice, loss functions are more complex and have many <strong>local minima</strong> (Figure <a href="ann.html#fig:nnLM">8.11</a>). For complex functions, it is difficult to find a global minimum but gradient descent can find a local minimum that is good enough.</p>
<div class="figure" style="text-align: center"><span id="fig:nnLM"></span>
<img src="images/nn_lm.png" alt="Function with 1 global minimum and several local minima." width="50%" />
<p class="caption">
Figure 8.11: Function with 1 global minimum and several local minima.
</p>
</div>
<p>But in what direction and how much is <span class="math inline">\(w\)</span> moved in each iteration? The direction and magnitude are estimated by computing the derivative of the loss function with respect to the weight <span class="math inline">\(\frac{\partial L}{\partial w}\)</span>. The derivative is also called the gradient and denoted by <span class="math inline">\(\nabla L\)</span>. The iterative gradient descent procedure is listed below:</p>
<div class="line-block"><strong>loop</strong> until convergence or max iterations (<em>epochs</em>)<br />
  <strong>for each</strong> <span class="math inline">\(w_i\)</span> in <span class="math inline">\(W\)</span> <strong>do:</strong><br />
     <span class="math inline">\(w_i = w_i - \alpha \frac{\partial L(W)}{\partial w_i}\)</span><br />
</div>
<p>The outer loop is run until the algorithm converges or until a predefined number of iterations is reached. Each iteration is also called an <strong>epoch</strong>. Each weight is updated with the rule: <span class="math inline">\(w_i = w_i - \alpha \frac{\partial L(W)}{\partial w_i}\)</span>. The derivative part will give us the direction and magnitude. The <span class="math inline">\(\alpha\)</span> is called the <strong>learning rate</strong> and it controls how ‘fast’ we move. The learning rate is a constant defined by the user, thus, it is a <strong>hyperparameter</strong>. A high learning rate can cause the algorithm to miss the local minima and the loss can start to increase. A small learning rate will cause the algorithm to take more time to converge. Figure <a href="ann.html#fig:nnLR">8.12</a> illustrates both scenarios.</p>
<div class="figure" style="text-align: center"><span id="fig:nnLR"></span>
<img src="images/nn_lr.png" alt="a) Big learning rate. b) Small learning rate." width="100%" />
<p class="caption">
Figure 8.12: a) Big learning rate. b) Small learning rate.
</p>
</div>
<p>Selecting an appropriate learning rate will depend on the application but common values are between <span class="math inline">\(0.0001\)</span> and <span class="math inline">\(0.05\)</span>.</p>
<p>Let’s see how gradient descent works with a step by step example. Consider a very simple neural network consisting of an input layer with only one input feature and an output layer with one unit. To make it even simpler, the activation function of the output unit is the identity function <span class="math inline">\(f(x)=x\)</span>. Assume that as training data we have a single data point. Figure <a href="ann.html#fig:nnStepExample">8.13</a> shows the simple network and the training data. The training data point only has one input variable (<span class="math inline">\(x\)</span>) and an output (<span class="math inline">\(y\)</span>). We want to train this network such that it can make predictions on new data points. The training point has an input feature of <span class="math inline">\(x=3\)</span> and the expected output is <span class="math inline">\(y=1.5\)</span>. For this particular training point, it seems that the output is equal to the input divided by <span class="math inline">\(2\)</span>. Thus, based on this single training data point the network should learn how to divide any other input by <span class="math inline">\(2\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:nnStepExample"></span>
<img src="images/nn_step_example.png" alt="a) A simple neural network consisting of one unit. b) The training data with only one row." width="100%" />
<p class="caption">
Figure 8.13: a) A simple neural network consisting of one unit. b) The training data with only one row.
</p>
</div>
<p>Before we start the training we need to define <span class="math inline">\(3\)</span> things:</p>
<ol style="list-style-type: decimal">
<li><p>The loss function. This is a regression problem so we can use the MSE. Since there is a single data point our loss function becomes <span class="math inline">\(L(w)=(y&#39; - y)^2\)</span>. Here, <span class="math inline">\(y\)</span> is the ground truth output value and <span class="math inline">\(y&#39;\)</span> is the predicted value. We know how to make predictions using forward propagation. In this case, it is the product between the input value and the single weight, and the activation function has no effect (it returns the same value as its input). We can rewrite the loss function as <span class="math inline">\(L(w)=(xw - y)^2\)</span>.</p></li>
<li><p>We need to define a learning rate. For now, we can set it to <span class="math inline">\(\alpha = 0.05\)</span>.</p></li>
<li><p>The weights need to be initialized at random. Let’s assume the single weight is ‘randomly’ initialized with <span class="math inline">\(w=2\)</span>.</p></li>
</ol>
<p>Now we use gradient descent to iteratively update the weight. Remember that the updating rule is:</p>
<p><span class="math display">\[\begin{equation}
  w = w - \alpha \frac{\partial L(w)}{\partial w}
\end{equation}\]</span></p>
<p>The partial derivative of the loss function with respect to <span class="math inline">\(w\)</span> is:</p>
<p><span class="math display">\[\begin{equation}
  \frac{\partial L(w)}{\partial w} = 2x(xw - y)
\end{equation}\]</span></p>
<p>If we substitute the derivative in the updating rule we get:</p>
<p><span class="math display">\[\begin{equation}
  w = w - \alpha 2x(xw - y)
\end{equation}\]</span></p>
<p>We already know that <span class="math inline">\(\alpha=0.05\)</span>, the input value is <span class="math inline">\(x=3\)</span>, the output is <span class="math inline">\(y=1.5\)</span> and the initial weight is <span class="math inline">\(w=2\)</span>. So we can start updating <span class="math inline">\(w\)</span>. Figure <a href="ann.html#fig:nnTrainProgress">8.14</a> shows the initial state (iteration 0) and <span class="math inline">\(3\)</span> additional iterations. In the initial state, <span class="math inline">\(w=2\)</span> and with that weight the loss is <span class="math inline">\(20.25\)</span>. In iteration <span class="math inline">\(1\)</span>, the weight is updated and now its value is <span class="math inline">\(0.65\)</span>. With this new weight, the loss is <span class="math inline">\(0.2025\)</span>. That was a substantial reduction in the error! After three iterations we see that the final weight is <span class="math inline">\(w=0.501\)</span> and the loss is very close to zero.</p>
<div class="figure" style="text-align: center"><span id="fig:nnTrainProgress"></span>
<img src="images/nn_train_progress.png" alt="First 3 gradient descent iterations (epochs)." width="100%" />
<p class="caption">
Figure 8.14: First 3 gradient descent iterations (epochs).
</p>
</div>
<p>Now, we can start doing predictions with our very simple neural network! To do so, we use forward propagation on the new input data using the learned weight <span class="math inline">\(w=0.501\)</span>. Figure <a href="ann.html#fig:nnExamplePredictions">8.15</a> shows the predictions on new training data points that were never seen by the network before.</p>
<div class="figure" style="text-align: center"><span id="fig:nnExamplePredictions"></span>
<img src="images/nn_example_predictions.png" alt="Example predictions on new data points." width="60%" />
<p class="caption">
Figure 8.15: Example predictions on new data points.
</p>
</div>
<p>Even though the predictions are not perfect, they are very close to the expected value (division by <span class="math inline">\(2\)</span>) considering that the network is very simple and was only trained with a single data point and for only <span class="math inline">\(3\)</span> epochs!</p>
<p>If the training set has more than one data point, then we need to compute the derivative of each point and accumulate them (the derivative of a sum is equal to the sum of the derivatives). In the previous example, the update rule becomes:</p>
<p><span class="math display">\[\begin{equation}
  w = w - \alpha \sum_{i=1}^N{2x_i(x_i w - y)}
\end{equation}\]</span></p>
<p>This means that before updating a weight, first, we need to compute the derivative for each point and add them. This needs to be done for every parameter in the network. Thus, one <strong>epoch</strong> is a pass through all training points and all parameters.</p>
</div>
<div id="parameter-learning-example-in-r" class="section level3">
<h3><span class="header-section-number">8.1.5</span> Parameter Learning Example in R</h3>

<div class="rmdfolder">
<code>gradient_descent.R</code>
</div>

<p>In the previous section, we went step by step to train a neural network with a single unit and with a single training data point. Here, we will see how we can implement that simple network in R but when we have more training data. The code can be found in the script <code>gradient_descent.R</code>.</p>
<p>This code implements the same network as the previous example. That is, one neuron, one input, no bias, and activation function <span class="math inline">\(f(x) = x\)</span>. We start by creating a sample training set with <span class="math inline">\(3\)</span> points. Again, the output is the input divided by <span class="math inline">\(2\)</span>.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="ann.html#cb125-1"></a>train_set &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="fl">3.0</span>,<span class="fl">4.0</span>,<span class="fl">1.0</span>), <span class="dt">y =</span> <span class="kw">c</span>(<span class="fl">1.5</span>, <span class="fl">2.0</span>, <span class="fl">0.5</span>))</span>
<span id="cb125-2"><a href="ann.html#cb125-2"></a><span class="co"># Print the train set.</span></span>
<span id="cb125-3"><a href="ann.html#cb125-3"></a><span class="kw">print</span>(train_set)</span>
<span id="cb125-4"><a href="ann.html#cb125-4"></a><span class="co">#&gt;   x   y</span></span>
<span id="cb125-5"><a href="ann.html#cb125-5"></a><span class="co">#&gt; 1 3 1.5</span></span>
<span id="cb125-6"><a href="ann.html#cb125-6"></a><span class="co">#&gt; 2 4 2.0</span></span>
<span id="cb125-7"><a href="ann.html#cb125-7"></a><span class="co">#&gt; 3 1 0.5</span></span></code></pre></div>
<p>Then we need to implement <span class="math inline">\(3\)</span> functions: forward propagation, the loss function, and the derivative of the loss function.</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="ann.html#cb126-1"></a><span class="co"># Forward propagation w*x</span></span>
<span id="cb126-2"><a href="ann.html#cb126-2"></a>fp &lt;-<span class="st"> </span><span class="cf">function</span>(w, x){</span>
<span id="cb126-3"><a href="ann.html#cb126-3"></a>    <span class="kw">return</span>(w <span class="op">*</span><span class="st"> </span>x)</span>
<span id="cb126-4"><a href="ann.html#cb126-4"></a>}</span>
<span id="cb126-5"><a href="ann.html#cb126-5"></a></span>
<span id="cb126-6"><a href="ann.html#cb126-6"></a><span class="co"># Loss function (y - y&#39;)^2</span></span>
<span id="cb126-7"><a href="ann.html#cb126-7"></a>loss &lt;-<span class="st"> </span><span class="cf">function</span>(w, x, y){</span>
<span id="cb126-8"><a href="ann.html#cb126-8"></a>  predicted &lt;-<span class="st"> </span><span class="kw">fp</span>(w, x) <span class="co"># This is y&#39;</span></span>
<span id="cb126-9"><a href="ann.html#cb126-9"></a>  <span class="kw">return</span>((y <span class="op">-</span><span class="st"> </span>predicted)<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb126-10"><a href="ann.html#cb126-10"></a>}</span>
<span id="cb126-11"><a href="ann.html#cb126-11"></a></span>
<span id="cb126-12"><a href="ann.html#cb126-12"></a><span class="co"># Derivative of the loss function. 2x(xw - y)</span></span>
<span id="cb126-13"><a href="ann.html#cb126-13"></a>derivative &lt;-<span class="st"> </span><span class="cf">function</span>(w, x, y){</span>
<span id="cb126-14"><a href="ann.html#cb126-14"></a>  <span class="kw">return</span>(<span class="fl">2.0</span> <span class="op">*</span><span class="st"> </span>x <span class="op">*</span><span class="st"> </span>((x <span class="op">*</span><span class="st"> </span>w) <span class="op">-</span><span class="st"> </span>y))</span>
<span id="cb126-15"><a href="ann.html#cb126-15"></a>}</span></code></pre></div>
<p>Now we are all set to implement the <code>gradient.descent()</code> function. The first parameter is the train set, the second parameter is the learning rate <span class="math inline">\(\alpha\)</span> and the last parameter is the number of epochs. The initial weight is initialized to some ‘random’ number (selected manually here for the sake of the example). The function returns the final learned weight.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="ann.html#cb127-1"></a><span class="co"># Gradient descent.</span></span>
<span id="cb127-2"><a href="ann.html#cb127-2"></a>gradient.descent &lt;-<span class="st"> </span><span class="cf">function</span>(train_set, <span class="dt">lr =</span> <span class="fl">0.01</span>, <span class="dt">epochs =</span> <span class="dv">5</span>){</span>
<span id="cb127-3"><a href="ann.html#cb127-3"></a>  </span>
<span id="cb127-4"><a href="ann.html#cb127-4"></a>  w =<span class="st"> </span><span class="fl">-2.5</span> <span class="co"># Initialize weight at &#39;random&#39;</span></span>
<span id="cb127-5"><a href="ann.html#cb127-5"></a>  </span>
<span id="cb127-6"><a href="ann.html#cb127-6"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epochs){</span>
<span id="cb127-7"><a href="ann.html#cb127-7"></a>    derivative.sum &lt;-<span class="st"> </span><span class="fl">0.0</span></span>
<span id="cb127-8"><a href="ann.html#cb127-8"></a>    loss.sum &lt;-<span class="st"> </span><span class="fl">0.0</span></span>
<span id="cb127-9"><a href="ann.html#cb127-9"></a>    </span>
<span id="cb127-10"><a href="ann.html#cb127-10"></a>    <span class="co"># Iterate each data point in train_set.</span></span>
<span id="cb127-11"><a href="ann.html#cb127-11"></a>    <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(train_set)){</span>
<span id="cb127-12"><a href="ann.html#cb127-12"></a>      point &lt;-<span class="st"> </span>train_set[j, ]</span>
<span id="cb127-13"><a href="ann.html#cb127-13"></a>      </span>
<span id="cb127-14"><a href="ann.html#cb127-14"></a>      derivative.sum &lt;-<span class="st"> </span>derivative.sum <span class="op">+</span><span class="st"> </span><span class="kw">derivative</span>(w, point<span class="op">$</span>x, point<span class="op">$</span>y)</span>
<span id="cb127-15"><a href="ann.html#cb127-15"></a>      </span>
<span id="cb127-16"><a href="ann.html#cb127-16"></a>      loss.sum &lt;-<span class="st"> </span>loss.sum <span class="op">+</span><span class="st"> </span><span class="kw">loss</span>(w, point<span class="op">$</span>x, point<span class="op">$</span>y)</span>
<span id="cb127-17"><a href="ann.html#cb127-17"></a>    }</span>
<span id="cb127-18"><a href="ann.html#cb127-18"></a>    </span>
<span id="cb127-19"><a href="ann.html#cb127-19"></a>    <span class="co"># Update weight.</span></span>
<span id="cb127-20"><a href="ann.html#cb127-20"></a>    w &lt;-<span class="st">  </span>w <span class="op">-</span><span class="st"> </span>lr <span class="op">*</span><span class="st"> </span>derivative.sum</span>
<span id="cb127-21"><a href="ann.html#cb127-21"></a>    </span>
<span id="cb127-22"><a href="ann.html#cb127-22"></a>    <span class="co"># mean squared error (MSE)</span></span>
<span id="cb127-23"><a href="ann.html#cb127-23"></a>    mse &lt;-<span class="st"> </span>loss.sum <span class="op">/</span><span class="st"> </span><span class="kw">nrow</span>(train_set)</span>
<span id="cb127-24"><a href="ann.html#cb127-24"></a>    </span>
<span id="cb127-25"><a href="ann.html#cb127-25"></a>    <span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;epoch: &quot;</span>, i, <span class="st">&quot; loss: &quot;</span>,</span>
<span id="cb127-26"><a href="ann.html#cb127-26"></a>                 <span class="kw">formatC</span>(mse, <span class="dt">digits =</span> <span class="dv">8</span>, <span class="dt">format =</span> <span class="st">&quot;f&quot;</span>),</span>
<span id="cb127-27"><a href="ann.html#cb127-27"></a>                 <span class="st">&quot; w = &quot;</span>, <span class="kw">formatC</span>(w, <span class="dt">digits =</span> <span class="dv">5</span>, <span class="dt">format =</span> <span class="st">&quot;f&quot;</span>)))</span>
<span id="cb127-28"><a href="ann.html#cb127-28"></a>  }</span>
<span id="cb127-29"><a href="ann.html#cb127-29"></a>  </span>
<span id="cb127-30"><a href="ann.html#cb127-30"></a>  <span class="kw">return</span>(w)</span>
<span id="cb127-31"><a href="ann.html#cb127-31"></a>}</span></code></pre></div>
<p>Now, let’s train the network with a learning rate of <span class="math inline">\(0.01\)</span> and for <span class="math inline">\(10\)</span> epochs. This function will print for each epoch, the loss and the current weight.</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="ann.html#cb128-1"></a><span class="co">#### Train the 1 unit network with gradient descent ####</span></span>
<span id="cb128-2"><a href="ann.html#cb128-2"></a>lr &lt;-<span class="st"> </span><span class="fl">0.01</span> <span class="co"># set learning rate.</span></span>
<span id="cb128-3"><a href="ann.html#cb128-3"></a></span>
<span id="cb128-4"><a href="ann.html#cb128-4"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb128-5"><a href="ann.html#cb128-5"></a></span>
<span id="cb128-6"><a href="ann.html#cb128-6"></a><span class="co"># Run gradient decent to find the optimal weight.</span></span>
<span id="cb128-7"><a href="ann.html#cb128-7"></a>learned_w =<span class="st"> </span><span class="kw">gradient.descent</span>(train_set, lr, <span class="dt">epochs =</span> <span class="dv">10</span>)</span>
<span id="cb128-8"><a href="ann.html#cb128-8"></a></span>
<span id="cb128-9"><a href="ann.html#cb128-9"></a><span class="co">#&gt; [1] &quot;epoch: 1 loss: 78.00000000 w = -0.94000&quot;</span></span>
<span id="cb128-10"><a href="ann.html#cb128-10"></a><span class="co">#&gt; [1] &quot;epoch: 2 loss: 17.97120000 w = -0.19120&quot;</span></span>
<span id="cb128-11"><a href="ann.html#cb128-11"></a><span class="co">#&gt; [1] &quot;epoch: 3 loss: 4.14056448 w = 0.16822&quot;</span></span>
<span id="cb128-12"><a href="ann.html#cb128-12"></a><span class="co">#&gt; [1] &quot;epoch: 4 loss: 0.95398606 w = 0.34075&quot;</span></span>
<span id="cb128-13"><a href="ann.html#cb128-13"></a><span class="co">#&gt; [1] &quot;epoch: 5 loss: 0.21979839 w = 0.42356&quot;</span></span>
<span id="cb128-14"><a href="ann.html#cb128-14"></a><span class="co">#&gt; [1] &quot;epoch: 6 loss: 0.05064155 w = 0.46331&quot;</span></span>
<span id="cb128-15"><a href="ann.html#cb128-15"></a><span class="co">#&gt; [1] &quot;epoch: 7 loss: 0.01166781 w = 0.48239&quot;</span></span>
<span id="cb128-16"><a href="ann.html#cb128-16"></a><span class="co">#&gt; [1] &quot;epoch: 8 loss: 0.00268826 w = 0.49155&quot;</span></span>
<span id="cb128-17"><a href="ann.html#cb128-17"></a><span class="co">#&gt; [1] &quot;epoch: 9 loss: 0.00061938 w = 0.49594&quot;</span></span>
<span id="cb128-18"><a href="ann.html#cb128-18"></a><span class="co">#&gt; [1] &quot;epoch: 10 loss: 0.00014270 w = 0.49805&quot;</span></span></code></pre></div>
<p>From the output, we can see that the loss decreases as the weight is updated. The final value of the weight at iteration <span class="math inline">\(10\)</span> is <span class="math inline">\(0.49805\)</span>. We can now make predictions on new data.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="ann.html#cb129-1"></a><span class="co"># Make predictions on new data using the learned weight.</span></span>
<span id="cb129-2"><a href="ann.html#cb129-2"></a><span class="kw">fp</span>(learned_w, <span class="dv">7</span>)</span>
<span id="cb129-3"><a href="ann.html#cb129-3"></a><span class="co">#&gt; [1] 3.486366</span></span>
<span id="cb129-4"><a href="ann.html#cb129-4"></a></span>
<span id="cb129-5"><a href="ann.html#cb129-5"></a><span class="kw">fp</span>(learned_w, <span class="dv">-88</span>)</span>
<span id="cb129-6"><a href="ann.html#cb129-6"></a><span class="co">#&gt; [1] -43.8286</span></span></code></pre></div>
<p>Now, you can try to change the training set to make the network learn a different arithmetic operation!</p>
<p>In the previous example, we considered a very simple neural network consisting of a single unit. In this case, the partial derivative with respect to the single weight was calculated directly. For bigger networks with more layers and activations, the final output becomes a composition of functions. That is, the activation values of a layer <span class="math inline">\(l\)</span> depend on its weights which are also affected by the previous layer’s <span class="math inline">\(l-1\)</span> weights and so on. So, the derivatives (gradients) can be computed using the chain rule <span class="math inline">\(f(g(x))&#39; = f&#39;(g(x)) \cdot g&#39;(x)\)</span>. This can be performed efficiently by an algorithm known as <strong>backpropagation</strong>.</p>
<blockquote>
<p>“What backpropagation actually lets us do is compute the partial derivatives <span class="math inline">\(\partial C_x / \partial w\)</span> and <span class="math inline">\(\partial C_x / \partial b\)</span> for a single training example.” (Michael Nielsen, 2019)<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a>.</p>
</blockquote>
<p>Here, <span class="math inline">\(C\)</span> refers to the loss function which is also called the cost function. In modern deep learning libraries like TensorFlow, this procedure is efficiently implemented with a computational graph. If you want to learn the details about backpropagation I recommend you to check this post by DEEPLIZARD (<a href="https://deeplizard.com/learn/video/XE3krf3CQls" class="uri">https://deeplizard.com/learn/video/XE3krf3CQls</a>) which consists of <span class="math inline">\(5\)</span> parts including videos.</p>
</div>
<div id="stochastic-gradient-descent" class="section level3">
<h3><span class="header-section-number">8.1.6</span> Stochastic Gradient Descent</h3>
<p>We have seen how gradient descent iterates over all training points before updating each parameter. To recall, an epoch is one pass through all parameters and for each parameter, the derivative with each training point needs to be computed. If the training set consists of thousands or millions of points this method becomes very time-consuming. Furthermore, in practice neural networks do not have one or two parameters but thousands or millions. In those cases, the training can be done more efficiently by using <strong>stochastic gradient descent (SGD)</strong>. This method adds two main modifications to the classic gradient descent:</p>
<ol style="list-style-type: decimal">
<li>At the beginning, the training set is shuffled (this is the stochastic part). This is necessary for the method to work.</li>
<li>The training set is divided into <span class="math inline">\(b\)</span> batches with <span class="math inline">\(m\)</span> data points each. This <span class="math inline">\(m\)</span> is known as the <strong>batch size</strong> and is a hyperparameter that we need to define.</li>
</ol>
<p>Then, at each epoch all batches are iterated and the parameters are updated based on each batch and not the entire training set, for example:</p>
<p><span class="math display">\[\begin{equation}
  w = w - \alpha \sum_{i=1}^m{2x_i(x_i w - y)}
\end{equation}\]</span></p>
<p>Again, an epoch is one pass through all parameters and all batches. Now you may be wondering why this method is more efficient if an epoch still involves the same number of operations but they are split into chunks. Part of this is because since the parameter updates are more frequent, the loss also improves quicker. Another reason is that the operations within each batch can be optimized and performed in parallel, for example, by using a GPU. One thing to note is that each update is based on less information by only using <span class="math inline">\(m\)</span> points instead of the entire data set. This can introduce some noise in the learning but at the same time this can help to get out of local minima. In practice, SGD needs more epochs to converge compared to gradient descent but overall, it will take less time. From now on, this is the method we will use to train our networks.</p>

<div class="rmdinfo">
Typical batch sizes are: 4,8,16,32,64,128, etc. There is a divided opinion with this respect. Some say it is better to choose small batch sizes but others say the bigger the best. For any particular problem, it is difficult to say what batch size is the optimal. Usually, one needs to choose the batch size empirically by trying different sizes.
</div>


<div class="rmdcaution">
Be aware that when using GPUs, a big batch size can cause out of memory errors since the GPU may not have enough memory to allocate the batch.
</div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-rosenblatt1958">
<p>Rosenblatt, Frank. 1958. “The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.” <em>Psychological Review</em> 65 (6): 386.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="19">
<li id="fn19"><p><a href="http://neuralnetworksanddeeplearning.com/chap2.html" class="uri">http://neuralnetworksanddeeplearning.com/chap2.html</a><a href="ann.html#fnref19" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="deeplearning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="keras-and-tensorflow-with-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
