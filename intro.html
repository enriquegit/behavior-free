<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Introduction to Behavior and Machine Learning | Behavior Analysis with Machine Learning Using R</title>
  <meta name="description" content="Chapter 1 Introduction to Behavior and Machine Learning | Behavior Analysis with Machine Learning Using R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Introduction to Behavior and Machine Learning | Behavior Analysis with Machine Learning Using R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="/images/cover.png" />
  <meta property="og:description" content="Chapter 1 Introduction to Behavior and Machine Learning | Behavior Analysis with Machine Learning Using R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Introduction to Behavior and Machine Learning | Behavior Analysis with Machine Learning Using R" />
  
  <meta name="twitter:description" content="Chapter 1 Introduction to Behavior and Machine Learning | Behavior Analysis with Machine Learning Using R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  <meta name="twitter:image" content="/images/cover.png" />

<meta name="author" content="Enrique Garcia Ceja" />


<meta name="date" content="2021-10-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="preface.html"/>
<link rel="next" href="classification.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/d3/d3.min.js"></script>
<link href="libs/d3panels/d3panels.min.css" rel="stylesheet" />
<script src="libs/d3panels/d3panels.min.js"></script>
<script src="libs/qtlcharts_iplotCorr/iplotCorr.js"></script>
<script src="libs/qtlcharts_iplotCorr/iplotCorr_noscat.js"></script>
<script src="libs/iplotCorr-binding/iplotCorr.js"></script>
<link href="libs/dygraphs/dygraph.css" rel="stylesheet" />
<script src="libs/dygraphs/dygraph-combined.js"></script>
<script src="libs/dygraphs/shapes.js"></script>
<script src="libs/moment/moment.js"></script>
<script src="libs/moment-timezone/moment-timezone-with-data.js"></script>
<script src="libs/moment-fquarter/moment-fquarter.min.js"></script>
<script src="libs/dygraphs-binding/dygraphs.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178679335-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178679335-1', { 'anonymize_ip': true });
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="#">Behavior Analysis with Machine Learning Using R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-front-cover"><i class="fa fa-check"></i>About the Front Cover</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#supplemental-material"><i class="fa fa-check"></i>Supplemental Material</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#conventions"><i class="fa fa-check"></i>Conventions</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to Behavior and Machine Learning</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#what-is-behavior"><i class="fa fa-check"></i><b>1.1</b> What Is Behavior?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.2</b> What Is Machine Learning?</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#taxonomy"><i class="fa fa-check"></i><b>1.3</b> Types of Machine Learning</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#terminology"><i class="fa fa-check"></i><b>1.4</b> Terminology</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#tables"><i class="fa fa-check"></i><b>1.4.1</b> Tables</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro.html"><a href="intro.html#variable-types"><i class="fa fa-check"></i><b>1.4.2</b> Variable Types</a></li>
<li class="chapter" data-level="1.4.3" data-path="intro.html"><a href="intro.html#predictive-models"><i class="fa fa-check"></i><b>1.4.3</b> Predictive Models</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#pipeline"><i class="fa fa-check"></i><b>1.5</b> Data Analysis Pipeline</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#trainingeval"><i class="fa fa-check"></i><b>1.6</b> Evaluating Predictive Models</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#simple-classification-example"><i class="fa fa-check"></i><b>1.7</b> Simple Classification Example</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="intro.html"><a href="intro.html#k-fold-cross-validation-example"><i class="fa fa-check"></i><b>1.7.1</b> <span class="math inline">\(k\)</span>-fold Cross-validation Example</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#simple-regression-example"><i class="fa fa-check"></i><b>1.8</b> Simple Regression Example</a></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a href="intro.html#underfitting-and-overfitting"><i class="fa fa-check"></i><b>1.9</b> Underfitting and Overfitting</a></li>
<li class="chapter" data-level="1.10" data-path="intro.html"><a href="intro.html#bias-and-variance"><i class="fa fa-check"></i><b>1.10</b> Bias and Variance</a></li>
<li class="chapter" data-level="1.11" data-path="intro.html"><a href="intro.html#SummaryIntro"><i class="fa fa-check"></i><b>1.11</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>2</b> Predicting Behavior with Classification Models</a>
<ul>
<li class="chapter" data-level="2.1" data-path="classification.html"><a href="classification.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>2.1</b> <em>k</em>-Nearest Neighbors</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="classification.html"><a href="classification.html#indoor-location-with-wi-fi-signals"><i class="fa fa-check"></i><b>2.1.1</b> Indoor Location with Wi-Fi Signals</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="classification.html"><a href="classification.html#performance-metrics"><i class="fa fa-check"></i><b>2.2</b> Performance Metrics</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="classification.html"><a href="classification.html#confusion-matrix"><i class="fa fa-check"></i><b>2.2.1</b> Confusion Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="classification.html"><a href="classification.html#decision-trees"><i class="fa fa-check"></i><b>2.3</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="classification.html"><a href="classification.html#activityRecognition"><i class="fa fa-check"></i><b>2.3.1</b> Activity Recognition with Smartphones</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="classification.html"><a href="classification.html#naive-bayes"><i class="fa fa-check"></i><b>2.4</b> Naive Bayes</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="classification.html"><a href="classification.html#activity-recognition-with-naive-bayes"><i class="fa fa-check"></i><b>2.4.1</b> Activity Recognition with Naive Bayes</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="classification.html"><a href="classification.html#dynamic-time-warping"><i class="fa fa-check"></i><b>2.5</b> Dynamic Time Warping</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="classification.html"><a href="classification.html#sechandgestures"><i class="fa fa-check"></i><b>2.5.1</b> Hand Gesture Recognition</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="classification.html"><a href="classification.html#dummy-models"><i class="fa fa-check"></i><b>2.6</b> Dummy Models</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="classification.html"><a href="classification.html#most-frequent-class-classifier"><i class="fa fa-check"></i><b>2.6.1</b> Most-frequent-class Classifier</a></li>
<li class="chapter" data-level="2.6.2" data-path="classification.html"><a href="classification.html#uniform-classifier"><i class="fa fa-check"></i><b>2.6.2</b> Uniform Classifier</a></li>
<li class="chapter" data-level="2.6.3" data-path="classification.html"><a href="classification.html#frequency-based-classifier"><i class="fa fa-check"></i><b>2.6.3</b> Frequency-based Classifier</a></li>
<li class="chapter" data-level="2.6.4" data-path="classification.html"><a href="classification.html#other-dummy-classifiers"><i class="fa fa-check"></i><b>2.6.4</b> Other Dummy Classifiers</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="classification.html"><a href="classification.html#summaryClassification"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>3</b> Predicting Behavior with Ensemble Learning</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ensemble.html"><a href="ensemble.html#bagging"><i class="fa fa-check"></i><b>3.1</b> Bagging</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ensemble.html"><a href="ensemble.html#activity-recognition-with-bagging"><i class="fa fa-check"></i><b>3.1.1</b> Activity Recognition with Bagging</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ensemble.html"><a href="ensemble.html#random-forest"><i class="fa fa-check"></i><b>3.2</b> Random Forest</a></li>
<li class="chapter" data-level="3.3" data-path="ensemble.html"><a href="ensemble.html#stacked-generalization"><i class="fa fa-check"></i><b>3.3</b> Stacked Generalization</a></li>
<li class="chapter" data-level="3.4" data-path="ensemble.html"><a href="ensemble.html#multiviewhometasks"><i class="fa fa-check"></i><b>3.4</b> Multi-view Stacking for Home Tasks Recognition</a></li>
<li class="chapter" data-level="3.5" data-path="ensemble.html"><a href="ensemble.html#SummaryEnsemble"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="edavis.html"><a href="edavis.html"><i class="fa fa-check"></i><b>4</b> Exploring and Visualizing Behavioral Data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="edavis.html"><a href="edavis.html#talking-with-field-experts"><i class="fa fa-check"></i><b>4.1</b> Talking with Field Experts</a></li>
<li class="chapter" data-level="4.2" data-path="edavis.html"><a href="edavis.html#summary-statistics"><i class="fa fa-check"></i><b>4.2</b> Summary Statistics</a></li>
<li class="chapter" data-level="4.3" data-path="edavis.html"><a href="edavis.html#class-distributions"><i class="fa fa-check"></i><b>4.3</b> Class Distributions</a></li>
<li class="chapter" data-level="4.4" data-path="edavis.html"><a href="edavis.html#user-class-sparsity-matrix"><i class="fa fa-check"></i><b>4.4</b> User-class Sparsity Matrix</a></li>
<li class="chapter" data-level="4.5" data-path="edavis.html"><a href="edavis.html#boxplots"><i class="fa fa-check"></i><b>4.5</b> Boxplots</a></li>
<li class="chapter" data-level="4.6" data-path="edavis.html"><a href="edavis.html#correlation-plots"><i class="fa fa-check"></i><b>4.6</b> Correlation Plots</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="edavis.html"><a href="edavis.html#interactive-correlation-plots"><i class="fa fa-check"></i><b>4.6.1</b> Interactive Correlation Plots</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="edavis.html"><a href="edavis.html#timeseries"><i class="fa fa-check"></i><b>4.7</b> Timeseries</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="edavis.html"><a href="edavis.html#interactive-timeseries"><i class="fa fa-check"></i><b>4.7.1</b> Interactive Timeseries</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="edavis.html"><a href="edavis.html#multidimensional-scaling-mds"><i class="fa fa-check"></i><b>4.8</b> Multidimensional Scaling (MDS)</a></li>
<li class="chapter" data-level="4.9" data-path="edavis.html"><a href="edavis.html#heatmaps"><i class="fa fa-check"></i><b>4.9</b> Heatmaps</a></li>
<li class="chapter" data-level="4.10" data-path="edavis.html"><a href="edavis.html#automated-eda"><i class="fa fa-check"></i><b>4.10</b> Automated EDA</a></li>
<li class="chapter" data-level="4.11" data-path="edavis.html"><a href="edavis.html#SummaryExploratory"><i class="fa fa-check"></i><b>4.11</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="preprocessing.html"><a href="preprocessing.html"><i class="fa fa-check"></i><b>5</b> Preprocessing Behavioral Data</a>
<ul>
<li class="chapter" data-level="5.1" data-path="preprocessing.html"><a href="preprocessing.html#missing-values"><i class="fa fa-check"></i><b>5.1</b> Missing Values</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="preprocessing.html"><a href="preprocessing.html#imputation"><i class="fa fa-check"></i><b>5.1.1</b> Imputation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="preprocessing.html"><a href="preprocessing.html#smoothing"><i class="fa fa-check"></i><b>5.2</b> Smoothing</a></li>
<li class="chapter" data-level="5.3" data-path="preprocessing.html"><a href="preprocessing.html#normalization"><i class="fa fa-check"></i><b>5.3</b> Normalization</a></li>
<li class="chapter" data-level="5.4" data-path="preprocessing.html"><a href="preprocessing.html#imbalanced-classes"><i class="fa fa-check"></i><b>5.4</b> Imbalanced Classes</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="preprocessing.html"><a href="preprocessing.html#random-oversampling"><i class="fa fa-check"></i><b>5.4.1</b> Random Oversampling</a></li>
<li class="chapter" data-level="5.4.2" data-path="preprocessing.html"><a href="preprocessing.html#smote"><i class="fa fa-check"></i><b>5.4.2</b> SMOTE</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="preprocessing.html"><a href="preprocessing.html#infoinjection"><i class="fa fa-check"></i><b>5.5</b> Information Injection</a></li>
<li class="chapter" data-level="5.6" data-path="preprocessing.html"><a href="preprocessing.html#one-hot-encoding"><i class="fa fa-check"></i><b>5.6</b> One-hot Encoding</a></li>
<li class="chapter" data-level="5.7" data-path="preprocessing.html"><a href="preprocessing.html#SummaryPreprocessing"><i class="fa fa-check"></i><b>5.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="unsupervised.html"><a href="unsupervised.html"><i class="fa fa-check"></i><b>6</b> Discovering Behaviors with Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="6.1" data-path="unsupervised.html"><a href="unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>6.1</b> <span class="math inline">\(k\)</span>-means Clustering</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="unsupervised.html"><a href="unsupervised.html#studentresponses"><i class="fa fa-check"></i><b>6.1.1</b> Grouping Student Responses</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="unsupervised.html"><a href="unsupervised.html#the-silhouette-index"><i class="fa fa-check"></i><b>6.2</b> The Silhouette Index</a></li>
<li class="chapter" data-level="6.3" data-path="unsupervised.html"><a href="unsupervised.html#associationrules"><i class="fa fa-check"></i><b>6.3</b> Mining Association Rules</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="unsupervised.html"><a href="unsupervised.html#finding-rules-for-criminal-behavior"><i class="fa fa-check"></i><b>6.3.1</b> Finding Rules for Criminal Behavior</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="unsupervised.html"><a href="unsupervised.html#SummaryUnsupervised"><i class="fa fa-check"></i><b>6.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="representations.html"><a href="representations.html"><i class="fa fa-check"></i><b>7</b> Encoding Behavioral Data</a>
<ul>
<li class="chapter" data-level="7.1" data-path="representations.html"><a href="representations.html#feature-vectors"><i class="fa fa-check"></i><b>7.1</b> Feature Vectors</a></li>
<li class="chapter" data-level="7.2" data-path="representations.html"><a href="representations.html#sectimeseries"><i class="fa fa-check"></i><b>7.2</b> Timeseries</a></li>
<li class="chapter" data-level="7.3" data-path="representations.html"><a href="representations.html#transactions"><i class="fa fa-check"></i><b>7.3</b> Transactions</a></li>
<li class="chapter" data-level="7.4" data-path="representations.html"><a href="representations.html#images"><i class="fa fa-check"></i><b>7.4</b> Images</a></li>
<li class="chapter" data-level="7.5" data-path="representations.html"><a href="representations.html#recurrence-plots"><i class="fa fa-check"></i><b>7.5</b> Recurrence Plots</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="representations.html"><a href="representations.html#computing-recurrence-plots"><i class="fa fa-check"></i><b>7.5.1</b> Computing Recurrence Plots</a></li>
<li class="chapter" data-level="7.5.2" data-path="representations.html"><a href="representations.html#recurrence-plots-of-hand-gestures"><i class="fa fa-check"></i><b>7.5.2</b> Recurrence Plots of Hand Gestures</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="representations.html"><a href="representations.html#bag-of-words"><i class="fa fa-check"></i><b>7.6</b> Bag-of-Words</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="representations.html"><a href="representations.html#bow-for-complex-activities."><i class="fa fa-check"></i><b>7.6.1</b> BoW for Complex Activities.</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="representations.html"><a href="representations.html#graphs"><i class="fa fa-check"></i><b>7.7</b> Graphs</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="representations.html"><a href="representations.html#complex-activities-as-graphs"><i class="fa fa-check"></i><b>7.7.1</b> Complex Activities as Graphs</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="representations.html"><a href="representations.html#SummaryRepresentations"><i class="fa fa-check"></i><b>7.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deeplearning.html"><a href="deeplearning.html"><i class="fa fa-check"></i><b>8</b> Predicting Behavior with Deep Learning</a>
<ul>
<li class="chapter" data-level="8.1" data-path="deeplearning.html"><a href="deeplearning.html#ann"><i class="fa fa-check"></i><b>8.1</b> Introduction to Artificial Neural Networks</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="deeplearning.html"><a href="deeplearning.html#sigmoid-and-relu-units"><i class="fa fa-check"></i><b>8.1.1</b> Sigmoid and ReLU Units</a></li>
<li class="chapter" data-level="8.1.2" data-path="deeplearning.html"><a href="deeplearning.html#assembling-units-into-layers"><i class="fa fa-check"></i><b>8.1.2</b> Assembling Units into Layers</a></li>
<li class="chapter" data-level="8.1.3" data-path="deeplearning.html"><a href="deeplearning.html#deep-neural-networks"><i class="fa fa-check"></i><b>8.1.3</b> Deep Neural Networks</a></li>
<li class="chapter" data-level="8.1.4" data-path="deeplearning.html"><a href="deeplearning.html#learning-the-parameters"><i class="fa fa-check"></i><b>8.1.4</b> Learning the Parameters</a></li>
<li class="chapter" data-level="8.1.5" data-path="deeplearning.html"><a href="deeplearning.html#parameter-learning-example-in-r"><i class="fa fa-check"></i><b>8.1.5</b> Parameter Learning Example in R</a></li>
<li class="chapter" data-level="8.1.6" data-path="deeplearning.html"><a href="deeplearning.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>8.1.6</b> Stochastic Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="deeplearning.html"><a href="deeplearning.html#keras-and-tensorflow-with-r"><i class="fa fa-check"></i><b>8.2</b> Keras and TensorFlow with R</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="deeplearning.html"><a href="deeplearning.html#keras-example"><i class="fa fa-check"></i><b>8.2.1</b> Keras Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="deeplearning.html"><a href="deeplearning.html#classification-with-neural-networks"><i class="fa fa-check"></i><b>8.3</b> Classification with Neural Networks</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="deeplearning.html"><a href="deeplearning.html#classification-of-electromyography-signals"><i class="fa fa-check"></i><b>8.3.1</b> Classification of Electromyography Signals</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="deeplearning.html"><a href="deeplearning.html#overfitting"><i class="fa fa-check"></i><b>8.4</b> Overfitting</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="deeplearning.html"><a href="deeplearning.html#early-stopping"><i class="fa fa-check"></i><b>8.4.1</b> Early Stopping</a></li>
<li class="chapter" data-level="8.4.2" data-path="deeplearning.html"><a href="deeplearning.html#dropout"><i class="fa fa-check"></i><b>8.4.2</b> Dropout</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="deeplearning.html"><a href="deeplearning.html#fine-tuning-a-neural-network"><i class="fa fa-check"></i><b>8.5</b> Fine-tuning a Neural Network</a></li>
<li class="chapter" data-level="8.6" data-path="deeplearning.html"><a href="deeplearning.html#cnns"><i class="fa fa-check"></i><b>8.6</b> Convolutional Neural Networks</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="deeplearning.html"><a href="deeplearning.html#convolutions"><i class="fa fa-check"></i><b>8.6.1</b> Convolutions</a></li>
<li class="chapter" data-level="8.6.2" data-path="deeplearning.html"><a href="deeplearning.html#pooling-operations"><i class="fa fa-check"></i><b>8.6.2</b> Pooling Operations</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="deeplearning.html"><a href="deeplearning.html#cnns-with-keras"><i class="fa fa-check"></i><b>8.7</b> CNNs with Keras</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="deeplearning.html"><a href="deeplearning.html#example-1"><i class="fa fa-check"></i><b>8.7.1</b> Example 1</a></li>
<li class="chapter" data-level="8.7.2" data-path="deeplearning.html"><a href="deeplearning.html#example-2"><i class="fa fa-check"></i><b>8.7.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="deeplearning.html"><a href="deeplearning.html#cnnSmile"><i class="fa fa-check"></i><b>8.8</b> Smiles Detection with a CNN</a></li>
<li class="chapter" data-level="8.9" data-path="deeplearning.html"><a href="deeplearning.html#SummaryDeepLearning"><i class="fa fa-check"></i><b>8.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multiuser.html"><a href="multiuser.html"><i class="fa fa-check"></i><b>9</b> Multi-user Validation</a>
<ul>
<li class="chapter" data-level="9.1" data-path="multiuser.html"><a href="multiuser.html#mixed-models"><i class="fa fa-check"></i><b>9.1</b> Mixed Models</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="multiuser.html"><a href="multiuser.html#skeleton-action-recognition-with-mixed-models"><i class="fa fa-check"></i><b>9.1.1</b> Skeleton Action Recognition with Mixed Models</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="multiuser.html"><a href="multiuser.html#user-independent-models"><i class="fa fa-check"></i><b>9.2</b> User-independent Models</a></li>
<li class="chapter" data-level="9.3" data-path="multiuser.html"><a href="multiuser.html#user-dependent-models"><i class="fa fa-check"></i><b>9.3</b> User-dependent Models</a></li>
<li class="chapter" data-level="9.4" data-path="multiuser.html"><a href="multiuser.html#user-adaptive-models"><i class="fa fa-check"></i><b>9.4</b> User-adaptive Models</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="multiuser.html"><a href="multiuser.html#transfer-learning"><i class="fa fa-check"></i><b>9.4.1</b> Transfer Learning</a></li>
<li class="chapter" data-level="9.4.2" data-path="multiuser.html"><a href="multiuser.html#a-user-adaptive-model-for-activity-recognition"><i class="fa fa-check"></i><b>9.4.2</b> A User-adaptive Model for Activity Recognition</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="multiuser.html"><a href="multiuser.html#SummaryMultiUser"><i class="fa fa-check"></i><b>9.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html"><i class="fa fa-check"></i><b>10</b> Detecting Abnormal Behaviors</a>
<ul>
<li class="chapter" data-level="10.1" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#isolation-forests"><i class="fa fa-check"></i><b>10.1</b> Isolation Forests</a></li>
<li class="chapter" data-level="10.2" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#detecting-abnormal-fish-behaviors"><i class="fa fa-check"></i><b>10.2</b> Detecting Abnormal Fish Behaviors</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#exploring-and-visualizing-trajectories"><i class="fa fa-check"></i><b>10.2.1</b> Exploring and Visualizing Trajectories</a></li>
<li class="chapter" data-level="10.2.2" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#preprocessing-and-feature-extraction"><i class="fa fa-check"></i><b>10.2.2</b> Preprocessing and Feature Extraction</a></li>
<li class="chapter" data-level="10.2.3" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#training-the-model"><i class="fa fa-check"></i><b>10.2.3</b> Training the Model</a></li>
<li class="chapter" data-level="10.2.4" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#roc-curve-and-auc"><i class="fa fa-check"></i><b>10.2.4</b> ROC Curve and AUC</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#autoencoders"><i class="fa fa-check"></i><b>10.3</b> Autoencoders</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#autoencoders-for-anomaly-detection"><i class="fa fa-check"></i><b>10.3.1</b> Autoencoders for Anomaly Detection</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#SummaryAnomalyDetection"><i class="fa fa-check"></i><b>10.4</b> Summary</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendixInstall.html"><a href="appendixInstall.html"><i class="fa fa-check"></i><b>A</b> Setup Your Environment</a>
<ul>
<li class="chapter" data-level="A.1" data-path="appendixInstall.html"><a href="appendixInstall.html#installing-the-datasets"><i class="fa fa-check"></i><b>A.1</b> Installing the Datasets</a></li>
<li class="chapter" data-level="A.2" data-path="appendixInstall.html"><a href="appendixInstall.html#installing-the-examples-source-code"><i class="fa fa-check"></i><b>A.2</b> Installing the Examples Source Code</a></li>
<li class="chapter" data-level="A.3" data-path="appendixInstall.html"><a href="appendixInstall.html#running-shiny-apps"><i class="fa fa-check"></i><b>A.3</b> Running Shiny Apps</a></li>
<li class="chapter" data-level="A.4" data-path="appendixInstall.html"><a href="appendixInstall.html#installing-keras-and-tensorflow"><i class="fa fa-check"></i><b>A.4</b> Installing Keras and TensorFlow</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendixDatasets.html"><a href="appendixDatasets.html"><i class="fa fa-check"></i><b>B</b> Datasets</a>
<ul>
<li class="chapter" data-level="B.1" data-path="appendixDatasets.html"><a href="appendixDatasets.html#complex-activities"><i class="fa fa-check"></i><b>B.1</b> COMPLEX ACTIVITIES</a></li>
<li class="chapter" data-level="B.2" data-path="appendixDatasets.html"><a href="appendixDatasets.html#depresjon"><i class="fa fa-check"></i><b>B.2</b> DEPRESJON</a></li>
<li class="chapter" data-level="B.3" data-path="appendixDatasets.html"><a href="appendixDatasets.html#electromyography"><i class="fa fa-check"></i><b>B.3</b> ELECTROMYOGRAPHY</a></li>
<li class="chapter" data-level="B.4" data-path="appendixDatasets.html"><a href="appendixDatasets.html#fish-trajectories"><i class="fa fa-check"></i><b>B.4</b> FISH TRAJECTORIES</a></li>
<li class="chapter" data-level="B.5" data-path="appendixDatasets.html"><a href="appendixDatasets.html#hand-gestures"><i class="fa fa-check"></i><b>B.5</b> HAND GESTURES</a></li>
<li class="chapter" data-level="B.6" data-path="appendixDatasets.html"><a href="appendixDatasets.html#home-tasks"><i class="fa fa-check"></i><b>B.6</b> HOME TASKS</a></li>
<li class="chapter" data-level="B.7" data-path="appendixDatasets.html"><a href="appendixDatasets.html#homicide-reports"><i class="fa fa-check"></i><b>B.7</b> HOMICIDE REPORTS</a></li>
<li class="chapter" data-level="B.8" data-path="appendixDatasets.html"><a href="appendixDatasets.html#indoor-location"><i class="fa fa-check"></i><b>B.8</b> INDOOR LOCATION</a></li>
<li class="chapter" data-level="B.9" data-path="appendixDatasets.html"><a href="appendixDatasets.html#sheep-goats"><i class="fa fa-check"></i><b>B.9</b> SHEEP GOATS</a></li>
<li class="chapter" data-level="B.10" data-path="appendixDatasets.html"><a href="appendixDatasets.html#skeleton-actions"><i class="fa fa-check"></i><b>B.10</b> SKELETON ACTIONS</a></li>
<li class="chapter" data-level="B.11" data-path="appendixDatasets.html"><a href="appendixDatasets.html#smartphone-activities"><i class="fa fa-check"></i><b>B.11</b> SMARTPHONE ACTIVITIES</a></li>
<li class="chapter" data-level="B.12" data-path="appendixDatasets.html"><a href="appendixDatasets.html#smiles"><i class="fa fa-check"></i><b>B.12</b> SMILES</a></li>
<li class="chapter" data-level="B.13" data-path="appendixDatasets.html"><a href="appendixDatasets.html#students-mental-health"><i class="fa fa-check"></i><b>B.13</b> STUDENTS’ MENTAL HEALTH</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="citing-this-book.html"><a href="citing-this-book.html"><i class="fa fa-check"></i>Citing this Book</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Behavior Analysis with Machine Learning Using R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="intro" class="section level1" number="1">
<h1><span class="header-section-number">Chapter 1</span> Introduction to Behavior and Machine Learning</h1>
<p>In the last years, machine learning has surged as one of the key technologies that enables and supports many of the services and products that we use in our everyday lives and is expanding quickly. Machine learning has also helped to accelerate research and development in almost every field including natural sciences, engineering, social sciences, medicine, art and culture. Even though all those fields (and their respective sub-fields) are very diverse, most of them have something in common: They involve living organisms (cells, microbes, plants, humans, animals, etc.) and living organisms express behaviors. This book teaches you machine learning and data-driven methods to analyze different types of behaviors. Some of those methods include supervised, unsupervised, and deep learning. You will also learn how to explore, encode, preprocess, and visualize behavioral data. While the examples in this book focus on behavior analysis, the methods and techniques can be applied in any other context.</p>
<p>This chapter starts by introducing the concepts of <em>behavior</em> and <em>machine learning</em>. Next, basic machine learning terminology is presented and you will build your first classification and regression models. Then, you will learn how to evaluate the performance of your models and important concepts such as <em>underfitting</em>, <em>overfitting</em>, <em>bias</em>, and <em>variance</em>.</p>
<div id="what-is-behavior" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> What Is Behavior?</h2>
<p>Living organisms are constantly sensing and analyzing their surrounding environment. This includes inanimate objects but also other living entities. All of this is with the objective of making decisions and taking actions, either consciously or unconsciously. If we see someone running, we will react differently depending on whether we are at a stadium or in a bank. At the same time, we may also analyze other cues such as the runner’s facial expressions, clothes, items, and the reactions of the other people around us. Based on this aggregated information, we can decide how to react and behave. All this is supported by the organisms’ sensing capabilities and decision-making processes (the brain and/or chemical reactions). Understanding our environment and how others behave is crucial for conducting our everyday life activities and provides support for other tasks. But, <strong>what is behavior</strong>? The Cambridge dictionary defines behavior as:</p>
<blockquote>
<p><em>“the way that a person, an animal, a substance, etc. behaves in a particular situation or under particular conditions.”</em></p>
</blockquote>
<p>Another definition by dictionary.com is:</p>
<blockquote>
<p><em>“observable activity in a human or animal.”</em></p>
</blockquote>
<p>The definitions are similar and both include humans and animals. Following those definitions, this book will focus on the automatic analysis of human and animal behaviors however, the methods can also be applied to robots and to a wide variety of problems in different domains. There are three main reasons why one may want to analyze behaviors in an automatic manner:</p>
<ol style="list-style-type: decimal">
<li><p><strong>React.</strong> A biological or an artificial agent (or a combination of both) can take actions based on what is happening in the surrounding environment. For example, if suspicious behavior is detected in an airport, preventive actions can be triggered by security systems and the corresponding authorities. Without the possibility to automate such a detection system, it would be infeasible to implement it in practice. Just imagine trying to analyze airport traffic by hand.</p></li>
<li><p><strong>Understand.</strong> Analyzing the behavior of an organism can help us to understand other associated behaviors and processes and to answer research questions. For example, <span class="citation"><a href="#ref-williams2020" role="doc-biblioref">Williams et al.</a> (<a href="#ref-williams2020" role="doc-biblioref">2020</a>)</span> found that <em>Andean condors</em>, the heaviest soaring bird (see Figure <a href="intro.html#fig:condor">1.1</a>), only flap their wings for about <span class="math inline">\(1\%\)</span> of their total flight time. In one of the cases, a condor flew <span class="math inline">\(\approx 172\)</span> km without flapping. Those findings were the result of analyzing the birds’ behavior from data recorded by bio-logging devices. In this book, several examples that make use of inertial devices will be studied.</p></li>
</ol>

<div class="figure" style="text-align: center"><span id="fig:condor"></span>
<img src="images/condor.jpg" alt="Andean condor. (Hugo Pédel, France, Travail personnel. Cliché réalisé dans le Parc National Argentin Nahuel Huapi, San Carlos de Bariloche, Laguna Tonchek. Source: Wikipedia (CC BY-SA 3.0) [https://creativecommons.org/licenses/by-sa/3.0/legalcode])." width="50%" />
<p class="caption">
FIGURE 1.1: Andean condor. (Hugo Pédel, France, Travail personnel. Cliché réalisé dans le Parc National Argentin Nahuel Huapi, San Carlos de Bariloche, Laguna Tonchek. Source: Wikipedia (CC BY-SA 3.0) [<a href="https://creativecommons.org/licenses/by-sa/3.0/legalcode" class="uri">https://creativecommons.org/licenses/by-sa/3.0/legalcode</a>]).
</p>
</div>
<ol start="3" style="list-style-type: decimal">
<li><strong>Document and Archive.</strong> Finally, we may want to document certain behaviors for future use. It could be for evidence purposes or maybe it is not clear how the information can be used now but may come in handy later. Based on the archived information, one could gain new knowledge in the future and use it to react (take decisions/actions), as shown in Figure <a href="intro.html#fig:decisionsActions">1.2</a>. For example, we could document our nutritional habits (what do we eat, how often, etc.). If there is a health issue, a specialist could use this historical information to make a more precise diagnosis and propose actions.</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:decisionsActions"></span>
<img src="images/decisions.png" alt="Taking decisions from archived behaviors." width="70%" />
<p class="caption">
FIGURE 1.2: Taking decisions from archived behaviors.
</p>
</div>
<p>Some behaviors can be used as a proxy to understand other behaviors, states, and/or processes. For example, detecting body movement behaviors during a job interview could serve as the basis to understand stress levels. Behaviors can also be modeled as a composition of lower-level behaviors. In chapter <a href="representations.html#representations">7</a>, a method called <em>Bag of Words</em> that can be used to decompose complex behaviors into a set of simpler ones will be presented.</p>
<p>In order to analyze and monitor behaviors, we need a way to observe them. Living organisms use their available senses such as eyesight, hearing, smell, echolocation (bats, dolphins), thermal senses (snakes, mosquitoes), etc. In the case of machines, they need <em>sensors</em> to accomplish or approximate those tasks, for example color and thermal cameras, microphones, temperature sensors, and so on.</p>
<p>The reduction in the size of sensors has allowed the development of more powerful wearable devices. <em>Wearable devices</em> are electronic devices that are worn by a user, usually as accessories or embedded in clothes. Examples of wearable devices are smartphones, smartwatches, fitness bracelets, actigraphy watches, etc. These devices have embedded sensors that allow them to monitor different aspects of a user such as activity levels, blood pressure, temperature, and location, to name a few. Examples of sensors that can be found in those devices are accelerometers, magnetometers, gyroscopes, heart rate, microphones, Wi-Fi, Bluetooth, Galvanic skin response (GSR), etc.</p>
<p>Several of those sensors were initially used for some specific purposes. For example, accelerometers in smartphones were intended to be used for gaming or detecting the device’s orientation. Later, some people started to propose and implement new use cases such as activity recognition <span class="citation">(<a href="#ref-shoaib2015" role="doc-biblioref">Shoaib et al. 2015</a>)</span> and fall detection. The magnetometer, which measures the earth’s magnetic field, was mainly used with map applications to determine the orientation of the device, but later, it was found that it can also be used for indoor location purposes <span class="citation">(<a href="#ref-brena2017" role="doc-biblioref">Brena et al. 2017</a>)</span>.</p>
<p>In general, wearable devices have been successfully applied to track different types of behaviors such as physical activity, sports activities, location, and even mental health states <span class="citation">(<a href="#ref-garciaSurvey2018" role="doc-biblioref">Garcia-Ceja, Riegler, Nordgreen, et al. 2018</a>)</span>. Those devices generate a lot of raw data, but it will be our task to process and analyze it. Doing it by hand becomes impossible given the large amounts of data and their variety. In this book, several <em>machine learning</em> methods will be introduced that will allow you to extract and analyze different types of behaviors from data. The next section will begin with an introduction to machine learning. The rest of this chapter will introduce the required machine learning concepts before we start analyzing behaviors in chapter <a href="classification.html#classification">2</a>.</p>
</div>
<div id="what-is-machine-learning" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> What Is Machine Learning?</h2>
<p>You can think of <em>machine learning</em> as a set of computational algorithms that <em>automatically</em> find useful patterns and relationships from data. Here, the keyword is <em>automatic</em>. When trying to solve a problem, one can hard-code a predefined set of rules, for example, chained if-else conditions. For instance, if we want to detect if the object in a picture is an <em>orange</em> or a <em>pear</em>, we can do something like:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="intro.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span>(number_green_pixels <span class="sc">&gt;</span> <span class="dv">90</span>%)</span>
<span id="cb1-2"><a href="intro.html#cb1-2" aria-hidden="true" tabindex="-1"></a>  return <span class="st">&quot;pear&quot;</span></span>
<span id="cb1-3"><a href="intro.html#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span></span>
<span id="cb1-4"><a href="intro.html#cb1-4" aria-hidden="true" tabindex="-1"></a>  return <span class="st">&quot;orange&quot;</span></span></code></pre></div>
<p>This simple rule should work well and will do the job. Imagine that now your boss tells you that the system needs to recognize <em>green apples</em> as well. Our previous rule will no longer work, and we will need to include additional rules and thresholds. On the other hand, a machine learning algorithm will automatically learn such rules based on the updated data. So, you only need to update your data with examples of <em>green apples</em> and “click” the re-train button!</p>
<p>The result of <em>learning</em> is <em>knowledge</em> that the system can use to solve new instances of a problem. In this case, when you show a new image to the system, it should be able to recognize the type of fruit. Figure <a href="intro.html#fig:mlPhases">1.3</a> shows this general idea.</p>
<div class="figure" style="text-align: center"><span id="fig:mlPhases"></span>
<img src="images/ml.png" alt="Overall Machine Learning phases. The '?' represents the new unknown object for which we want to obtain a prediction using the learned model." width="100%" />
<p class="caption">
FIGURE 1.3: Overall Machine Learning phases. The ‘?’ represents the new unknown object for which we want to obtain a prediction using the learned model.
</p>
</div>

<div class="rmdinfo">
For more formal definitions of machine learning, I recommend you check <span class="citation">(<a href="#ref-kononenko2007" role="doc-biblioref">Kononenko and Kukar 2007</a>)</span>.
</div>
<p>Machine learning methods rely on three main building blocks:</p>
<ul>
<li><strong>Data</strong></li>
<li><strong>Algorithms</strong></li>
<li><strong>Models</strong></li>
</ul>
<p>Every machine learning method needs <strong>data</strong> to learn from. For the example of the fruits, we need examples of images for each type of fruit we want to recognize. Additionally, we need their corresponding output types (labels) so the algorithm can learn how to associate each image with its corresponding label.</p>

<div class="rmdinfo">
Not every machine learning method needs the expected output or labels (more on this in the Taxonomy section <a href="intro.html#taxonomy">1.3</a>).
</div>
<p>Typically, an <strong>algorithm</strong> will use the <strong>data</strong> to learn a <strong>model</strong>. This is called the learning or training phase. The learned <strong>model</strong> can then be used to generate predictions when presented with new data. The data used to train the models is called the <strong>train set</strong>. Since we need a way to test how the model will perform once it is deployed in a real setting (in production), we also need what is known as the <strong>test set</strong>. The test set is used to estimate the model’s performance on data it has never seen before (more on this will be presented in section <a href="intro.html#trainingeval">1.6</a>).</p>
</div>
<div id="taxonomy" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Types of Machine Learning</h2>
<p>Machine learning methods can be grouped into different types. Figure <a href="intro.html#fig:mlTaxonomy">1.4</a> depicts a categorization of machine learning ‘types.’ This figure is based on <span class="citation">(<a href="#ref-biecek2012" role="doc-biblioref">Biecek et al. 2012</a>)</span>. The <span class="math inline">\(x\)</span> axis represents the certainty of the labels and the <span class="math inline">\(y\)</span> axis the percent of training data that is labeled. In the previous example, the labels are the names of the fruits associated with each image.</p>

<div class="figure" style="text-align: center"><span id="fig:mlTaxonomy"></span>
<img src="images/taxonomy.png" alt="Machine learning taxonomy. (Adapted from Biecek, Przemyslaw, et al. “The R package bgmm: mixture modeling with uncertain knowledge.” Journal of Statistical Software 47.i03 (2012). (CC BY 3.0) [https://creativecommons.org/licenses/by/3.0/legalcode])." width="70%" />
<p class="caption">
FIGURE 1.4: Machine learning taxonomy. (Adapted from Biecek, Przemyslaw, et al. “The R package bgmm: mixture modeling with uncertain knowledge.” <em>Journal of Statistical Software</em> 47.i03 (2012). (CC BY 3.0) [<a href="https://creativecommons.org/licenses/by/3.0/legalcode" class="uri">https://creativecommons.org/licenses/by/3.0/legalcode</a>]).
</p>
</div>
<p>From the figure, four main types of machine learning methods can be observed:</p>
<ul>
<li><strong>Supervised learning.</strong> In this case, <span class="math inline">\(100\%\)</span> of the training data is labeled and the certainty of those labels is <span class="math inline">\(100\%\)</span>. This is like the fruits example. For every image used to train the system, the respective label is also known and there is no uncertainty about the label. When the expected output is a category (the type of fruit), this is called <strong>classification</strong>. Examples of classification models (a.k.a classifiers) are decision trees, <span class="math inline">\(k\)</span>-Nearest Neighbors, Random Forest, neural networks, etc. When the output is a real number (e.g., temperature), it is called <strong>regression</strong>. Examples of regression models are linear regression, regression trees, neural networks, Random Forest, <span class="math inline">\(k\)</span>-Nearest Neighbors, etc. Note that some models can be used for both classification and regression. A supervised learning problem can be formalized as follows:</li>
</ul>
<p><span class="math display" id="eq:supervisedLearning">\[\begin{equation}
  f\left(x\right) = y
  \tag{1.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(f\)</span> is a function that maps some input data <span class="math inline">\(x\)</span> (for example images) to an output <span class="math inline">\(y\)</span> (types of fruits). Usually, an <strong>algorithm</strong> will try to <em>learn</em> the best <strong>model</strong> <span class="math inline">\(f\)</span> given some <strong>data</strong> consisting of <span class="math inline">\(n\)</span> pairs <span class="math inline">\((x,y)\)</span> of examples. During learning, the algorithm has access to the expected output/label <span class="math inline">\(y\)</span> for each input <span class="math inline">\(x\)</span>. At <em>inference time</em>, that is, when we want to make predictions for new examples, we can use the learned model <span class="math inline">\(f\)</span> and feed it with a new input <span class="math inline">\(x\)</span> to obtain the corresponding predicted value <span class="math inline">\(y\)</span>.</p>
<ul>
<li><p><strong>Semi-supervised learning.</strong> This is the case when the certainty of the labels is <span class="math inline">\(100\%\)</span> but not all training data are labeled. Usually, this scenario considers the case when only a very small proportion of the data is labeled. That is, the dataset contains pairs of examples of the form <span class="math inline">\((x,y)\)</span> but also examples where <span class="math inline">\(y\)</span> is missing <span class="math inline">\((x,?)\)</span>. In supervised learning, both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> must be present. On the other hand, semi-supervised algorithms can learn even if some examples are missing the expected output <span class="math inline">\(y\)</span>. This is a common situation in real life since labeling data can be expensive and time-consuming. In the fruits example, someone needs to tag every training image manually before training a model. Semi-supervised learning methods try to extract information also from the unlabeled data to improve the models. Examples of some semi-supervised learning methods are self-learning, co-training, and tri-training. <span class="citation">(<a href="#ref-trigueroselflabeled" role="doc-biblioref">Triguero, García, and Herrera 2013</a>)</span>.</p></li>
<li><p><strong>Partially-supervised learning.</strong> This is a generalization that encompasses supervised and semi-supervised learning. Here, the examples have uncertain (<em>soft</em>) labels. For example, the label of a fruit image instead of being an <em>‘orange’</em> or <em>‘pear’</em> could be a vector <span class="math inline">\([0.7, 0.3]\)</span> where the first element is the probability that the image corresponds to an orange and the second element is the probability that it’s a pear. Maybe the image was not very clear, and these are the beliefs of the person tagging the images encoded as probabilities. Examples of models that can be used for partially-supervised learning are mixture models with belief functions <span class="citation">(<a href="#ref-comelearning" role="doc-biblioref">Côme et al. 2009</a>)</span> and neural networks.</p></li>
<li><p><strong>Unsupervised learning.</strong> This is the extreme case when none of the training examples have a label. That is, the dataset only has pairs <span class="math inline">\((x,?)\)</span>. Now, you may be wondering: If there are no labels, is it possible to extract information from these data? The answer is <em>yes</em>. Imagine you have fruit images with no labels. What you could try to do is to automatically group them into meaningful categories/groups. The categories could be the types of fruits themselves, i.e., trying to form groups in which images within the same category belong to the same type. In the fruits example, we could infer the true types by visually inspecting the images, but in many cases, visual inspection is difficult and the formed groups may not have an easy interpretation, but still, they can be very useful and can be used as a preprocessing step (like in vector quantization). These types of algorithms that find groups (hierarchical groups in some cases) are called <strong>clustering methods</strong>. Examples of clustering methods are <span class="math inline">\(k\)</span>-means, <span class="math inline">\(k\)</span>-medoids, and hierarchical clustering. Clustering algorithms are not the only unsupervised learning methods. Association rules, word embeddings, and autoencoders are examples of other unsupervised learning methods. <em>Note:</em> Some people may claim that word embeddings and autoencoders are not fully unsupervised methods but for practical purposes, this is not relevant.</p></li>
</ul>
<p>Additionally, there is another type of machine learning called <strong>Reinforcement Learning (RL)</strong> which has substantial differences from the previous ones. This type of learning does not rely on example data as the previous ones but on stimuli from an agent’s environment. At any given point in time, an agent can perform an action which will lead it to a new state where a <em>reward</em> is collected. The aim is to learn the sequence of actions that maximize the reward. This type of learning is not covered in this book. A good introduction to the topic can be consulted here<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>.</p>
<p>This book will mainly cover supervised learning problems and more specifically, classification problems. For example, given a set of wearable sensor readings, we want to predict contextual information about a given user such as location, current activity, mood, and so on. Unsupervised learning methods (clustering and association rules) will be covered in chapter <a href="unsupervised.html#unsupervised">6</a> and autoencoders are introduced in chapter <a href="abnormalbehaviors.html#abnormalbehaviors">10</a>.</p>
</div>
<div id="terminology" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Terminology</h2>
<p>This section introduces some basic terminology that will be helpful for the rest of the book.</p>
<div id="tables" class="section level3" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> Tables</h3>
<p>Since data is the most important ingredient in machine learning, let’s start with some related terms. First, data needs to be stored/structured so it can be easily manipulated and processed. Most of the time, datasets will be stored as <em>tables</em> or in R terminology, <em>data frames</em>. Figure <a href="intro.html#fig:terminology1">1.5</a> shows the classic <code>mtcars</code> dataset<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> stored in a data frame.</p>

<div class="figure" style="text-align: center"><span id="fig:terminology1"></span>
<img src="images/terminologyTable.png" alt="Table/Data frame components. Source: Data from the 1974 Motor Trend US magazine." width="90%" />
<p class="caption">
FIGURE 1.5: Table/Data frame components. Source: Data from the 1974 Motor Trend US magazine.
</p>
</div>
<p>The columns represent <em>variables</em> and the rows represent <em>examples</em> also known as <em>instances</em> or <em>data points</em>. In this table, there are <span class="math inline">\(5\)</span> variables <em>mpg</em>, <em>cyl</em>, <em>disp</em>, <em>hp</em> and the <em>model</em> (the first column). In this example, the first column does not have a name, but it is still a variable. Each row represents a specific car model with its values per variable. In machine learning terminology, rows are more commonly called <em>instances</em> whereas in statistics they are often called <em>data points</em> or <em>observations</em>. Here, those terms will be used interchangeably.</p>
<p>Figure <a href="intro.html#fig:terminology2">1.6</a> shows a data frame for the <code>iris</code> dataset which consists of different kinds of plants <span class="citation">(<a href="#ref-Fisher1936" role="doc-biblioref">Fisher 1936</a>)</span>. Suppose that we are interested in predicting the <em>Species</em> based on the other variables. In machine learning terminology, the variable of interest (the one that depends on the others) is called the <em>class</em> or <em>label</em> for classification problems. For regression, it is often referred to as <em>y</em>. In statistics, it is more commonly known as the <em>response</em>, <em>dependent</em>, or <em>y</em> variable, for both classification and regression.</p>
<p>In machine learning terminology, the rest of the variables are called <em>features</em> or <em>attributes</em>. In statistics, they are called <em>predictors</em>, <em>independent variables</em>, or just <em>X</em>. From the context, most of the time it should be easy to identify dependent from independent variables regardless of the used terminology. The word <strong>feature vector</strong> is also very common in machine learning. A feature vector is just a structure containing the features of a given instance. For example, the features of the first instance in Figure <a href="intro.html#fig:terminology2">1.6</a> can be stored as a feature vector <span class="math inline">\([5.4,3.9,1.3,0.4]\)</span> of size <span class="math inline">\(4\)</span>. In a programming language, this can be implemented with an array.</p>

<div class="figure" style="text-align: center"><span id="fig:terminology2"></span>
<img src="images/terminologyTable2.png" alt="Table/Data frame components (cont.). Source: Data from Fisher, Ronald A., “The Use of Multiple Measurements in Taxonomic Problems.” Annals of Eugenics 7, no. 2 (1936): 179–88." width="90%" />
<p class="caption">
FIGURE 1.6: Table/Data frame components (cont.). Source: Data from Fisher, Ronald A., “The Use of Multiple Measurements in Taxonomic Problems.” <em>Annals of Eugenics</em> 7, no. 2 (1936): 179–88.
</p>
</div>
</div>
<div id="variable-types" class="section level3" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> Variable Types</h3>
<p>When working with machine learning algorithms, the following are the most commonly used variable types. Here, when I talk about variable types, I do not refer to programming-language-specific data types (int, boolean, string, etc.) but to more general types regardless of the underlying implementation for each specific programming language.</p>
<ul>
<li><p><strong>Categorical/Nominal:</strong> These variables take values from a discrete set of possible values (categories). For example, the categorical variable <em>color</em> can take the values <em>‘red’</em>, <em>‘green’</em>, <em>‘black’</em>, and so on. Categorical variables do not have an ordering.</p></li>
<li><p><strong>Numeric:</strong> Real values such as height, weight, price, etc.</p></li>
<li><p><strong>Integer:</strong> Integer values such as number of rooms, age, number of floors, etc.</p></li>
<li><p><strong>Ordinal:</strong> Similar to categorical variables, these take their values from a set of discrete values, but they do have an ordering. For example, low &lt; medium &lt; high.</p></li>
</ul>
</div>
<div id="predictive-models" class="section level3" number="1.4.3">
<h3><span class="header-section-number">1.4.3</span> Predictive Models</h3>
<p>In machine learning terminology, a <em>predictive model</em> is a model that takes some input and produces an output. <em>Classifiers</em> and <em>Regressors</em> are predictive models. I will use the terms classifier/model and regressor/model interchangeably.</p>
</div>
</div>
<div id="pipeline" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> Data Analysis Pipeline</h2>
<p>Usually, the data analysis pipeline consists of several steps which are depicted in Figure <a href="intro.html#fig:pipeline">1.7</a>. This is not a complete list but includes the most common steps. It all starts with the data collection. Then the data exploration and so on, until the results are presented. These steps can be followed in sequence, but you can always jump from one step to another one. In fact, most of the time you will end up using an iterative approach by going from one step to the other (forward or backward) as needed.</p>
<div class="figure" style="text-align: center"><span id="fig:pipeline"></span>
<img src="images/pipeline.png" alt="Data analysis pipeline." width="100%" />
<p class="caption">
FIGURE 1.7: Data analysis pipeline.
</p>
</div>
<p>The big gray box at the bottom means that machine learning methods can be used in all those steps and not just during training or evaluation. For example, one may use dimensionality reduction methods in the <em>data exploration</em> phase to plot the data or classification/regression methods in the <em>cleaning</em> phase to impute missing values. Now, let’s give a brief description of each of those phases:</p>
<ul>
<li><p><strong>Data exploration.</strong> This step aims to familiarize yourself and understand the data so you can make informed decisions during the following steps. Some of the tasks involved in this phase include summarizing your data, generating plots, validating assumptions, and so on. During this phase you can, for example, identify outliers, missing values, or noisy data points that can be cleaned in the next phase. Chapter <a href="edavis.html#edavis">4</a> will introduce some data exploration techniques. Throughout the book, we will also use some other data exploratory methods but if you are interested in diving deeper into this topic, I recommend you check out the “Exploratory Data Analysis with R” book by <span class="citation"><a href="#ref-peng2016" role="doc-biblioref">Peng</a> (<a href="#ref-peng2016" role="doc-biblioref">2016</a>)</span>.</p></li>
<li><p><strong>Data cleaning.</strong> After the data exploration phase, we can remove the identified outliers, remove noisy data points, remove variables that are not needed for further computation, and so on.</p></li>
<li><p><strong>Preprocessing.</strong> Predictive models expect the data to be in some structured format and satisfying some constraints. For example, several models are sensitive to class imbalances, i.e., the presence of many instances with a given class but a small number of instances with other classes. In fraud detection scenarios, most of the instances will belong to the normal class but just a small proportion will be of type <em>‘illegal transaction’</em>. In this case, we may want to do some preprocessing to try to balance the dataset. Some models are also sensitive to feature-scale differences. For example, a variable <em>weight</em> could be in kilograms but another variable <em>height</em> in centimeters. Before training a predictive model, the data needs to be prepared in such a way that the models can get the most out of it. Chapter <a href="preprocessing.html#preprocessing">5</a> will present some common preprocessing steps.</p></li>
<li><p><strong>Training and evaluation.</strong> Once the data is preprocessed, we can proceed to train the models. Furthermore, we also need ways to evaluate their generalization performance on new unseen instances. The purpose of this phase is to try, and fine-tune different models to find the one that performs the best. Later in this chapter, some model evaluation techniques will be introduced.</p></li>
<li><p><strong>Interpretation and presentation of results.</strong> The purpose of this phase is to analyze and interpret the models’ results. We can use performance metrics derived from the evaluation phase to make informed decisions. We may also want to understand how the models work internally and how the predictions are derived.</p></li>
</ul>
</div>
<div id="trainingeval" class="section level2" number="1.6">
<h2><span class="header-section-number">1.6</span> Evaluating Predictive Models</h2>
<p>Before showing you how to train a machine learning model, in this section, I would like to introduce the process of <strong>evaluating</strong> a predictive model, which is part of the data analysis pipeline. This applies to both classification and regression problems. I’m starting with this topic because it will be a recurring one every time you work with machine learning. You will also be training a lot of models, but you will need ways to validate them as well.</p>
<p>Once you have trained a model (with a training set), that is, finding the best function <span class="math inline">\(f\)</span> that maps inputs to their corresponding outputs, you may want to estimate how good the model is at solving a particular problem when presented with examples it has never seen before (that were not part of the training set). This estimate of how good the model is at predicting the output of new examples is called the <strong>generalization performance</strong>.</p>
<p>To estimate the generalization performance of a model, a dataset is usually divided into a <em>train set</em> and a <em>test set</em>. As the name implies, the train set is used to train the model (learn its parameters) and the test set is used to evaluate/test its generalization performance. We need independent sets because when deploying models in the wild, they will be presented with new instances never seen before. By dividing the dataset into two subsets, we are simulating this scenario where the test set instances were never seen by the model at training time so the performance estimate will be more accurate rather than if we used the same set to train and then to evaluate the performance. There are two main validation methods that differ in the way the dataset is divided into train and test sets: <em>hold-out validation</em> and <em>k-fold cross validation</em>.</p>
<p><strong>1) Hold-out validation.</strong> This method randomly splits the dataset into train and test sets based on some predefined percentages. For example, randomly select <span class="math inline">\(70\%\)</span> of the instances and use them as the train set and use the remaining <span class="math inline">\(30\%\)</span> of the examples for the test set. This will vary depending on the application and the amount of data, but typical splits are <span class="math inline">\(50/50\)</span> and <span class="math inline">\(70/30\)</span> percent for the train and test sets, respectively. Figure <a href="intro.html#fig:holdout">1.8</a> shows an example of a dataset divided into <span class="math inline">\(70/30\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:holdout"></span>
<img src="images/holdout.png" alt="Hold-out validation." width="70%" />
<p class="caption">
FIGURE 1.8: Hold-out validation.
</p>
</div>
<p>Then, the train set is used to train (fit) a model, and the test set to evaluate how well that model performs on new data. The performance can be measured using performance metrics such as the <em>accuracy</em> for classification problems. The accuracy is the percent of correctly classified instances.</p>

<div class="rmdgoodpractice">
It is a good practice to estimate the performance on both, the train and test sets. Usually, the performance on the train set will be better since the model was trained with that very same data. It is also common to measure the performance computing the error instead of accuracy. For example, the percent of misclassified instances. These are called the <em>train error</em> and <em>test error</em> (also known as the <em>generalization error</em>), for both the train and test sets, respectively. Estimating these two errors will allow you to ‘debug’ your models and understand if they are underfitting or overfitting (more on this in the following sections).
</div>
<p><strong>2) <span class="math inline">\(k\)</span>-fold cross-validation.</strong> Hold-out validation is a good way to evaluate your models when you have a lot of data. However, in many cases, your data will be limited. In those cases, you want to make efficient use of the data. With hold-out validation, each instance is included either in the train or test set. <span class="math inline">\(k\)</span>-fold cross-validation provides a way in which instances take part in both, the test and train set, thus making more efficient use of the data.</p>
<p>This method consists of randomly assigning each instance into one of <span class="math inline">\(k\)</span> folds (subsets) with approximately the same size. Then, <span class="math inline">\(k\)</span> iterations are performed. In each iteration, one of the folds is used to test the model while the remaining ones are used to train it. Each fold is used once as the test set and <span class="math inline">\(k-1\)</span> times as part of the train set. Typical values for <span class="math inline">\(k\)</span> are <span class="math inline">\(3\)</span>, <span class="math inline">\(5\)</span>, and <span class="math inline">\(10\)</span>. In the extreme case where <span class="math inline">\(k\)</span> is equal to the total number of instances in the dataset, it is called leave-one-out cross-validation (LOOCV). Figure <a href="intro.html#fig:holdout">1.8</a> shows an example of cross-validation with <span class="math inline">\(k=5\)</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:crossvalidation"></span>
<img src="images/crossvalidation.png" alt="\(k\)-fold cross validation with \(k=5\) and \(5\) iterations." width="90%" />
<p class="caption">
FIGURE 1.9: <span class="math inline">\(k\)</span>-fold cross validation with <span class="math inline">\(k=5\)</span> and <span class="math inline">\(5\)</span> iterations.
</p>
</div>
<p>The generalization performance is then computed by taking the average accuracy/error from each iteration.</p>
<p>Hold-out validation is typically used when there is a lot of available data and models take significant time to be trained. On the other hand, <span class="math inline">\(k\)</span>-fold cross-validation is used when data is limited. However, it is more computational intensive since it requires training <span class="math inline">\(k\)</span> models.</p>
<p><strong>Validation set.</strong></p>
<p>Most predictive models require some hyperparameter tuning. For example, a <span class="math inline">\(k\)</span>-Nearest Neighbors model requires to set <span class="math inline">\(k\)</span>, the number of neighbors. For decision trees, one can specify the maximum allowed tree depth, among other hyperparameters. Neural networks require even more hyperparameter tuning to work properly. Also, one may try different preprocessing techniques and features. All those changes affect the final performance. If all those hyperparameter changes are evaluated using the test set, there is a risk of <em>overfitting</em> the model. That is, making the model very specific to this particular data. Instead of using the <em>test set</em> to fine-tune parameters, a <em>validation set</em> needs to be used instead. Thus, the dataset is randomly partitioned into three subsets: <strong>train/validation/test</strong> sets. The <em>train set</em> is used to train the model. The <em>validation set</em> is used to estimate the model’s performance while trying different hyperparameters and preprocessing methods. Once you are happy with your final model, you use the <em>test set</em> to assess the final generalization performance and this is what you report. The <strong>test set is used only once</strong>. Remember that we want to assess performance on unseen instances. When using <em>k-fold cross validation</em>, first, an independent test set needs to be put aside. Hyperparameters are tuned using cross-validation and the test set is used at the very end and just once to estimate the final performance.</p>

<div class="rmdinfo">
When working with multi-user systems, we need to additionally take into account between-user differences. In those situations, it is advised to perform extra validations. Those multi-user validation techniques will be covered in chapter <a href="multiuser.html#multiuser">9</a>.
</div>
</div>
<div id="simple-classification-example" class="section level2" number="1.7">
<h2><span class="header-section-number">1.7</span> Simple Classification Example</h2>

<div class="rmdfolder">
simple_model.R
</div>
<p>So far, a lot of terminology and concepts have been introduced. In this section, we will work through a practical example that will demonstrate how most of these concepts fit together. Here you will build (from scratch) your first classification and regression models! Furthermore, you will learn how to evaluate their generalization performance.</p>
<p>Suppose you have a dataset that contains information about felines including their maximum speed in km/hr and their specific type. For the sake of the example, suppose that these two variables are the only ones that we can observe. As for the types, consider that there are two possibilities: <em>‘tiger’</em> and <em>‘leopard’</em>. Figure <a href="intro.html#fig:felinesTable">1.10</a> shows the first <span class="math inline">\(10\)</span> instances (rows) of the dataset.</p>
<div class="figure" style="text-align: center"><span id="fig:felinesTable"></span>
<img src="images/felines_table.png" alt="First 10 instances of felines dataset." width="25%" />
<p class="caption">
FIGURE 1.10: First 10 instances of felines dataset.
</p>
</div>
<p>This table has <span class="math inline">\(2\)</span> variables: <em>speed</em> and <em>class</em>. The first one is a numeric variable. The second one is a categorical variable. In this case, it can take two possible values: <em>‘tiger’</em> or <em>‘leopard’</em>.</p>
<p>This dataset was synthetically created for illustration purposes, but I promise you that hereafter, we will mostly use real datasets!</p>
<p>The code to reproduce this example is available in the <em>‘Introduction to Behavior and Machine Learning’</em> folder in the script file <code>simple_model.R</code>. The script contains the code used to generate the dataset. The dataset is stored in a data frame named <code>dataset</code>. Let’s start by doing a simple exploratory analysis of the dataset. More detailed exploratory analysis methods will be presented in chapter <a href="edavis.html#edavis">4</a>. First, we can print the data frame dimensions with the <code>dim()</code> function.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="intro.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print number of rows and columns.</span></span>
<span id="cb2-2"><a href="intro.html#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(dataset)</span>
<span id="cb2-3"><a href="intro.html#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 100   2</span></span></code></pre></div>
<p>The output tells us that the data frame has <span class="math inline">\(100\)</span> rows and <span class="math inline">\(2\)</span> columns. Now we may be interested to know how many of those correspond to <em>tigers</em>. We can use the <code>table()</code> function to get that information.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="intro.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Count instances in each class.</span></span>
<span id="cb3-2"><a href="intro.html#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(dataset<span class="sc">$</span>class)</span>
<span id="cb3-3"><a href="intro.html#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; leopard   tiger </span></span>
<span id="cb3-4"><a href="intro.html#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      50      50 </span></span></code></pre></div>
<p>Here we see that <span class="math inline">\(50\)</span> instances are of type <em>‘leopard’</em> and also that <span class="math inline">\(50\)</span> instances are of type <em>‘tiger’</em>. In fact, this is how the dataset was intentionally generated. The next thing we can do is to compute some summary statistics for each column. R already provides a very convenient function for that purpose. Yes, it is the <code>summary()</code> function.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="intro.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute some summary statistics.</span></span>
<span id="cb4-2"><a href="intro.html#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(dataset)</span>
<span id="cb4-3"><a href="intro.html#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      speed           class   </span></span>
<span id="cb4-4"><a href="intro.html#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  Min.   :42.96   leopard:50  </span></span>
<span id="cb4-5"><a href="intro.html#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  1st Qu.:48.41   tiger  :50  </span></span>
<span id="cb4-6"><a href="intro.html#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  Median :51.12               </span></span>
<span id="cb4-7"><a href="intro.html#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  Mean   :51.53               </span></span>
<span id="cb4-8"><a href="intro.html#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  3rd Qu.:53.99               </span></span>
<span id="cb4-9"><a href="intro.html#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  Max.   :61.65               </span></span></code></pre></div>
<p>Since <em>speed</em> is a numeric variable, <code>summary()</code> computes some statistics like the mean, min, max, etc. The <em>class</em> variable is a factor. Thus, it returns row counts instead. In R, categorical variables are usually encoded as factors. It is similar to a string, but R treats factors in a special way. We can already appreciate that with the previous code snippet when the summary function returned class counts.</p>
<p>There are many other ways in which you can explore a dataset, but for now, let’s assume we already feel comfortable and that we have a good understanding of the data. Since this dataset is very simple, we won’t need to do any further data cleaning or preprocessing.</p>
<p>Now, imagine that you are asked to build a model that is able to predict the type of feline based on the observed attributes. In this case, the only thing we can observe is the <em>speed</em>. Our task is to build a function that maps speed measurements to classes. That is, we want to be able to predict the type of feline based on how fast it runs. According to the terminology presented in section <a href="intro.html#terminology">1.4</a>, <em>speed</em> would be a <strong>feature</strong> variable and <em>class</em> would be the <strong>class</strong> variable.</p>
<p>Based on the types of machine learning methods presented in section <a href="intro.html#taxonomy">1.3</a>, this one is a <strong>supervised learning</strong> problem because for each instance, the class is available. And, specifically, since we want to predict a category, this is a <strong>classification</strong> problem.</p>
<p>Before building our classification model, it would be worth plotting the data. Figure <a href="intro.html#fig:felineSpeeds">1.11</a> shows the speeds for both tigers and leopards.</p>
<div class="figure" style="text-align: center"><span id="fig:felineSpeeds"></span>
<img src="images/felineSpeeds.png" alt="Feline speeds with vertical dashed lines at the means." width="100%" />
<p class="caption">
FIGURE 1.11: Feline speeds with vertical dashed lines at the means.
</p>
</div>
<p>Here, I omitted the code for building the plot, but it is included in the script. I also added vertical dashed lines at the mean speeds for the two classes. From this plot, it seems that leopards are faster than tigers (with some exceptions). One thing we can note is that the data points are grouped around the mean values of their corresponding classes. That is, most of the tiger data points are closer to the mean speed for tigers and the same can be observed for leopards. Of course, there are some exceptions where an instance is closer to the mean of the opposite class. This could be because some tigers may be as fast as leopards. Some leopards may also be slower than the average, maybe because they are newborns or they are old. Unfortunately, we do not have more information, so the best we can do is use our single feature <em>speed</em>. We can use these insights to come up with a simple model that discriminates between the two classes based on this single feature variable.</p>
<p>One thing we can do for any new instance we want to classify is to compute its distance to the ‘center’ of each class and predict the class that is the closest one. In this case, the center is the mean value. We can formally define our model as the set of <span class="math inline">\(n\)</span> centrality measures where <span class="math inline">\(n\)</span> is the number of classes (<span class="math inline">\(2\)</span> in our example).</p>
<p><span class="math display" id="eq:simpleModel">\[\begin{equation}
  M = \{\mu_1,\dots ,\mu_n\}
  \tag{1.2}
\end{equation}\]</span></p>
<p>Those centrality measures (the class means in this particular case) are called the <strong>parameters</strong> of the model. Training a model consists of finding those optimal parameters that will allow us to achieve the best performance on new instances that were not part of the training data. In most cases, we will need an <strong>algorithm</strong> to find those parameters. In our example, the algorithm consists of simply computing the mean speed for each class. That is, for each class, sum all the corresponding speeds and divide them by the number of data points that belong to that class.</p>
<p>Once those parameters are found, we can start making predictions on new data points. This is called <em>inference</em> or <em>prediction</em>. In this case, when a new data point arrives, we can predict its class by computing its distance to each of the <span class="math inline">\(n\)</span> centrality measures in <span class="math inline">\(M\)</span> and return the class of the closest one.</p>
<p>The following function implements the training part of our model.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="intro.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple classifier that learns</span></span>
<span id="cb5-2"><a href="intro.html#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># a centrality measure for each class.</span></span>
<span id="cb5-3"><a href="intro.html#cb5-3" aria-hidden="true" tabindex="-1"></a>simple.model.train <span class="ot">&lt;-</span> <span class="cf">function</span>(data, <span class="at">centrality=</span>mean){</span>
<span id="cb5-4"><a href="intro.html#cb5-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-5"><a href="intro.html#cb5-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Store unique classes.</span></span>
<span id="cb5-6"><a href="intro.html#cb5-6" aria-hidden="true" tabindex="-1"></a>  classes <span class="ot">&lt;-</span> <span class="fu">unique</span>(data<span class="sc">$</span>class)</span>
<span id="cb5-7"><a href="intro.html#cb5-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-8"><a href="intro.html#cb5-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Define an array to store the learned parameters.</span></span>
<span id="cb5-9"><a href="intro.html#cb5-9" aria-hidden="true" tabindex="-1"></a>  params <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(classes))</span>
<span id="cb5-10"><a href="intro.html#cb5-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-11"><a href="intro.html#cb5-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Make this a named array.</span></span>
<span id="cb5-12"><a href="intro.html#cb5-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">names</span>(params) <span class="ot">&lt;-</span> classes</span>
<span id="cb5-13"><a href="intro.html#cb5-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-14"><a href="intro.html#cb5-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Iterate through each class and compute its centrality measure.</span></span>
<span id="cb5-15"><a href="intro.html#cb5-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(c <span class="cf">in</span> classes){</span>
<span id="cb5-16"><a href="intro.html#cb5-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-17"><a href="intro.html#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Filter instances by class.</span></span>
<span id="cb5-18"><a href="intro.html#cb5-18" aria-hidden="true" tabindex="-1"></a>    tmp <span class="ot">&lt;-</span> data[<span class="fu">which</span>(data<span class="sc">$</span>class <span class="sc">==</span> c),]</span>
<span id="cb5-19"><a href="intro.html#cb5-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-20"><a href="intro.html#cb5-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the centrality measure.</span></span>
<span id="cb5-21"><a href="intro.html#cb5-21" aria-hidden="true" tabindex="-1"></a>    centrality.measure <span class="ot">&lt;-</span> <span class="fu">centrality</span>(tmp<span class="sc">$</span>speed)</span>
<span id="cb5-22"><a href="intro.html#cb5-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-23"><a href="intro.html#cb5-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store the centrality measure for this class.</span></span>
<span id="cb5-24"><a href="intro.html#cb5-24" aria-hidden="true" tabindex="-1"></a>    params[c] <span class="ot">&lt;-</span> centrality.measure</span>
<span id="cb5-25"><a href="intro.html#cb5-25" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb5-26"><a href="intro.html#cb5-26" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-27"><a href="intro.html#cb5-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(params)</span>
<span id="cb5-28"><a href="intro.html#cb5-28" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>The first argument is the training data and the second argument is the centrality function we want to use (the mean, by default). This function iterates each class, computes the centrality measure based on the speed, and stores the results in a named array called <code>params</code> which is then returned at the end.</p>
<p>Most of the time, training a model involves feeding it with the training data and any additional <strong>hyperparameters</strong> specific to each model. In this case, the centrality measure is a hyperparameter and here, we set it to be the <em>mean</em>.</p>

<div class="rmdinfo">
The difference between <strong>parameters</strong> and <strong>hyperparameters</strong> is that the former are learned during training. The <strong>hyperparameters</strong> are settings specific to each model that can be defined before the actual training starts.
</div>
<p>Now that we have a function that performs the training, we need another one that performs the actual inference or prediction on new data points. Let’s call this one <code>simple.classifier.predict()</code>. Its first argument is a data frame with the instances we want to get predictions for. The second argument is the named vector of parameters learned during training. This function will return an array with the predicted class for each instance in <code>newdata</code>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="intro.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function that predicts a class</span></span>
<span id="cb6-2"><a href="intro.html#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># based on the learned parameters.</span></span>
<span id="cb6-3"><a href="intro.html#cb6-3" aria-hidden="true" tabindex="-1"></a>simple.classifier.predict <span class="ot">&lt;-</span> <span class="cf">function</span>(newdata, params){</span>
<span id="cb6-4"><a href="intro.html#cb6-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-5"><a href="intro.html#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Variable to store the predictions of</span></span>
<span id="cb6-6"><a href="intro.html#cb6-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># each instance in newdata.</span></span>
<span id="cb6-7"><a href="intro.html#cb6-7" aria-hidden="true" tabindex="-1"></a>  predictions <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb6-8"><a href="intro.html#cb6-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-9"><a href="intro.html#cb6-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Iterate instances in newdata</span></span>
<span id="cb6-10"><a href="intro.html#cb6-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(newdata)){</span>
<span id="cb6-11"><a href="intro.html#cb6-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-12"><a href="intro.html#cb6-12" aria-hidden="true" tabindex="-1"></a>    instance <span class="ot">&lt;-</span> newdata[i,]</span>
<span id="cb6-13"><a href="intro.html#cb6-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-14"><a href="intro.html#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predict the name of the class which</span></span>
<span id="cb6-15"><a href="intro.html#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># centrality measure is closest.</span></span>
<span id="cb6-16"><a href="intro.html#cb6-16" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">names</span>(<span class="fu">which.min</span>(<span class="fu">abs</span>(instance<span class="sc">$</span>speed <span class="sc">-</span> params)))</span>
<span id="cb6-17"><a href="intro.html#cb6-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-18"><a href="intro.html#cb6-18" aria-hidden="true" tabindex="-1"></a>    predictions <span class="ot">&lt;-</span> <span class="fu">c</span>(predictions, pred)</span>
<span id="cb6-19"><a href="intro.html#cb6-19" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb6-20"><a href="intro.html#cb6-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-21"><a href="intro.html#cb6-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(predictions)</span>
<span id="cb6-22"><a href="intro.html#cb6-22" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>This function iterates through each row and computes the distance to each centrality measure and returns the name of the class that was the closest one. The distance computation is done with the following line of code:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="intro.html#cb7-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">names</span>(<span class="fu">which.min</span>(<span class="fu">abs</span>(instance<span class="sc">$</span>speed <span class="sc">-</span> params)))</span></code></pre></div>
<p>First, it computes the absolute difference between the speed and each centrality measure stored in <code>params</code> and then, it returns the class name of the minimum one. Now that we have defined the training and prediction procedures, we are ready to test our classifier!</p>
<p>In section <a href="intro.html#trainingeval">1.6</a>, two evaluation methods were presented. <em>Hold-out</em> and <em>k-fold cross-validation</em>. These methods allow you to estimate how your model will perform on new data. Let’s start with <em>hold-out validation</em>.</p>
<p>First, we need to split the data into two independent sets. We will use <span class="math inline">\(70\%\)</span> of the data to train our classifier and the remaining <span class="math inline">\(30\%\)</span> to test it. The following code splits <code>dataset</code> into a <code>trainset</code> and <code>testset</code>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="intro.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Percent to be used as training data.</span></span>
<span id="cb8-2"><a href="intro.html#cb8-2" aria-hidden="true" tabindex="-1"></a>pctTrain <span class="ot">&lt;-</span> <span class="fl">0.7</span></span>
<span id="cb8-3"><a href="intro.html#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="intro.html#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed for reproducibility.</span></span>
<span id="cb8-5"><a href="intro.html#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb8-6"><a href="intro.html#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="intro.html#cb8-7" aria-hidden="true" tabindex="-1"></a>idxs <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(dataset),</span>
<span id="cb8-8"><a href="intro.html#cb8-8" aria-hidden="true" tabindex="-1"></a>               <span class="at">size =</span> <span class="fu">nrow</span>(dataset) <span class="sc">*</span> pctTrain,</span>
<span id="cb8-9"><a href="intro.html#cb8-9" aria-hidden="true" tabindex="-1"></a>               <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb8-10"><a href="intro.html#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="intro.html#cb8-11" aria-hidden="true" tabindex="-1"></a>trainset <span class="ot">&lt;-</span> dataset[idxs,]</span>
<span id="cb8-12"><a href="intro.html#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="intro.html#cb8-13" aria-hidden="true" tabindex="-1"></a>testset <span class="ot">&lt;-</span> dataset[<span class="sc">-</span>idxs,]</span></code></pre></div>
<p>The <code>sample()</code> function was used to select integer numbers at random from <span class="math inline">\(1\)</span> to <span class="math inline">\(n\)</span>, where <span class="math inline">\(n\)</span> is the total number of data points in <code>dataset</code>. These randomly selected data points are the ones that will go to the train set. The <code>size</code> argument tells the function to return <span class="math inline">\(70\)</span> numbers which correspond to <span class="math inline">\(70\%\)</span> of the total since <code>dataset</code> has <span class="math inline">\(100\)</span> instances.</p>

<div class="rmdcaution">
The last argument <code>replace</code> is set to <code>FALSE</code> because we do not want repeated numbers. This ensures that any instance only belongs to either the train or the test set. <strong>We don’t want an instance to be copied into both sets.</strong>
</div>
<p>Now it’s time to test our functions. We can train our model using the <code>trainset</code> by calling our previously defined function <code>simple.model.train()</code>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="intro.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model using the trainset.</span></span>
<span id="cb9-2"><a href="intro.html#cb9-2" aria-hidden="true" tabindex="-1"></a>params <span class="ot">&lt;-</span> <span class="fu">simple.model.train</span>(trainset, mean)</span>
<span id="cb9-3"><a href="intro.html#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="intro.html#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the learned parameters.</span></span>
<span id="cb9-5"><a href="intro.html#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(params)</span>
<span id="cb9-6"><a href="intro.html#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    tiger  leopard </span></span>
<span id="cb9-7"><a href="intro.html#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 48.88246 54.58369</span></span></code></pre></div>
<p>After training the model, we print the learned parameters. In this case, the mean for <em>tiger</em> is <span class="math inline">\(48.88\)</span> and for <em>leopard</em>, it is <span class="math inline">\(54.58\)</span>. With these parameters, we can start making predictions on our test set! We pass the test set and the newly-learned parameters to our function <code>simple.classifier.predict()</code>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="intro.html#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict classes on the test set.</span></span>
<span id="cb10-2"><a href="intro.html#cb10-2" aria-hidden="true" tabindex="-1"></a>test.predictions <span class="ot">&lt;-</span> <span class="fu">simple.classifier.predict</span>(testset, params)</span>
<span id="cb10-3"><a href="intro.html#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="intro.html#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Display first predictions.</span></span>
<span id="cb10-5"><a href="intro.html#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(test.predictions)</span>
<span id="cb10-6"><a href="intro.html#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;tiger&quot;   &quot;tiger&quot;   &quot;leopard&quot; &quot;tiger&quot;   &quot;tiger&quot;   &quot;leopard&quot;</span></span></code></pre></div>
<p>Our predict function returns predictions for each instance in the test set. We can use the <code>head()</code> function to print the first predictions. The first two instances were classified as tigers, the third one as leopard, and so on.</p>
<p>But how good are those predictions? Since we know what the true classes are (also known as <strong>ground truth</strong>) in our test set, we can compute the performance. In this case, we will compute the accuracy, which is the percentage of correct classifications. Note that we did not use the class information when making predictions, we only used the <em>speed</em>. We pretended that we didn’t have the true class. We will use the true class only to evaluate the model’s performance.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="intro.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute test accuracy.</span></span>
<span id="cb11-2"><a href="intro.html#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(test.predictions <span class="sc">==</span> <span class="fu">as.character</span>(testset<span class="sc">$</span>class)) <span class="sc">/</span></span>
<span id="cb11-3"><a href="intro.html#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nrow</span>(testset)</span>
<span id="cb11-4"><a href="intro.html#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.8333333</span></span></code></pre></div>
<p>We can compute the accuracy by counting how many predictions were equal to the true classes and divide them by the total number of points in the test set. In this case, the test accuracy was <span class="math inline">\(83.0\%\)</span>. <strong>Congratulations! you have trained and evaluated your first classifier.</strong></p>
<p>It is also a good idea to compute the performance on the same train set that was used to train the model.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="intro.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute train accuracy.</span></span>
<span id="cb12-2"><a href="intro.html#cb12-2" aria-hidden="true" tabindex="-1"></a>train.predictions <span class="ot">&lt;-</span> <span class="fu">simple.classifier.predict</span>(trainset, params)</span>
<span id="cb12-3"><a href="intro.html#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(train.predictions <span class="sc">==</span> <span class="fu">as.character</span>(trainset<span class="sc">$</span>class)) <span class="sc">/</span></span>
<span id="cb12-4"><a href="intro.html#cb12-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nrow</span>(trainset)</span>
<span id="cb12-5"><a href="intro.html#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.8571429</span></span></code></pre></div>
<p>The <em>train accuracy</em> was <span class="math inline">\(85.7\%\)</span>. As expected, this was higher than the <em>test accuracy</em>. Typically, what you report is the performance on the <em>test set</em>, but we can use the performance on the <em>train set</em> to look for signs of over/under-fitting which will be covered in the following sections.</p>
<div id="k-fold-cross-validation-example" class="section level3" number="1.7.1">
<h3><span class="header-section-number">1.7.1</span> <span class="math inline">\(k\)</span>-fold Cross-validation Example</h3>
<p>Now, let’s see how <span class="math inline">\(k\)</span>-fold cross-validation can be implemented to test our classifier. I will choose a <span class="math inline">\(k=5\)</span>. This means that <span class="math inline">\(5\)</span> independent sets are going to be generated and <span class="math inline">\(5\)</span> iterations will be run.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="intro.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of folds.</span></span>
<span id="cb13-2"><a href="intro.html#cb13-2" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb13-3"><a href="intro.html#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="intro.html#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb13-5"><a href="intro.html#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="intro.html#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random folds.</span></span>
<span id="cb13-7"><a href="intro.html#cb13-7" aria-hidden="true" tabindex="-1"></a>folds <span class="ot">&lt;-</span> <span class="fu">sample</span>(k, <span class="at">size =</span> <span class="fu">nrow</span>(dataset), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb13-8"><a href="intro.html#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="intro.html#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Print how many instances ended up in each fold.</span></span>
<span id="cb13-10"><a href="intro.html#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(folds)</span>
<span id="cb13-11"><a href="intro.html#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; folds</span></span>
<span id="cb13-12"><a href="intro.html#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  1  2  3  4  5 </span></span>
<span id="cb13-13"><a href="intro.html#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 21 20 23 17 19 </span></span></code></pre></div>
<p>Again, we can use the <code>sample()</code> function. This time we want to select random integers between <span class="math inline">\(1\)</span> and <span class="math inline">\(k\)</span>. The total number of integers will be equal to the total number of instances <span class="math inline">\(n\)</span> in the entire dataset. Note that this time we set <code>replace = TRUE</code> since <span class="math inline">\(k &lt; n\)</span>, so this implies that we need to pick repeated numbers. Each number will represent the fold to which each instance belongs to. As before, we need to make sure that each instance belongs only to one of the sets. Here, we are guaranteeing that by assigning each instance a single fold number. We can use the <code>table()</code> function to print how many instances ended up in each fold. Here, we see that the folds will contain between <span class="math inline">\(17\)</span> and <span class="math inline">\(23\)</span> instances.</p>
<p><span class="math inline">\(k\)</span>-fold cross-validation consists of iterating <span class="math inline">\(k\)</span> times. In each iteration, one of the folds is selected as the test set and the remaining folds are used to build the train set. Within each iteration, the model is trained with the train set and evaluated with the test set. At the end, the average accuracy across folds is reported.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="intro.html#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Variables to store accuracies on each fold.</span></span>
<span id="cb14-2"><a href="intro.html#cb14-2" aria-hidden="true" tabindex="-1"></a>test.accuracies <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb14-3"><a href="intro.html#cb14-3" aria-hidden="true" tabindex="-1"></a>train.accuracies <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb14-4"><a href="intro.html#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="intro.html#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k){</span>
<span id="cb14-6"><a href="intro.html#cb14-6" aria-hidden="true" tabindex="-1"></a>  testset <span class="ot">&lt;-</span> dataset[<span class="fu">which</span>(folds <span class="sc">==</span> i),]</span>
<span id="cb14-7"><a href="intro.html#cb14-7" aria-hidden="true" tabindex="-1"></a>  trainset <span class="ot">&lt;-</span> dataset[<span class="fu">which</span>(folds <span class="sc">!=</span> i),]</span>
<span id="cb14-8"><a href="intro.html#cb14-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-9"><a href="intro.html#cb14-9" aria-hidden="true" tabindex="-1"></a>  params <span class="ot">&lt;-</span> <span class="fu">simple.model.train</span>(trainset, mean)</span>
<span id="cb14-10"><a href="intro.html#cb14-10" aria-hidden="true" tabindex="-1"></a>  test.predictions <span class="ot">&lt;-</span> <span class="fu">simple.classifier.predict</span>(testset, params)</span>
<span id="cb14-11"><a href="intro.html#cb14-11" aria-hidden="true" tabindex="-1"></a>  train.predictions <span class="ot">&lt;-</span> <span class="fu">simple.classifier.predict</span>(trainset, params)</span>
<span id="cb14-12"><a href="intro.html#cb14-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-13"><a href="intro.html#cb14-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Accuracy on test set.</span></span>
<span id="cb14-14"><a href="intro.html#cb14-14" aria-hidden="true" tabindex="-1"></a>  acc <span class="ot">&lt;-</span> <span class="fu">sum</span>(test.predictions <span class="sc">==</span> </span>
<span id="cb14-15"><a href="intro.html#cb14-15" aria-hidden="true" tabindex="-1"></a>               <span class="fu">as.character</span>(testset<span class="sc">$</span>class)) <span class="sc">/</span></span>
<span id="cb14-16"><a href="intro.html#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nrow</span>(testset)</span>
<span id="cb14-17"><a href="intro.html#cb14-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-18"><a href="intro.html#cb14-18" aria-hidden="true" tabindex="-1"></a>  test.accuracies <span class="ot">&lt;-</span> <span class="fu">c</span>(test.accuracies, acc)</span>
<span id="cb14-19"><a href="intro.html#cb14-19" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-20"><a href="intro.html#cb14-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Accuracy on train set.</span></span>
<span id="cb14-21"><a href="intro.html#cb14-21" aria-hidden="true" tabindex="-1"></a>  acc <span class="ot">&lt;-</span> <span class="fu">sum</span>(train.predictions <span class="sc">==</span> </span>
<span id="cb14-22"><a href="intro.html#cb14-22" aria-hidden="true" tabindex="-1"></a>               <span class="fu">as.character</span>(trainset<span class="sc">$</span>class)) <span class="sc">/</span></span>
<span id="cb14-23"><a href="intro.html#cb14-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">nrow</span>(trainset)</span>
<span id="cb14-24"><a href="intro.html#cb14-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-25"><a href="intro.html#cb14-25" aria-hidden="true" tabindex="-1"></a>  train.accuracies <span class="ot">&lt;-</span> <span class="fu">c</span>(train.accuracies, acc)</span>
<span id="cb14-26"><a href="intro.html#cb14-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-27"><a href="intro.html#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="intro.html#cb14-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Print mean accuracy across folds on the test set.</span></span>
<span id="cb14-29"><a href="intro.html#cb14-29" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(test.accuracies)</span>
<span id="cb14-30"><a href="intro.html#cb14-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.829823</span></span>
<span id="cb14-31"><a href="intro.html#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="intro.html#cb14-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Print mean accuracy across folds on the train set.</span></span>
<span id="cb14-33"><a href="intro.html#cb14-33" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(train.accuracies)</span>
<span id="cb14-34"><a href="intro.html#cb14-34" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.8422414</span></span></code></pre></div>
<p>The test mean accuracy across the <span class="math inline">\(5\)</span> folds was <span class="math inline">\(\approx 83\%\)</span> which is very similar to the accuracy estimated by hold-out validation.</p>

<div class="rmdgoodpractice">
<p>Note that in section <a href="intro.html#trainingeval">1.6</a> a <strong>validation set</strong> was also mentioned. This one is useful when you want to fine-tune a model and/or try different preprocessing methods on your data. In case you are using hold-out validation, you may want to split your data into three sets: train/validation/test sets. So, you train your model using the train set and estimate its performance using the validation set. Then you can fine-tune your model. For example, here, instead of the mean as centrality measure, you can try to use the median and measure the performance again with the validation set. When you are pleased with your settings, you estimate the final performance of the model with the test set <em>only once</em>.</p>
In the case of <span class="math inline">\(k\)</span>-fold cross-validation, you can set aside a test set at the beginning. Then you use the remaining data to perform cross-validation and fine-tune your model. Within each iteration, you test the performance with the validation data. Once you are sure you are not going to do any parameter tuning, you can train a model with the train and validation sets and test the generalization performance using the test set.
</div>

<div class="rmdinfo">
<p>One of the benefits of machine learning is that it allows us to find patterns based on data freeing us from having to program hard-coded rules. This means more scalable and flexible code. If for some reason, now, instead of <span class="math inline">\(2\)</span> classes we needed to add another class, for example, a <em>‘jaguar’</em>, the only thing we need to do is update our database and retrain our model. We don’t need to modify the internals of the algorithms. They will update themselves based on the data.</p>
We can try this by adding a third class <em>‘jaguar’</em> to the dataset as shown in the script <code>simple_model.R</code>. It then trains the model as usual and performs predictions.
</div>
</div>
</div>
<div id="simple-regression-example" class="section level2" number="1.8">
<h2><span class="header-section-number">1.8</span> Simple Regression Example</h2>

<div class="rmdfolder">
simple_model.R
</div>
<p>As opposed to classification models where the aim is to predict a category, <strong>regression models predict numeric values</strong>. To exemplify this, we can use our felines dataset but instead try to predict <em>speed</em> based on the type of feline. The <em>class</em> column will be treated as a <strong>feature</strong> variable and <em>speed</em> will be the <strong>response variable</strong>. Since there is only one predictor, and it is categorical, the best thing we can do to implement our regression model is to predict the mean speed depending on the class.</p>
<p>Recall that for the classification scenario, our learned parameters consisted of the means for each class. Thus, we can reuse our training function <code>simple.model.train()</code>. All we need to do is to define a new predict function that returns the speed based on the class. This is the opposite of what we did in the classification case (return the class based on the speed).</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="intro.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function that predicts speed</span></span>
<span id="cb15-2"><a href="intro.html#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co"># based on the type of feline.</span></span>
<span id="cb15-3"><a href="intro.html#cb15-3" aria-hidden="true" tabindex="-1"></a>simple.regression.predict <span class="ot">&lt;-</span> <span class="cf">function</span>(newdata, params){</span>
<span id="cb15-4"><a href="intro.html#cb15-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-5"><a href="intro.html#cb15-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Variable to store the predictions of</span></span>
<span id="cb15-6"><a href="intro.html#cb15-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># each instance in newdata.</span></span>
<span id="cb15-7"><a href="intro.html#cb15-7" aria-hidden="true" tabindex="-1"></a>  predictions <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb15-8"><a href="intro.html#cb15-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-9"><a href="intro.html#cb15-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Iterate instances in newdata</span></span>
<span id="cb15-10"><a href="intro.html#cb15-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(newdata)){</span>
<span id="cb15-11"><a href="intro.html#cb15-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-12"><a href="intro.html#cb15-12" aria-hidden="true" tabindex="-1"></a>    instance <span class="ot">&lt;-</span> newdata[i,]</span>
<span id="cb15-13"><a href="intro.html#cb15-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-14"><a href="intro.html#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return the mean value of the corresponding class stored in params.</span></span>
<span id="cb15-15"><a href="intro.html#cb15-15" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> params[<span class="fu">which</span>(<span class="fu">names</span>(params) <span class="sc">==</span> instance<span class="sc">$</span>class)]</span>
<span id="cb15-16"><a href="intro.html#cb15-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-17"><a href="intro.html#cb15-17" aria-hidden="true" tabindex="-1"></a>    predictions <span class="ot">&lt;-</span> <span class="fu">c</span>(predictions, pred)</span>
<span id="cb15-18"><a href="intro.html#cb15-18" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb15-19"><a href="intro.html#cb15-19" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-20"><a href="intro.html#cb15-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(predictions)</span>
<span id="cb15-21"><a href="intro.html#cb15-21" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>The <code>simple.regression.predict()</code> function iterates through each instance in <code>newdata</code> and returns the mean speed from <code>params</code> for the corresponding class.</p>
<p>Again, we can validate our model using <em>hold-out validation</em>. The train set will contain <span class="math inline">\(70\%\)</span> of the instances and the remaining will be used as the test set.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="intro.html#cb16-1" aria-hidden="true" tabindex="-1"></a>pctTrain <span class="ot">&lt;-</span> <span class="fl">0.7</span></span>
<span id="cb16-2"><a href="intro.html#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb16-3"><a href="intro.html#cb16-3" aria-hidden="true" tabindex="-1"></a>idxs <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(dataset),</span>
<span id="cb16-4"><a href="intro.html#cb16-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">size =</span> <span class="fu">nrow</span>(dataset) <span class="sc">*</span> pctTrain,</span>
<span id="cb16-5"><a href="intro.html#cb16-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb16-6"><a href="intro.html#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="intro.html#cb16-7" aria-hidden="true" tabindex="-1"></a>trainset <span class="ot">&lt;-</span> dataset[idxs,]</span>
<span id="cb16-8"><a href="intro.html#cb16-8" aria-hidden="true" tabindex="-1"></a>testset <span class="ot">&lt;-</span> dataset[<span class="sc">-</span>idxs,]</span>
<span id="cb16-9"><a href="intro.html#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="intro.html#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Reuse our train function.</span></span>
<span id="cb16-11"><a href="intro.html#cb16-11" aria-hidden="true" tabindex="-1"></a>params <span class="ot">&lt;-</span> <span class="fu">simple.model.train</span>(trainset, mean)</span>
<span id="cb16-12"><a href="intro.html#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="intro.html#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(params)</span>
<span id="cb16-14"><a href="intro.html#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    tiger  leopard </span></span>
<span id="cb16-15"><a href="intro.html#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 48.88246 54.5836</span></span></code></pre></div>
<p>Here, we reused our previous function <code>simple.model.train()</code> to learn the parameters and then print them. Then we can use those parameters to infer the speed. If a test instance belongs to the class <em>‘tiger’</em> then return <span class="math inline">\(48.88\)</span>. If it is of class <em>‘leopard’</em> then return <span class="math inline">\(54.58\)</span>.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="intro.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict speeds on the test set.</span></span>
<span id="cb17-2"><a href="intro.html#cb17-2" aria-hidden="true" tabindex="-1"></a>test.predictions <span class="ot">&lt;-</span> </span>
<span id="cb17-3"><a href="intro.html#cb17-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">simple.regression.predict</span>(testset, params)</span>
<span id="cb17-4"><a href="intro.html#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="intro.html#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print first predictions.</span></span>
<span id="cb17-6"><a href="intro.html#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(test.predictions)</span>
<span id="cb17-7"><a href="intro.html#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 48.88246 54.58369 54.58369 48.88246 48.88246 54.58369 </span></span></code></pre></div>
<p>Since these are numeric predictions, we cannot use accuracy as in the classification case to evaluate the performance. One way to evaluate the performance of regression models is by computing the <strong>mean absolute error (MAE)</strong>. This measure tells you, on average, how much each prediction deviates from its true value. It is computed by subtracting each prediction from its real value and taking the absolute value: <span class="math inline">\(|predicted - realValue|\)</span>. This can be visualized in Figure <a href="intro.html#fig:maeExample">1.12</a>. The distances between the true and predicted values are the errors and the MAE is the average of all those errors.</p>
<div class="figure" style="text-align: center"><span id="fig:maeExample"></span>
<img src="images/mae.png" alt="Prediction errors." width="40%" />
<p class="caption">
FIGURE 1.12: Prediction errors.
</p>
</div>
<p>We can use the following code to compute the MAE:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="intro.html#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute mean absolute error (MAE) on the test set.</span></span>
<span id="cb18-2"><a href="intro.html#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(test.predictions <span class="sc">-</span> testset<span class="sc">$</span>speed))</span>
<span id="cb18-3"><a href="intro.html#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 2.562598</span></span></code></pre></div>
<p>The MAE on the <em>test set</em> was <span class="math inline">\(2.56\)</span>. That is, on average, our simple model had a deviation of <span class="math inline">\(2.56\)</span> km/hr with respect to the true values, which is not bad. We can also compute the MAE on the <em>train set</em>.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="intro.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict speeds on the train set.</span></span>
<span id="cb19-2"><a href="intro.html#cb19-2" aria-hidden="true" tabindex="-1"></a>train.predictions <span class="ot">&lt;-</span> </span>
<span id="cb19-3"><a href="intro.html#cb19-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">simple.regression.predict</span>(trainset, params)</span>
<span id="cb19-4"><a href="intro.html#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="intro.html#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute mean absolute error (MAE) on the train set.</span></span>
<span id="cb19-6"><a href="intro.html#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(train.predictions <span class="sc">-</span> trainset<span class="sc">$</span>speed))</span>
<span id="cb19-7"><a href="intro.html#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 2.16097</span></span></code></pre></div>
<p>The MAE on the <em>train set</em> was <span class="math inline">\(2.16\)</span>, which is better than the <em>test set</em> MAE (small MAE values are preferred). <strong>Now, you have built, trained, and evaluated a regression model!</strong></p>
<p>This was a simple example, but it illustrates the basic idea of regression and how it differs from classification. It also shows how the performance of regression models is typically evaluated with the MAE as opposed to the accuracy used in classification. In chapter <a href="deeplearning.html#deeplearning">8</a>, more advanced methods such as neural networks will be introduced, which can be used to solve regression problems.</p>
<p>In this section, we have gone through several of the data analysis pipeline phases. We did a simple exploratory analysis of the data and then we built, trained, and validated the models to perform both classification and regression. Finally, we estimated the overall performance of the models and presented the results. Here, we coded our models from scratch, but in practice, you typically use models that have already been implemented and tested. All in all, I hope these examples have given you the feeling of how it is to work with machine learning.</p>
</div>
<div id="underfitting-and-overfitting" class="section level2" number="1.9">
<h2><span class="header-section-number">1.9</span> Underfitting and Overfitting</h2>
<p>From the felines classification example, we saw how we can separate two classes by computing the mean for each class. For the two-class problem, this is equivalent to having a decision line between the two means (Figure <a href="intro.html#fig:boundary">1.13</a>). Everything to the right of this decision line will be closer to the mean that corresponds to <em>‘leopard’</em> and everything to the left to <em>‘tiger’</em>. In this case, the classification function is a vertical line. During learning, the position of the line that reduces the classification error is searched for. We implicitly estimated the position of the line by finding the <em>mean values</em> for each of the classes.</p>
<div class="figure" style="text-align: center"><span id="fig:boundary"></span>
<img src="images/boundary.png" alt="Decision line between the two classes." width="100%" />
<p class="caption">
FIGURE 1.13: Decision line between the two classes.
</p>
</div>
<p>Now, imagine that we do not only have access to the <em>speed</em> but also to the felines’ <em>age</em>. This extra information could help us reduce the prediction error since age plays an important role in how fast a feline is. Figure <a href="intro.html#fig:underOverFitting">1.14</a> (left) shows how it will look like if we plot <em>age</em> in the x-axis and <em>speed</em> in the y-axis. Here, we can see that for both, tigers and leopards, the <em>speed</em> seems to increase as <em>age</em> increases. Then, at some point, as <em>age</em> increases the <em>speed</em> begins to decrease.</p>
<p>Constructing a classifier with a single vertical line as we did before will not work in this <span class="math inline">\(2\)</span>-dimensional case where we have <span class="math inline">\(2\)</span> predictors. Now we will need a more complex decision boundary (function) to separate the two classes. One approach would be to use a line as before but this time we allow the line to have a slope (angle). Everything below the line is classified as <em>‘tiger’</em> and everything else as <em>‘leopard’</em>. Thus, the learning phase involves finding the line’s <em>position</em> and its <em>slope</em> that achieves the smallest error.</p>
<p>Figure <a href="intro.html#fig:underOverFitting">1.14</a> (left) shows a possible decision line. Even though this function is more complex than a vertical line, it will still produce a lot of misclassifications (it does not clearly separate both classes). This is called <strong>underfitting</strong>, that is, the model is so simple that it is not able to capture the underlying data patterns.</p>
<div class="figure" style="text-align: center"><span id="fig:underOverFitting"></span>
<img src="images/under_over_fit.png" alt="Underfitting and overfitting." width="100%" />
<p class="caption">
FIGURE 1.14: Underfitting and overfitting.
</p>
</div>
<p>Let’s try a more complex function, for example, a curve. Figure <a href="intro.html#fig:underOverFitting">1.14</a> (middle) shows that a curve does a better job at separating the two classes with fewer misclassifications but still, <span class="math inline">\(3\)</span> leopards are misclassified as tigers and <span class="math inline">\(1\)</span> tiger is misclassified as leopard. Can we do better than that? Yes, just keep increasing the complexity of the decision function.</p>
<p>Figure <a href="intro.html#fig:underOverFitting">1.14</a> (right) shows a more complex function that was able to separate the two classes with <span class="math inline">\(100\%\)</span> accuracy or equivalently, with a <span class="math inline">\(0\%\)</span> error. However, there is a problem. This function learned how to accurately separate the <em>training data</em>, but it is likely that it will not do as well with a new <em>test set</em>. This function became so specialized with respect to this particular data that it failed to capture the overall pattern. This is called <strong>overfitting</strong>. In this case, the model ‘memorizes’ the train set instead of finding general patterns applicable to new unseen instances. If we were to choose a model, the best one would be the one in the middle. Even if it is not perfect on the train data, it will do better than the other models when evaluated on new test data.</p>
<p>Overfitting is a common problem in machine learning. One way to know if a model is overfitting is by checking if the error in the train set is low while it is high on a new set (can be a test or validation set). Figure <a href="intro.html#fig:modelComplexity">1.15</a> illustrates this idea. Too-simple models will produce a high error for both, the train and validation sets (underfitting). As the complexity of the model increases, the errors on both sets are reduced. Then, at some point, the complexity of a model becomes so high that it gets too specific on the train set and fails to perform well on a new independent set (overfitting).</p>
<div class="figure" style="text-align: center"><span id="fig:modelComplexity"></span>
<img src="images/model_complexity.png" alt="Model complexity vs. train and validation error." width="60%" />
<p class="caption">
FIGURE 1.15: Model complexity vs. train and validation error.
</p>
</div>
<p>In this example, we saw how <em>underfitting</em> and <em>overfitting</em> can affect the generalization performance of a model in a classification setting but the same can occur in regression problems.</p>
<p>There are several methods that aim to reduce overfitting, but many of them are specific to the type of model. For example, with decision trees (covered in chapter <a href="classification.html#classification">2</a>), one way to reduce overfitting is to limit their depth or build ensembles of trees (chapter <a href="ensemble.html#ensemble">3</a>). Neural networks are also highly prone to overfitting since they can be very complex and have millions of parameters. In chapter <a href="deeplearning.html#deeplearning">8</a>, several techniques to reduce the effect of overfitting will be presented.</p>
</div>
<div id="bias-and-variance" class="section level2" number="1.10">
<h2><span class="header-section-number">1.10</span> Bias and Variance</h2>
<p>So far, we have seen how to train predictive models and evaluate how well they do on new data (test/validation sets). The main goal is to have predictive models that have a low error rate when used with new data. Understanding the source of the error can help us make more informed decisions when building predictive models. The <em>test error</em>, also known as the <em>generalization error</em> of a predictive model can be decomposed into three components: bias, variance, and noise.</p>
<p><strong>Noise.</strong> This component is inherent to the data itself and there is nothing we can do about it. For example, two instances having the same values in their features but with a different label.</p>
<p><strong>Bias.</strong> How much the average prediction differs from the true value. Note the <em>average</em> keyword. This means that we make the assumption that an infinite (or very large) number of train sets can be generated and for each, a predictive model is trained. Then we average the predictions of all those models and see how much that average differs from the true value.</p>
<p><strong>Variance.</strong> How much the predictions change for a given data point when training a model using a different train set each time.</p>
<p>Bias and variance are closely related to underfitting and overfitting. High variance is a sign of overfitting. That is, a model is so complex that it will fit a particular train set very well. Every time it is trained with a different train set, the <em>train error</em> will be low, but it will likely generate very different predictions for the same test points and a much higher <em>test error</em>.</p>
<p>Figure <a href="intro.html#fig:overfittingVariance">1.16</a> illustrates the relation between overfitting and high variance with a regression problem.</p>
<div class="figure" style="text-align: center"><span id="fig:overfittingVariance"></span>
<img src="images/overfitting_variance.png" alt="High variance and overfitting." width="100%" />
<p class="caption">
FIGURE 1.16: High variance and overfitting.
</p>
</div>
<p>Given a feature <span class="math inline">\(x\)</span>, two models are trained to predict <span class="math inline">\(y\)</span>: i) a <em>complex model</em> (top row), and ii) a <em>simpler model</em> (bottom row). Both models are fitted with two training sets (<span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>) sampled from the same distribution. The complex model fits the train data perfectly but makes very different predictions (big <span class="math inline">\(\Delta\)</span>) for the same test point when using a different train set. The simpler model does not fit the train data so well but has a smaller <span class="math inline">\(\Delta\)</span> and a lower error on the test point as well. Visually, the function (red curve) of the complex model also varies a lot across train sets whereas the shapes of the simpler model functions look very similar.</p>
<p>On the other hand, if a model is too simple, it will underfit causing <em>highly biased</em> results without being able to capture the input-output relationships. This results in a high <em>train error</em> and in consequence, a high <em>test error</em> as well.</p>

<div class="rmdinfo">
A formal definition of the error decomposition is explained in the book “The elements of statistical learning: data mining, inference, and prediction” <span class="citation">(<a href="#ref-hastie2009elements" role="doc-biblioref">Hastie, Tibshirani, and Friedman 2009</a>)</span>.
</div>
</div>
<div id="SummaryIntro" class="section level2" number="1.11">
<h2><span class="header-section-number">1.11</span> Summary</h2>
<p>In this chapter, several introductory machine learning concepts and terms were introduced and they are the basis for the methods that will be covered in the following chapters.</p>
<ul>
<li><strong>Behavior</strong> can be defined as <em>“an observable activity in a human or animal”</em>.</li>
<li>Three main reasons of why we may want to analyze behavior automatically were discussed: <strong>react</strong>, <strong>understand</strong>, and <strong>document/archive</strong>.</li>
<li>One way to observe behavior automatically is through the use of sensors and/or data.</li>
<li><strong>Machine Learning</strong> consists of a set of computational algorithms that automatically find useful patterns and relationships from data.</li>
<li>The three main building blocks of machine learning are: <strong>data</strong>, <strong>algorithms</strong>, and <strong>models</strong>.</li>
<li>The main types of machine learning are <strong>supervised learning</strong>, <strong>semi-supervised learning</strong>, <strong>partially-supervised learning</strong>, and <strong>unsupervised learning</strong>.</li>
<li>In R, data is usually stored in data frames. Data frames have variables (columns) and instances (rows). Depending on the task, variables can be <strong>independent</strong> or <strong>dependent</strong>.</li>
<li>A <strong>predictive model</strong> is a model that takes some input and produces an output. <em>Classifiers</em> and <em>regressors</em> are predictive models.</li>
<li>A data analysis pipeline consists of several tasks including data collection, cleaning, preprocessing, training/evaluation, and presentation of results.</li>
<li>Model evaluation can be performed with <strong>hold-out validation</strong> or <strong><span class="math inline">\(k\)</span>-fold cross-validation</strong>.</li>
<li><strong>Overfitting</strong> occurs when a model ‘memorizes’ the training data instead of finding useful underlying patterns.</li>
<li>The test error can be decomposed into <strong>noise</strong>, <strong>bias</strong>, and <strong>variance</strong>.</li>
</ul>
<p><img src="images/comic_intro.png" width="100%" style="display: block; margin: auto;" />
</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-biecek2012" class="csl-entry">
Biecek, Przemyslaw, Ewa Szczurek, Martin Vingron, Jerzy Tiuryn, and others. 2012. <span>“The r Package Bgmm: Mixture Modeling with Uncertain Knowledge.”</span> <em>Journal of Statistical Software</em> 47 (i03).
</div>
<div id="ref-brena2017" class="csl-entry">
Brena, Ramon F, Juan Pablo Garcı́a-Vázquez, Carlos E Galván-Tejada, David Muñoz-Rodriguez, Cesar Vargas-Rosales, and James Fangmeyer. 2017. <span>“Evolution of Indoor Positioning Technologies: A Survey.”</span> <em>Journal of Sensors</em> 2017: 1–22.
</div>
<div id="ref-comelearning" class="csl-entry">
Côme, Etienne, Latifa Oukhellou, Thierry Denoeux, and Patrice Aknin. 2009. <span>“Learning from Partially Supervised Data Using Mixture Models and Belief Functions.”</span> <em>Pattern Recognition</em> 42 (3): 334–48.
</div>
<div id="ref-Fisher1936" class="csl-entry">
Fisher, Ronald A. 1936. <span>“The Use of Multiple Measurements in Taxonomic Problems.”</span> <em>Annals of Eugenics</em> 7 (2): 179–88.
</div>
<div id="ref-garciaSurvey2018" class="csl-entry">
Garcia-Ceja, Enrique, Michael Riegler, Tine Nordgreen, Petter Jakobsen, Ketil J Oedegaard, and Jim Tørresen. 2018. <span>“Mental Health Monitoring with Multimodal Sensing and Machine Learning: A Survey.”</span> <em>Pervasive and Mobile Computing</em>.
</div>
<div id="ref-hastie2009elements" class="csl-entry">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. New York, New York: Springer Science &amp; Business Media.
</div>
<div id="ref-kononenko2007" class="csl-entry">
Kononenko, Igor, and Matjaz Kukar. 2007. <em>Machine Learning and Data Mining</em>. Horwood Publishing.
</div>
<div id="ref-peng2016" class="csl-entry">
Peng, Roger. 2016. <em>Exploratory Data Analysis with r</em>. Leanpub.com.
</div>
<div id="ref-shoaib2015" class="csl-entry">
Shoaib, Muhammad, Stephan Bosch, Ozlem Durmaz Incel, Hans Scholten, and Paul J. M. Havinga. 2015. <span>“A <span>Survey</span> of <span>Online</span> <span>Activity</span> <span>Recognition</span> <span>Using</span> <span>Mobile</span> <span>Phones</span>.”</span> <em>Sensors</em> 15 (1): 2059–85. <a href="https://doi.org/10.3390/s150102059">https://doi.org/10.3390/s150102059</a>.
</div>
<div id="ref-trigueroselflabeled" class="csl-entry">
Triguero, Isaac, Salvador García, and Francisco Herrera. 2013. <span>“Self-Labeled Techniques for Semi-Supervised Learning: Taxonomy, Software and Empirical Study.”</span> <em>Knowledge and Information Systems</em> 42 (2): 245–84. <a href="https://doi.org/10.1007/s10115-013-0706-y">https://doi.org/10.1007/s10115-013-0706-y</a>.
</div>
<div id="ref-williams2020" class="csl-entry">
Williams, HJ, ELC Shepard, Mark D Holton, PAE Alarcón, RP Wilson, and SA Lambertucci. 2020. <span>“Physical Limits of Flight Performance in the Heaviest Soaring Bird.”</span> <em>Proceedings of the National Academy of Sciences</em>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p><a href="http://www.scholarpedia.org/article/Reinforcement_learning" class="uri">http://www.scholarpedia.org/article/Reinforcement_learning</a><a href="intro.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>mtcars dataset <a href="https://stat.ethz.ch/R-manual/R-patched/library/datasets/html/mtcars.html" class="uri">https://stat.ethz.ch/R-manual/R-patched/library/datasets/html/mtcars.html</a> extracted from the 1974 Motor Trend US magazine.<a href="intro.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="preface.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
