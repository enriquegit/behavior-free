<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>B Datasets | Behavior Analysis with Machine Learning Using R</title>
  <meta name="description" content="B Datasets | Behavior Analysis with Machine Learning Using R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="B Datasets | Behavior Analysis with Machine Learning Using R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="/images/cover.png" />
  <meta property="og:description" content="B Datasets | Behavior Analysis with Machine Learning Using R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="B Datasets | Behavior Analysis with Machine Learning Using R" />
  
  <meta name="twitter:description" content="B Datasets | Behavior Analysis with Machine Learning Using R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  <meta name="twitter:image" content="/images/cover.png" />

<meta name="author" content="Enrique Garcia Ceja" />


<meta name="date" content="2023-08-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="appendixInstall.html"/>
<link rel="next" href="citing-this-book.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/d3/d3.min.js"></script>
<link href="libs/d3panels/d3panels.min.css" rel="stylesheet" />
<script src="libs/d3panels/d3panels.min.js"></script>
<script src="libs/qtlcharts_iplotCorr/iplotCorr.js"></script>
<script src="libs/qtlcharts_iplotCorr/iplotCorr_noscat.js"></script>
<script src="libs/iplotCorr-binding/iplotCorr.js"></script>
<link href="libs/dygraphs/dygraph.css" rel="stylesheet" />
<script src="libs/dygraphs/dygraph-combined.js"></script>
<script src="libs/dygraphs/shapes.js"></script>
<script src="libs/moment/moment.js"></script>
<script src="libs/moment-timezone/moment-timezone-with-data.js"></script>
<script src="libs/moment-fquarter/moment-fquarter.min.js"></script>
<script src="libs/dygraphs-binding/dygraphs.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178679335-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178679335-1', { 'anonymize_ip': true });
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="#">Behavior Analysis with Machine Learning Using R</a></li>

<li class="divider"></li>
<li><a href="index.html#welcome">Welcome<span></span></a>
<ul>
<li><a href="index.html#about-the-front-cover">About the Front Cover<span></span></a></li>
<li><a href="index.html#about-the-author">About the Author<span></span></a></li>
</ul></li>
<li><a href="preface.html#preface">Preface<span></span></a>
<ul>
<li><a href="preface.html#supplemental-material">Supplemental Material<span></span></a></li>
<li><a href="preface.html#conventions">Conventions<span></span></a></li>
<li><a href="preface.html#acknowledgments">Acknowledgments<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to Behavior and Machine Learning<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#what-is-behavior"><i class="fa fa-check"></i><b>1.1</b> What Is Behavior?<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.2</b> What Is Machine Learning?<span></span></a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#taxonomy"><i class="fa fa-check"></i><b>1.3</b> Types of Machine Learning<span></span></a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#terminology"><i class="fa fa-check"></i><b>1.4</b> Terminology<span></span></a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#tables"><i class="fa fa-check"></i><b>1.4.1</b> Tables<span></span></a></li>
<li class="chapter" data-level="1.4.2" data-path="intro.html"><a href="intro.html#variable-types"><i class="fa fa-check"></i><b>1.4.2</b> Variable Types<span></span></a></li>
<li class="chapter" data-level="1.4.3" data-path="intro.html"><a href="intro.html#predictive-models"><i class="fa fa-check"></i><b>1.4.3</b> Predictive Models<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#pipeline"><i class="fa fa-check"></i><b>1.5</b> Data Analysis Pipeline<span></span></a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#trainingeval"><i class="fa fa-check"></i><b>1.6</b> Evaluating Predictive Models<span></span></a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#simple-classification-example"><i class="fa fa-check"></i><b>1.7</b> Simple Classification Example<span></span></a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="intro.html"><a href="intro.html#k-fold-cross-validation-example"><i class="fa fa-check"></i><b>1.7.1</b> <span class="math inline">\(k\)</span>-fold Cross-validation Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#simple-regression-example"><i class="fa fa-check"></i><b>1.8</b> Simple Regression Example<span></span></a></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a href="intro.html#underfitting-and-overfitting"><i class="fa fa-check"></i><b>1.9</b> Underfitting and Overfitting<span></span></a></li>
<li class="chapter" data-level="1.10" data-path="intro.html"><a href="intro.html#bias-and-variance"><i class="fa fa-check"></i><b>1.10</b> Bias and Variance<span></span></a></li>
<li class="chapter" data-level="1.11" data-path="intro.html"><a href="intro.html#SummaryIntro"><i class="fa fa-check"></i><b>1.11</b> Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>2</b> Predicting Behavior with Classification Models<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="classification.html"><a href="classification.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>2.1</b> <em>k</em>-Nearest Neighbors<span></span></a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="classification.html"><a href="classification.html#indoor-location-with-wi-fi-signals"><i class="fa fa-check"></i><b>2.1.1</b> Indoor Location with Wi-Fi Signals<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="classification.html"><a href="classification.html#performance-metrics"><i class="fa fa-check"></i><b>2.2</b> Performance Metrics<span></span></a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="classification.html"><a href="classification.html#confusion-matrix"><i class="fa fa-check"></i><b>2.2.1</b> Confusion Matrix<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="classification.html"><a href="classification.html#decision-trees"><i class="fa fa-check"></i><b>2.3</b> Decision Trees<span></span></a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="classification.html"><a href="classification.html#activityRecognition"><i class="fa fa-check"></i><b>2.3.1</b> Activity Recognition with Smartphones<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="classification.html"><a href="classification.html#naive-bayes"><i class="fa fa-check"></i><b>2.4</b> Naive Bayes<span></span></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="classification.html"><a href="classification.html#activity-recognition-with-naive-bayes"><i class="fa fa-check"></i><b>2.4.1</b> Activity Recognition with Naive Bayes<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="classification.html"><a href="classification.html#dynamic-time-warping"><i class="fa fa-check"></i><b>2.5</b> Dynamic Time Warping<span></span></a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="classification.html"><a href="classification.html#sechandgestures"><i class="fa fa-check"></i><b>2.5.1</b> Hand Gesture Recognition<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="classification.html"><a href="classification.html#dummy-models"><i class="fa fa-check"></i><b>2.6</b> Dummy Models<span></span></a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="classification.html"><a href="classification.html#most-frequent-class-classifier"><i class="fa fa-check"></i><b>2.6.1</b> Most-frequent-class Classifier<span></span></a></li>
<li class="chapter" data-level="2.6.2" data-path="classification.html"><a href="classification.html#uniform-classifier"><i class="fa fa-check"></i><b>2.6.2</b> Uniform Classifier<span></span></a></li>
<li class="chapter" data-level="2.6.3" data-path="classification.html"><a href="classification.html#frequency-based-classifier"><i class="fa fa-check"></i><b>2.6.3</b> Frequency-based Classifier<span></span></a></li>
<li class="chapter" data-level="2.6.4" data-path="classification.html"><a href="classification.html#other-dummy-classifiers"><i class="fa fa-check"></i><b>2.6.4</b> Other Dummy Classifiers<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="classification.html"><a href="classification.html#summaryClassification"><i class="fa fa-check"></i><b>2.7</b> Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>3</b> Predicting Behavior with Ensemble Learning<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="ensemble.html"><a href="ensemble.html#bagging"><i class="fa fa-check"></i><b>3.1</b> Bagging<span></span></a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ensemble.html"><a href="ensemble.html#activity-recognition-with-bagging"><i class="fa fa-check"></i><b>3.1.1</b> Activity Recognition with Bagging<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ensemble.html"><a href="ensemble.html#random-forest"><i class="fa fa-check"></i><b>3.2</b> Random Forest<span></span></a></li>
<li class="chapter" data-level="3.3" data-path="ensemble.html"><a href="ensemble.html#stacked-generalization"><i class="fa fa-check"></i><b>3.3</b> Stacked Generalization<span></span></a></li>
<li class="chapter" data-level="3.4" data-path="ensemble.html"><a href="ensemble.html#multiviewhometasks"><i class="fa fa-check"></i><b>3.4</b> Multi-view Stacking for Home Tasks Recognition<span></span></a></li>
<li class="chapter" data-level="3.5" data-path="ensemble.html"><a href="ensemble.html#SummaryEnsemble"><i class="fa fa-check"></i><b>3.5</b> Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="edavis.html"><a href="edavis.html"><i class="fa fa-check"></i><b>4</b> Exploring and Visualizing Behavioral Data<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="edavis.html"><a href="edavis.html#talking-with-field-experts"><i class="fa fa-check"></i><b>4.1</b> Talking with Field Experts<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="edavis.html"><a href="edavis.html#summary-statistics"><i class="fa fa-check"></i><b>4.2</b> Summary Statistics<span></span></a></li>
<li class="chapter" data-level="4.3" data-path="edavis.html"><a href="edavis.html#class-distributions"><i class="fa fa-check"></i><b>4.3</b> Class Distributions<span></span></a></li>
<li class="chapter" data-level="4.4" data-path="edavis.html"><a href="edavis.html#user-class-sparsity-matrix"><i class="fa fa-check"></i><b>4.4</b> User-class Sparsity Matrix<span></span></a></li>
<li class="chapter" data-level="4.5" data-path="edavis.html"><a href="edavis.html#boxplots"><i class="fa fa-check"></i><b>4.5</b> Boxplots<span></span></a></li>
<li class="chapter" data-level="4.6" data-path="edavis.html"><a href="edavis.html#correlation-plots"><i class="fa fa-check"></i><b>4.6</b> Correlation Plots<span></span></a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="edavis.html"><a href="edavis.html#interactive-correlation-plots"><i class="fa fa-check"></i><b>4.6.1</b> Interactive Correlation Plots<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="edavis.html"><a href="edavis.html#timeseries"><i class="fa fa-check"></i><b>4.7</b> Timeseries<span></span></a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="edavis.html"><a href="edavis.html#interactive-timeseries"><i class="fa fa-check"></i><b>4.7.1</b> Interactive Timeseries<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="edavis.html"><a href="edavis.html#multidimensional-scaling-mds"><i class="fa fa-check"></i><b>4.8</b> Multidimensional Scaling (MDS)<span></span></a></li>
<li class="chapter" data-level="4.9" data-path="edavis.html"><a href="edavis.html#heatmaps"><i class="fa fa-check"></i><b>4.9</b> Heatmaps<span></span></a></li>
<li class="chapter" data-level="4.10" data-path="edavis.html"><a href="edavis.html#automated-eda"><i class="fa fa-check"></i><b>4.10</b> Automated EDA<span></span></a></li>
<li class="chapter" data-level="4.11" data-path="edavis.html"><a href="edavis.html#SummaryExploratory"><i class="fa fa-check"></i><b>4.11</b> Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="preprocessing.html"><a href="preprocessing.html"><i class="fa fa-check"></i><b>5</b> Preprocessing Behavioral Data<span></span></a>
<ul>
<li class="chapter" data-level="5.1" data-path="preprocessing.html"><a href="preprocessing.html#missing-values"><i class="fa fa-check"></i><b>5.1</b> Missing Values<span></span></a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="preprocessing.html"><a href="preprocessing.html#imputation"><i class="fa fa-check"></i><b>5.1.1</b> Imputation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="preprocessing.html"><a href="preprocessing.html#smoothing"><i class="fa fa-check"></i><b>5.2</b> Smoothing<span></span></a></li>
<li class="chapter" data-level="5.3" data-path="preprocessing.html"><a href="preprocessing.html#normalization"><i class="fa fa-check"></i><b>5.3</b> Normalization<span></span></a></li>
<li class="chapter" data-level="5.4" data-path="preprocessing.html"><a href="preprocessing.html#imbalanced-classes"><i class="fa fa-check"></i><b>5.4</b> Imbalanced Classes<span></span></a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="preprocessing.html"><a href="preprocessing.html#random-oversampling"><i class="fa fa-check"></i><b>5.4.1</b> Random Oversampling<span></span></a></li>
<li class="chapter" data-level="5.4.2" data-path="preprocessing.html"><a href="preprocessing.html#smote"><i class="fa fa-check"></i><b>5.4.2</b> SMOTE<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="preprocessing.html"><a href="preprocessing.html#infoinjection"><i class="fa fa-check"></i><b>5.5</b> Information Injection<span></span></a></li>
<li class="chapter" data-level="5.6" data-path="preprocessing.html"><a href="preprocessing.html#one-hot-encoding"><i class="fa fa-check"></i><b>5.6</b> One-hot Encoding<span></span></a></li>
<li class="chapter" data-level="5.7" data-path="preprocessing.html"><a href="preprocessing.html#SummaryPreprocessing"><i class="fa fa-check"></i><b>5.7</b> Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="unsupervised.html"><a href="unsupervised.html"><i class="fa fa-check"></i><b>6</b> Discovering Behaviors with Unsupervised Learning<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="unsupervised.html"><a href="unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>6.1</b> <span class="math inline">\(k\)</span>-means Clustering<span></span></a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="unsupervised.html"><a href="unsupervised.html#studentresponses"><i class="fa fa-check"></i><b>6.1.1</b> Grouping Student Responses<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="unsupervised.html"><a href="unsupervised.html#the-silhouette-index"><i class="fa fa-check"></i><b>6.2</b> The Silhouette Index<span></span></a></li>
<li class="chapter" data-level="6.3" data-path="unsupervised.html"><a href="unsupervised.html#associationrules"><i class="fa fa-check"></i><b>6.3</b> Mining Association Rules<span></span></a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="unsupervised.html"><a href="unsupervised.html#finding-rules-for-criminal-behavior"><i class="fa fa-check"></i><b>6.3.1</b> Finding Rules for Criminal Behavior<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="unsupervised.html"><a href="unsupervised.html#SummaryUnsupervised"><i class="fa fa-check"></i><b>6.4</b> Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="representations.html"><a href="representations.html"><i class="fa fa-check"></i><b>7</b> Encoding Behavioral Data<span></span></a>
<ul>
<li class="chapter" data-level="7.1" data-path="representations.html"><a href="representations.html#feature-vectors"><i class="fa fa-check"></i><b>7.1</b> Feature Vectors<span></span></a></li>
<li class="chapter" data-level="7.2" data-path="representations.html"><a href="representations.html#sectimeseries"><i class="fa fa-check"></i><b>7.2</b> Timeseries<span></span></a></li>
<li class="chapter" data-level="7.3" data-path="representations.html"><a href="representations.html#transactions"><i class="fa fa-check"></i><b>7.3</b> Transactions<span></span></a></li>
<li class="chapter" data-level="7.4" data-path="representations.html"><a href="representations.html#images"><i class="fa fa-check"></i><b>7.4</b> Images<span></span></a></li>
<li class="chapter" data-level="7.5" data-path="representations.html"><a href="representations.html#recurrence-plots"><i class="fa fa-check"></i><b>7.5</b> Recurrence Plots<span></span></a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="representations.html"><a href="representations.html#computing-recurrence-plots"><i class="fa fa-check"></i><b>7.5.1</b> Computing Recurrence Plots<span></span></a></li>
<li class="chapter" data-level="7.5.2" data-path="representations.html"><a href="representations.html#recurrence-plots-of-hand-gestures"><i class="fa fa-check"></i><b>7.5.2</b> Recurrence Plots of Hand Gestures<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="representations.html"><a href="representations.html#bag-of-words"><i class="fa fa-check"></i><b>7.6</b> Bag-of-Words<span></span></a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="representations.html"><a href="representations.html#bow-for-complex-activities."><i class="fa fa-check"></i><b>7.6.1</b> BoW for Complex Activities.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="representations.html"><a href="representations.html#graphs"><i class="fa fa-check"></i><b>7.7</b> Graphs<span></span></a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="representations.html"><a href="representations.html#complex-activities-as-graphs"><i class="fa fa-check"></i><b>7.7.1</b> Complex Activities as Graphs<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="representations.html"><a href="representations.html#SummaryRepresentations"><i class="fa fa-check"></i><b>7.8</b> Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deeplearning.html"><a href="deeplearning.html"><i class="fa fa-check"></i><b>8</b> Predicting Behavior with Deep Learning<span></span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="deeplearning.html"><a href="deeplearning.html#ann"><i class="fa fa-check"></i><b>8.1</b> Introduction to Artificial Neural Networks<span></span></a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="deeplearning.html"><a href="deeplearning.html#sigmoid-and-relu-units"><i class="fa fa-check"></i><b>8.1.1</b> Sigmoid and ReLU Units<span></span></a></li>
<li class="chapter" data-level="8.1.2" data-path="deeplearning.html"><a href="deeplearning.html#assembling-units-into-layers"><i class="fa fa-check"></i><b>8.1.2</b> Assembling Units into Layers<span></span></a></li>
<li class="chapter" data-level="8.1.3" data-path="deeplearning.html"><a href="deeplearning.html#deep-neural-networks"><i class="fa fa-check"></i><b>8.1.3</b> Deep Neural Networks<span></span></a></li>
<li class="chapter" data-level="8.1.4" data-path="deeplearning.html"><a href="deeplearning.html#learning-the-parameters"><i class="fa fa-check"></i><b>8.1.4</b> Learning the Parameters<span></span></a></li>
<li class="chapter" data-level="8.1.5" data-path="deeplearning.html"><a href="deeplearning.html#parameter-learning-example-in-r"><i class="fa fa-check"></i><b>8.1.5</b> Parameter Learning Example in R<span></span></a></li>
<li class="chapter" data-level="8.1.6" data-path="deeplearning.html"><a href="deeplearning.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>8.1.6</b> Stochastic Gradient Descent<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="deeplearning.html"><a href="deeplearning.html#keras-and-tensorflow-with-r"><i class="fa fa-check"></i><b>8.2</b> Keras and TensorFlow with R<span></span></a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="deeplearning.html"><a href="deeplearning.html#keras-example"><i class="fa fa-check"></i><b>8.2.1</b> Keras Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="deeplearning.html"><a href="deeplearning.html#classification-with-neural-networks"><i class="fa fa-check"></i><b>8.3</b> Classification with Neural Networks<span></span></a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="deeplearning.html"><a href="deeplearning.html#classification-of-electromyography-signals"><i class="fa fa-check"></i><b>8.3.1</b> Classification of Electromyography Signals<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="deeplearning.html"><a href="deeplearning.html#overfitting"><i class="fa fa-check"></i><b>8.4</b> Overfitting<span></span></a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="deeplearning.html"><a href="deeplearning.html#early-stopping"><i class="fa fa-check"></i><b>8.4.1</b> Early Stopping<span></span></a></li>
<li class="chapter" data-level="8.4.2" data-path="deeplearning.html"><a href="deeplearning.html#dropout"><i class="fa fa-check"></i><b>8.4.2</b> Dropout<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="deeplearning.html"><a href="deeplearning.html#fine-tuning-a-neural-network"><i class="fa fa-check"></i><b>8.5</b> Fine-tuning a Neural Network<span></span></a></li>
<li class="chapter" data-level="8.6" data-path="deeplearning.html"><a href="deeplearning.html#cnns"><i class="fa fa-check"></i><b>8.6</b> Convolutional Neural Networks<span></span></a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="deeplearning.html"><a href="deeplearning.html#convolutions"><i class="fa fa-check"></i><b>8.6.1</b> Convolutions<span></span></a></li>
<li class="chapter" data-level="8.6.2" data-path="deeplearning.html"><a href="deeplearning.html#pooling-operations"><i class="fa fa-check"></i><b>8.6.2</b> Pooling Operations<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="deeplearning.html"><a href="deeplearning.html#cnns-with-keras"><i class="fa fa-check"></i><b>8.7</b> CNNs with Keras<span></span></a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="deeplearning.html"><a href="deeplearning.html#example-1"><i class="fa fa-check"></i><b>8.7.1</b> Example 1<span></span></a></li>
<li class="chapter" data-level="8.7.2" data-path="deeplearning.html"><a href="deeplearning.html#example-2"><i class="fa fa-check"></i><b>8.7.2</b> Example 2<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="deeplearning.html"><a href="deeplearning.html#cnnSmile"><i class="fa fa-check"></i><b>8.8</b> Smiles Detection with a CNN<span></span></a></li>
<li class="chapter" data-level="8.9" data-path="deeplearning.html"><a href="deeplearning.html#SummaryDeepLearning"><i class="fa fa-check"></i><b>8.9</b> Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multiuser.html"><a href="multiuser.html"><i class="fa fa-check"></i><b>9</b> Multi-user Validation<span></span></a>
<ul>
<li class="chapter" data-level="9.1" data-path="multiuser.html"><a href="multiuser.html#mixed-models"><i class="fa fa-check"></i><b>9.1</b> Mixed Models<span></span></a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="multiuser.html"><a href="multiuser.html#skeleton-action-recognition-with-mixed-models"><i class="fa fa-check"></i><b>9.1.1</b> Skeleton Action Recognition with Mixed Models<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="multiuser.html"><a href="multiuser.html#user-independent-models"><i class="fa fa-check"></i><b>9.2</b> User-independent Models<span></span></a></li>
<li class="chapter" data-level="9.3" data-path="multiuser.html"><a href="multiuser.html#user-dependent-models"><i class="fa fa-check"></i><b>9.3</b> User-dependent Models<span></span></a></li>
<li class="chapter" data-level="9.4" data-path="multiuser.html"><a href="multiuser.html#user-adaptive-models"><i class="fa fa-check"></i><b>9.4</b> User-adaptive Models<span></span></a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="multiuser.html"><a href="multiuser.html#transfer-learning"><i class="fa fa-check"></i><b>9.4.1</b> Transfer Learning<span></span></a></li>
<li class="chapter" data-level="9.4.2" data-path="multiuser.html"><a href="multiuser.html#a-user-adaptive-model-for-activity-recognition"><i class="fa fa-check"></i><b>9.4.2</b> A User-adaptive Model for Activity Recognition<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="multiuser.html"><a href="multiuser.html#SummaryMultiUser"><i class="fa fa-check"></i><b>9.5</b> Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html"><i class="fa fa-check"></i><b>10</b> Detecting Abnormal Behaviors<span></span></a>
<ul>
<li class="chapter" data-level="10.1" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#isolation-forests"><i class="fa fa-check"></i><b>10.1</b> Isolation Forests<span></span></a></li>
<li class="chapter" data-level="10.2" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#detecting-abnormal-fish-behaviors"><i class="fa fa-check"></i><b>10.2</b> Detecting Abnormal Fish Behaviors<span></span></a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#exploring-and-visualizing-trajectories"><i class="fa fa-check"></i><b>10.2.1</b> Exploring and Visualizing Trajectories<span></span></a></li>
<li class="chapter" data-level="10.2.2" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#preprocessing-and-feature-extraction"><i class="fa fa-check"></i><b>10.2.2</b> Preprocessing and Feature Extraction<span></span></a></li>
<li class="chapter" data-level="10.2.3" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#training-the-model"><i class="fa fa-check"></i><b>10.2.3</b> Training the Model<span></span></a></li>
<li class="chapter" data-level="10.2.4" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#roc-curve-and-auc"><i class="fa fa-check"></i><b>10.2.4</b> ROC Curve and AUC<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#autoencoders"><i class="fa fa-check"></i><b>10.3</b> Autoencoders<span></span></a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#autoencoders-for-anomaly-detection"><i class="fa fa-check"></i><b>10.3.1</b> Autoencoders for Anomaly Detection<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="abnormalbehaviors.html"><a href="abnormalbehaviors.html#SummaryAnomalyDetection"><i class="fa fa-check"></i><b>10.4</b> Summary<span></span></a></li>
</ul></li>
<li class="appendix"><span><b>Appendix<span></span></b></span></li>
<li class="chapter" data-level="A" data-path="appendixInstall.html"><a href="appendixInstall.html"><i class="fa fa-check"></i><b>A</b> Setup Your Environment<span></span></a>
<ul>
<li class="chapter" data-level="A.1" data-path="appendixInstall.html"><a href="appendixInstall.html#installing-the-datasets"><i class="fa fa-check"></i><b>A.1</b> Installing the Datasets<span></span></a></li>
<li class="chapter" data-level="A.2" data-path="appendixInstall.html"><a href="appendixInstall.html#installing-the-examples-source-code"><i class="fa fa-check"></i><b>A.2</b> Installing the Examples Source Code<span></span></a></li>
<li class="chapter" data-level="A.3" data-path="appendixInstall.html"><a href="appendixInstall.html#running-shiny-apps"><i class="fa fa-check"></i><b>A.3</b> Running Shiny Apps<span></span></a></li>
<li class="chapter" data-level="A.4" data-path="appendixInstall.html"><a href="appendixInstall.html#installing-keras-and-tensorflow"><i class="fa fa-check"></i><b>A.4</b> Installing Keras and TensorFlow<span></span></a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendixDatasets.html"><a href="appendixDatasets.html"><i class="fa fa-check"></i><b>B</b> Datasets<span></span></a>
<ul>
<li class="chapter" data-level="B.1" data-path="appendixDatasets.html"><a href="appendixDatasets.html#complex-activities"><i class="fa fa-check"></i><b>B.1</b> COMPLEX ACTIVITIES<span></span></a></li>
<li class="chapter" data-level="B.2" data-path="appendixDatasets.html"><a href="appendixDatasets.html#depresjon"><i class="fa fa-check"></i><b>B.2</b> DEPRESJON<span></span></a></li>
<li class="chapter" data-level="B.3" data-path="appendixDatasets.html"><a href="appendixDatasets.html#electromyography"><i class="fa fa-check"></i><b>B.3</b> ELECTROMYOGRAPHY<span></span></a></li>
<li class="chapter" data-level="B.4" data-path="appendixDatasets.html"><a href="appendixDatasets.html#fish-trajectories"><i class="fa fa-check"></i><b>B.4</b> FISH TRAJECTORIES<span></span></a></li>
<li class="chapter" data-level="B.5" data-path="appendixDatasets.html"><a href="appendixDatasets.html#hand-gestures"><i class="fa fa-check"></i><b>B.5</b> HAND GESTURES<span></span></a></li>
<li class="chapter" data-level="B.6" data-path="appendixDatasets.html"><a href="appendixDatasets.html#home-tasks"><i class="fa fa-check"></i><b>B.6</b> HOME TASKS<span></span></a></li>
<li class="chapter" data-level="B.7" data-path="appendixDatasets.html"><a href="appendixDatasets.html#homicide-reports"><i class="fa fa-check"></i><b>B.7</b> HOMICIDE REPORTS<span></span></a></li>
<li class="chapter" data-level="B.8" data-path="appendixDatasets.html"><a href="appendixDatasets.html#indoor-location"><i class="fa fa-check"></i><b>B.8</b> INDOOR LOCATION<span></span></a></li>
<li class="chapter" data-level="B.9" data-path="appendixDatasets.html"><a href="appendixDatasets.html#sheep-goats"><i class="fa fa-check"></i><b>B.9</b> SHEEP GOATS<span></span></a></li>
<li class="chapter" data-level="B.10" data-path="appendixDatasets.html"><a href="appendixDatasets.html#skeleton-actions"><i class="fa fa-check"></i><b>B.10</b> SKELETON ACTIONS<span></span></a></li>
<li class="chapter" data-level="B.11" data-path="appendixDatasets.html"><a href="appendixDatasets.html#smartphone-activities"><i class="fa fa-check"></i><b>B.11</b> SMARTPHONE ACTIVITIES<span></span></a></li>
<li class="chapter" data-level="B.12" data-path="appendixDatasets.html"><a href="appendixDatasets.html#smiles"><i class="fa fa-check"></i><b>B.12</b> SMILES<span></span></a></li>
<li class="chapter" data-level="B.13" data-path="appendixDatasets.html"><a href="appendixDatasets.html#students-mental-health"><i class="fa fa-check"></i><b>B.13</b> STUDENTS’ MENTAL HEALTH<span></span></a></li>
</ul></li>
<li><a href="citing-this-book.html#citing-this-book">Citing this Book<span></span></a></li>
<li><a href="references.html#references">References<span></span></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Behavior Analysis with Machine Learning Using R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="appendixDatasets" class="section level1 hasAnchor" number="12">
<h1><span class="header-section-number">B</span> Datasets<a href="appendixDatasets.html#appendixDatasets" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This Appendix has a list with a description of all the datasets used in this book. A compressed file with a compilation of most of the datasets can be downloaded here: <a href="https://github.com/enriquegit/behavior-free-datasets" class="uri">https://github.com/enriquegit/behavior-free-datasets</a></p>
<p>I recommend you to download the datasets compilation file and extract its contents to a local directory. Due to some datasets with large file sizes or license restrictions, not all of them are included in the compiled set. But you can download them separately. Even though a dataset may not be included in the compiled set, it will have a corresponding directory with a README file with instructions on how to obtain it.</p>
<p>Each dataset in the following list, states whether or not it is included in the compiled set. The datasets are ordered alphabetically.</p>
<div id="complex-activities" class="section level2 hasAnchor" number="12.1">
<h2><span class="header-section-number">B.1</span> COMPLEX ACTIVITIES<a href="appendixDatasets.html#complex-activities" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Included:</strong> Yes.</p>
<p>This dataset was collected with a smartphone and contains <span class="math inline">\(5\)</span> complex activities: <em>‘commuting’</em>, <em>‘working, at home’</em>, <em>‘shopping at the supermarket’</em> and <em>‘exercising’</em>.
An Android 2.2 application running on a LG Optimus Me cellphone was used to collect the accelerometer data from each of the axes (x,y,z). The sample rate was set at <span class="math inline">\(50\)</span> Hz. The cellphone was placed in the user’s belt. A training and a test set were collected on different days. The duration of the activities varies from about <span class="math inline">\(5\)</span> minutes to a couple of hours. The total recorded data consists of approximately <span class="math inline">\(41\)</span> hours. The data was collected by one user. Each file contains a whole activity.</p>
</div>
<div id="depresjon" class="section level2 hasAnchor" number="12.2">
<h2><span class="header-section-number">B.2</span> DEPRESJON<a href="appendixDatasets.html#depresjon" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Included:</strong> Yes.</p>
<p>This dataset contains motor activity recordings of <span class="math inline">\(23\)</span> unipolar and bipolar depressed patients and <span class="math inline">\(32\)</span> healthy controls. Motor activity was monitored with an actigraph watch worn at the right wrist (Actiwatch, Cambridge Neurotechnology Ltd, England, model AW4). The sampling frequency was <span class="math inline">\(32\)</span> Hz. The device uses the inertial sensors data to compute an activity count every minute which is stored as an integer value in the memory unit of the actigraph watch. The number of counts is proportional to the intensity of the movement. The dataset also contains some additional information about the patients and the control group. For more details please see <span class="citation"><a href="#ref-Garcia2018mm" role="doc-biblioref">Garcia-Ceja, Riegler, Jakobsen, et al.</a> (<a href="#ref-Garcia2018mm" role="doc-biblioref">2018</a>)</span>.</p>
</div>
<div id="electromyography" class="section level2 hasAnchor" number="12.3">
<h2><span class="header-section-number">B.3</span> ELECTROMYOGRAPHY<a href="appendixDatasets.html#electromyography" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Included:</strong> Yes.</p>
<p>This dataset was made available by Kirill Yashuk. The data was collected using an armband device that has <span class="math inline">\(8\)</span> sensors placed on the skin surface that measure electrical activity from the right forearm at a sampling rate of <span class="math inline">\(200\)</span> Hz. A video of the device can be seen here: <a href="https://youtu.be/OuwDHfY2Awg" class="uri">https://youtu.be/OuwDHfY2Awg</a>.</p>
<p>The data contains <span class="math inline">\(4\)</span> different gestures: 0-rock, 1-scissors, 2-paper, 3-OK, and has <span class="math inline">\(65\)</span> columns. The last column is the class label from <span class="math inline">\(0\)</span> to <span class="math inline">\(3\)</span>. Each gesture was recorded <span class="math inline">\(6\)</span> times for <span class="math inline">\(20\)</span> seconds. The first <span class="math inline">\(64\)</span> columns are electrical measurements. <span class="math inline">\(8\)</span> consecutive readings for each of the <span class="math inline">\(8\)</span> sensors. For more details, please see <span class="citation"><a href="#ref-kirill" role="doc-biblioref">Yashuk</a> (<a href="#ref-kirill" role="doc-biblioref">2019</a>)</span>.</p>
</div>
<div id="fish-trajectories" class="section level2 hasAnchor" number="12.4">
<h2><span class="header-section-number">B.4</span> FISH TRAJECTORIES<a href="appendixDatasets.html#fish-trajectories" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Included:</strong> Yes.</p>
<p>The Fish4Knowledge<a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a> <span class="citation">(<a href="#ref-Beyan2013" role="doc-biblioref">Beyan and Fisher 2013</a>)</span> project made this database available. It contains <span class="math inline">\(3102\)</span> trajectories belonging to the <em>Dascyllus reticulatus</em> fish observed in the Taiwanese coral reef. Each trajectory is labeled as <em>‘normal’</em> or <em>‘abnormal’</em>. The trajectories were extracted from underwater video. Bounding box’s coordinates over time were extracted from the video. The data does not contain the video images but the final coordinates. The dataset compilation in this book also includes a .csv file with extracted features from the trajectories.</p>
</div>
<div id="hand-gestures" class="section level2 hasAnchor" number="12.5">
<h2><span class="header-section-number">B.5</span> HAND GESTURES<a href="appendixDatasets.html#hand-gestures" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Included:</strong> Yes.</p>
<p>The data was collected using an LG Optimus Me smartphone using its accelerometer sensor. The data was collected by <span class="math inline">\(10\)</span> subjects which performed <span class="math inline">\(5\)</span> repetitions for each of the <span class="math inline">\(10\)</span> different gestures (<em>‘triangle’</em>, <em>‘square’</em>, <em>‘circle’</em>, <em>‘a’</em>, <em>‘b’</em>, <em>‘c’</em>, <em>‘1’</em>, <em>‘2’</em>, <em>‘3’</em>, <em>‘4’</em>) giving a total of <span class="math inline">\(500\)</span> instances. The sensor is a tri-axial accelerometer which returns values for the x, y, and z axes. The sampling rate was set at <span class="math inline">\(50\)</span> Hz. To record a gesture the user presses the phone screen with her/his thumb, performs the gesture, and stops pressing the screen. For more information, please see <span class="citation"><a href="#ref-EnriqueGestures2014" role="doc-biblioref">Garcia-Ceja, Brena, and Galván-Tejada</a> (<a href="#ref-EnriqueGestures2014" role="doc-biblioref">n.d.</a>)</span>.</p>
</div>
<div id="home-tasks" class="section level2 hasAnchor" number="12.6">
<h2><span class="header-section-number">B.6</span> HOME TASKS<a href="appendixDatasets.html#home-tasks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Included:</strong> Yes.</p>
<p>Sound and accelerometer data were collected by <span class="math inline">\(3\)</span> volunteers while performing <span class="math inline">\(7\)</span> different home task activities: <em>‘mop floor’</em>, <em>‘sweep floor’</em>, <em>‘type on computer keyboard’</em>, <em>‘brush teeth’</em>, <em>‘wash hands’</em>, <em>‘eat chips’</em>, and <em>‘watch t.v’</em>. Each volunteer performed each activity for approximately <span class="math inline">\(3\)</span> minutes. If the activity lasted less than <span class="math inline">\(3\)</span> minutes, another session was recorded until completing the <span class="math inline">\(3\)</span> minutes. The data were collected with a wrist-band (Microsoft Band 2) and a cellphone. The wrist-band was used to collect accelerometer data and was worn by the volunteers in their dominant hand. The accelerometer sensor returns values from the x, y, and z axes, and the sampling rate was set to <span class="math inline">\(31\)</span> Hz. A cellphone was used to record environmental sound with a sampling rate of <span class="math inline">\(8000\)</span> Hz and it was placed on a table in the same room the user was performing the activity. To preserve privacy, the dataset does not contain the raw audio recordings but extracted features. Sixteen features from the accelerometer sensor and <span class="math inline">\(12\)</span> Mel frequency cepstral coefficients from the audio recordings. For more information, please see <span class="citation"><a href="#ref-garcia2018multiview" role="doc-biblioref">Garcia-Ceja, Galván-Tejada, and Brena</a> (<a href="#ref-garcia2018multiview" role="doc-biblioref">2018</a>)</span>.</p>
</div>
<div id="homicide-reports" class="section level2 hasAnchor" number="12.7">
<h2><span class="header-section-number">B.7</span> HOMICIDE REPORTS<a href="appendixDatasets.html#homicide-reports" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Included:</strong> Yes.</p>
<p>This dataset was compiled and made available by the Murder Accountability Project, founded by Thomas Hargrove<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a>.
It contains information about homicides in the United States. This dataset includes the age, race, sex, ethnicity of victims, and perpetrators, in addition to the relationship between the victim and perpetrator and weapon used. The original dataset includes the <code>database.csv</code> file. The files <code>processed.csv</code> and <code>transactions.RData</code> were generated with the R scripts included in the examples code of the corresponding sections to facilitate the analysis.</p>
</div>
<div id="indoor-location" class="section level2 hasAnchor" number="12.8">
<h2><span class="header-section-number">B.8</span> INDOOR LOCATION<a href="appendixDatasets.html#indoor-location" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Included:</strong> Yes.</p>
<p>This dataset contains Wi-Fi signal recordings fo access points from different locations in a building including their MAC address and signal strength. The data was collected with an Android 2.2 application running on a LG Optimus Me cell phone. To generate a single instance, the device scans and records the MAC address and signal strength of the nearby access points. A delay of <span class="math inline">\(500\)</span> ms is set between scans. For each location, approximately <span class="math inline">\(3\)</span> minutes of data were collected while the user walked around the specific location. The data includes four different locations: <em>‘bedroomA’</em>, <em>‘beadroomB’</em>, <em>‘tv room’</em> and the <em>‘lobby’</em>. To preserve privacy, the MAC addresses are encoded as integer numbers. For more information, please, see <span class="citation"><a href="#ref-Garcia2012WiFi" role="doc-biblioref">Garcia and Brena</a> (<a href="#ref-Garcia2012WiFi" role="doc-biblioref">2012</a>)</span>.</p>
</div>
<div id="sheep-goats" class="section level2 hasAnchor" number="12.9">
<h2><span class="header-section-number">B.9</span> SHEEP GOATS<a href="appendixDatasets.html#sheep-goats" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Included:</strong> No.</p>
<p>The dataset was made available by <span class="citation"><a href="#ref-kamminga2017" role="doc-biblioref">Kamminga et al.</a> (<a href="#ref-kamminga2017" role="doc-biblioref">2017</a>)</span> and can be downloaded from <a href="https://easy.dans.knaw.nl/ui/datasets/id/easy-dataset:76131" class="uri">https://easy.dans.knaw.nl/ui/datasets/id/easy-dataset:76131</a>. The researchers placed inertial sensors on sheep and goats and tracked their behavior during one day. They also video-recorded the session and annotated the data with different types of behaviors such as <em>‘grazing’</em>, <em>‘fighting’</em>, <em>‘scratch-biting’</em>, etc. The device was placed on the neck with random orientation and it collects acceleration, orientation, magnetic field, temperature, and barometric pressure. In this book, only data from one of the sheep is used (<code>Sheep/S1.csv</code>).</p>
</div>
<div id="skeleton-actions" class="section level2 hasAnchor" number="12.10">
<h2><span class="header-section-number">B.10</span> SKELETON ACTIONS<a href="appendixDatasets.html#skeleton-actions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Included:</strong> No.</p>
<p>The authors of this dataset are <span class="citation"><a href="#ref-chen2015utd" role="doc-biblioref">Chen, Jafari, and Kehtarnavaz</a> (<a href="#ref-chen2015utd" role="doc-biblioref">2015</a>)</span>. The data was recorded by <span class="math inline">\(8\)</span> subjects with a Kinect camera and an inertial sensor unit and each subject repeated each action <span class="math inline">\(4\)</span> times. The number of actions is <span class="math inline">\(27\)</span> and some of the actions include: <em>‘right hand wave’</em>, <em>‘two hand front clap’</em>, <em>‘basketball shoot’</em>, <em>‘front boxing’</em>, etc. More information about the collection process and pictures can be consulted on the website <a href="https://personal.utdallas.edu/~kehtar/UTD-MHAD.html" class="uri">https://personal.utdallas.edu/~kehtar/UTD-MHAD.html</a>. You only need to download the <code>Skeleton_Data.zip file</code>.</p>
</div>
<div id="smartphone-activities" class="section level2 hasAnchor" number="12.11">
<h2><span class="header-section-number">B.11</span> SMARTPHONE ACTIVITIES<a href="appendixDatasets.html#smartphone-activities" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Included:</strong> Yes.</p>
<p>This dataset is called WISDM<a href="#fn34" class="footnote-ref" id="fnref34"><sup>34</sup></a> and was made available by <span class="citation"><a href="#ref-kwapisz2010" role="doc-biblioref">Kwapisz, Weiss, and Moore</a> (<a href="#ref-kwapisz2010" role="doc-biblioref">2010</a>)</span>. The dataset includes <span class="math inline">\(6\)</span> different activities: <em>‘walking’</em>, <em>‘jogging’</em>, <em>‘walking upstairs’</em>, <em>‘walking downstairs’</em>, <em>‘sitting’</em>, and <em>‘standing’</em>. The data was collected by <span class="math inline">\(36\)</span> volunteers with the accelerometer of an Android phone located in the users’ pants pocket and with a sampling rate of <span class="math inline">\(20\)</span> Hz.</p>
</div>
<div id="smiles" class="section level2 hasAnchor" number="12.12">
<h2><span class="header-section-number">B.12</span> SMILES<a href="appendixDatasets.html#smiles" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Included:</strong> No.</p>
<p>This dataset contains color face images of <span class="math inline">\(64 \times 64\)</span> pixels and is published here: <a href="http://conradsanderson.id.au/lfwcrop/" class="uri">http://conradsanderson.id.au/lfwcrop/</a>. This is a cropped version <span class="citation">(<a href="#ref-sanderson2009multi" role="doc-biblioref">Sanderson and Lovell 2009</a>)</span> of the Labeled Faces in the Wild (LFW) database <span class="citation">(<a href="#ref-huang2008labeled" role="doc-biblioref">Gary B. Huang et al. 2008</a>)</span>. Please, download the color version (lfwcrop_color.zip) and copy all ppm files into the <code>faces/</code> directory.</p>
<p>A subset of the database was labeled by <span class="citation"><a href="#ref-arigbabu2016smile" role="doc-biblioref">O. A. Arigbabu et al.</a> (<a href="#ref-arigbabu2016smile" role="doc-biblioref">2016</a>)</span>, <span class="citation"><a href="#ref-olasimbo" role="doc-biblioref">O. Arigbabu</a> (<a href="#ref-olasimbo" role="doc-biblioref">2017</a>)</span>. The labels are provided as two text files (SMILE_list.txt, NON-SMILE_list.txt), each, containing the list of files that correspond to smiling and non-smiling faces (CC BY 4.0 <a href="https://creativecommons.org/licenses/by/4.0/legalcode" class="uri">https://creativecommons.org/licenses/by/4.0/legalcode</a>). The smiling set has <span class="math inline">\(600\)</span> pictures and the non-smiling has <span class="math inline">\(603\)</span> pictures.</p>
</div>
<div id="students-mental-health" class="section level2 hasAnchor" number="12.13">
<h2><span class="header-section-number">B.13</span> STUDENTS’ MENTAL HEALTH<a href="appendixDatasets.html#students-mental-health" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Included:</strong> Yes.</p>
<p>This dataset contains <span class="math inline">\(268\)</span> survey responses that include variables related to depression, acculturative stress, social connectedness, and help-seeking behaviors reported by international and domestic students at an international university in Japan. For a detailed description, please see <span class="citation">(<a href="#ref-Minh2019" role="doc-biblioref">Nguyen et al. 2019</a>)</span>.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-olasimbo" class="csl-entry">
Arigbabu, Olasimbo. 2017. <em>Dataset for Smile Detection from Face Images</em>. <a href="http://dx.doi.org/10.17632/yz4v8tb3tp.5">http://dx.doi.org/10.17632/yz4v8tb3tp.5</a>.
</div>
<div id="ref-arigbabu2016smile" class="csl-entry">
Arigbabu, Olasimbo Ayodeji, Saif Mahmood, Sharifah Mumtazah Syed Ahmad, and Abayomi A Arigbabu. 2016. <span>“Smile Detection Using Hybrid Face Representation.”</span> <em>Journal of Ambient Intelligence and Humanized Computing</em> 7 (3): 415–26.
</div>
<div id="ref-Beyan2013" class="csl-entry">
Beyan, Cigdem, and Robert B Fisher. 2013. <span>“Detecting Abnormal Fish Trajectories Using Clustered and Labeled Data.”</span> In <em>2013 <span>IEEE</span> International Conference on Image Processing</em>, 1476–80. <span>IEEE</span>.
</div>
<div id="ref-chen2015utd" class="csl-entry">
Chen, Chen, Roozbeh Jafari, and Nasser Kehtarnavaz. 2015. <span>“UTD-MHAD: A Multimodal Dataset for Human Action Recognition Utilizing a Depth Camera and a Wearable Inertial Sensor.”</span> In <em>2015 IEEE International Conference on Image Processing (ICIP)</em>, 168–72. IEEE.
</div>
<div id="ref-Garcia2012WiFi" class="csl-entry">
Garcia, Enrique A., and Ramon F. Brena. 2012. <span>“Real Time Activity Recognition Using a Cell Phone’s Accelerometer and Wi-Fi.”</span> In <em>Workshop Proceedings of the 8th International Conference on Intelligent Environments</em>, 13:94–103. Ambient Intelligence and Smart Environments. IOS Press. <a href="https://doi.org/10.3233/978-1-61499-080-2-94">https://doi.org/10.3233/978-1-61499-080-2-94</a>.
</div>
<div id="ref-EnriqueGestures2014" class="csl-entry">
Garcia-Ceja, Enrique, Ramon Brena, and CarlosE. Galván-Tejada. n.d. <span>“Contextualized Hand Gesture Recognition with Smartphones.”</span> In <em>Pattern Recognition</em>, edited by JoséFrancisco Martínez-Trinidad, JesúsAriel Carrasco-Ochoa, JoséArturo Olvera-Lopez, Joaquín Salas-Rodríguez, and ChingY. Suen, 8495:122–31. Lecture Notes in Computer Science. Cham, Switzerland: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-07491-7_13">https://doi.org/10.1007/978-3-319-07491-7_13</a>.
</div>
<div id="ref-garcia2018multiview" class="csl-entry">
Garcia-Ceja, Enrique, Carlos E Galván-Tejada, and Ramon Brena. 2018. <span>“Multi-View Stacking for Activity Recognition with Sound and Accelerometer Data.”</span> <em>Information Fusion</em> 40: 45–56.
</div>
<div id="ref-Garcia2018mm" class="csl-entry">
Garcia-Ceja, Enrique, Michael Riegler, Petter Jakobsen, Jim Tørresen, Tine Nordgreen, Ketil J. Oedegaard, and Ole Bernt Fasmer. 2018. <span>“Depresjon: A Motor Activity Database of Depression Episodes in Unipolar and Bipolar Patients.”</span> In <em>Proceedings of the 9th ACM on Multimedia Systems Conference</em>. MMSys’18. Amsterdam, The Netherlands: ACM. <a href="https://doi.org/10.1145/3204949.3208125">https://doi.org/10.1145/3204949.3208125</a>.
</div>
<div id="ref-huang2008labeled" class="csl-entry">
Huang, Gary B, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. 2008. <span>“Labeled Faces in the Wild: A Database Forstudying Face Recognition in Unconstrained Environments.”</span> In.
</div>
<div id="ref-kamminga2017" class="csl-entry">
Kamminga, Jacob W, Helena C Bisby, Duc V Le, Nirvana Meratnia, and Paul JM Havinga. 2017. <span>“Generic Online Animal Activity Recognition on Collar Tags.”</span> In <em>Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers</em>, 597–606.
</div>
<div id="ref-kwapisz2010" class="csl-entry">
Kwapisz, Jennifer R., Gary M. Weiss, and Samuel A. Moore. 2010. <span>“Activity Recognition Using Cell Phone Accelerometers.”</span> In <em>Proceedings of the Fourth International Workshop on Knowledge Discovery from Sensor Data (at <span>KDD</span>-10), Washington <span>DC</span>.</em>
</div>
<div id="ref-Minh2019" class="csl-entry">
Nguyen, Minh-Hoang, Manh-Toan Ho, Quynh-Yen T. Nguyen, and Quan-Hoang Vuong. 2019. <span>“A Dataset of Students’ Mental Health and Help-Seeking Behaviors in a Multicultural Environment.”</span> <em>Data</em> 4 (3). <a href="https://doi.org/10.3390/data4030124">https://doi.org/10.3390/data4030124</a>.
</div>
<div id="ref-sanderson2009multi" class="csl-entry">
Sanderson, Conrad, and Brian C Lovell. 2009. <span>“Multi-Region Probabilistic Histograms for Robust and Scalable Identity Inference.”</span> In <em>International Conference on Biometrics</em>, 199–208. Berlin, Heidelberg: Springer.
</div>
<div id="ref-kirill" class="csl-entry">
Yashuk, Kirill. 2019. <em>Classify Gestures by Reading Muscle Activity: A Recording of Human Hand Muscle Activity Producing Four Different Hand Gestures</em>. <a href="https://www.kaggle.com/kyr7plus/emg-4">https://www.kaggle.com/kyr7plus/emg-4</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="32">
<li id="fn32"><p><a href="http://groups.inf.ed.ac.uk/f4k/" class="uri">http://groups.inf.ed.ac.uk/f4k/</a><a href="appendixDatasets.html#fnref32" class="footnote-back">↩︎</a></p></li>
<li id="fn33"><p><a href="https://www.kaggle.com/murderaccountability/homicide-reports" class="uri">https://www.kaggle.com/murderaccountability/homicide-reports</a><a href="appendixDatasets.html#fnref33" class="footnote-back">↩︎</a></p></li>
<li id="fn34"><p><a href="http://www.cis.fordham.edu/wisdm/dataset.php" class="uri">http://www.cis.fordham.edu/wisdm/dataset.php</a><a href="appendixDatasets.html#fnref34" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="appendixInstall.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="citing-this-book.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
