<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.6 Simple Classification Example | Behavior Analysis with Machine Learning and R</title>
  <meta name="description" content="1.6 Simple Classification Example | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors or stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="1.6 Simple Classification Example | Behavior Analysis with Machine Learning and R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="1.6 Simple Classification Example | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors or stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.6 Simple Classification Example | Behavior Analysis with Machine Learning and R" />
  
  <meta name="twitter:description" content="1.6 Simple Classification Example | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors or stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  <meta name="twitter:image" content="images/cover.png" />

<meta name="author" content="Enrique Garcia Ceja" />


<meta name="date" content="2020-09-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="trainingeval.html"/>
<link rel="next" href="simple-regression-example.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/d3-4.9.0/d3.min.js"></script>
<script src="libs/d3-tip-0.7.1/index-min.js"></script>
<link href="libs/d3panels-1.4.9/d3panels.min.css" rel="stylesheet" />
<script src="libs/d3panels-1.4.9/d3panels.min.js"></script>
<script src="libs/qtlcharts_iplotCorr-0.11.6/iplotCorr.js"></script>
<script src="libs/qtlcharts_iplotCorr-0.11.6/iplotCorr_noscat.js"></script>
<script src="libs/iplotCorr-binding-0.11.6/iplotCorr.js"></script>
<link href="libs/dygraphs-1.1.1/dygraph.css" rel="stylesheet" />
<script src="libs/dygraphs-1.1.1/dygraph-combined.js"></script>
<script src="libs/dygraphs-1.1.1/shapes.js"></script>
<script src="libs/moment-2.8.4/moment.js"></script>
<script src="libs/moment-timezone-0.2.5/moment-timezone-with-data.js"></script>
<script src="libs/moment-fquarter-1.0.0/moment-fquarter.min.js"></script>
<script src="libs/dygraphs-binding-1.1.1.6/dygraphs.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178679335-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178679335-1', { 'anonymize_ip': true });
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="#">Behavior Analysis with Machine Learning and R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#supplemental-material"><i class="fa fa-check"></i>Supplemental Material</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#conventions"><i class="fa fa-check"></i>Conventions</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.2" data-path="taxonomy.html"><a href="taxonomy.html"><i class="fa fa-check"></i><b>1.2</b> Types of Machine Learning</a></li>
<li class="chapter" data-level="1.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>1.3</b> Terminology</a><ul>
<li class="chapter" data-level="1.3.1" data-path="terminology.html"><a href="terminology.html#tables"><i class="fa fa-check"></i><b>1.3.1</b> Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="terminology.html"><a href="terminology.html#variable-types"><i class="fa fa-check"></i><b>1.3.2</b> Variable types</a></li>
<li class="chapter" data-level="1.3.3" data-path="terminology.html"><a href="terminology.html#predictive-models"><i class="fa fa-check"></i><b>1.3.3</b> Predictive Models</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="pipeline.html"><a href="pipeline.html"><i class="fa fa-check"></i><b>1.4</b> Data Analysis Pipeline</a></li>
<li class="chapter" data-level="1.5" data-path="trainingeval.html"><a href="trainingeval.html"><i class="fa fa-check"></i><b>1.5</b> Evaluating Predictive Models</a></li>
<li class="chapter" data-level="1.6" data-path="simple-classification-example.html"><a href="simple-classification-example.html"><i class="fa fa-check"></i><b>1.6</b> Simple Classification Example</a><ul>
<li class="chapter" data-level="1.6.1" data-path="simple-classification-example.html"><a href="simple-classification-example.html#k-fold-cross-validation-example"><i class="fa fa-check"></i><b>1.6.1</b> K-fold Cross-Validation Example</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="simple-regression-example.html"><a href="simple-regression-example.html"><i class="fa fa-check"></i><b>1.7</b> Simple Regression Example</a></li>
<li class="chapter" data-level="1.8" data-path="underfitting-and-overfitting.html"><a href="underfitting-and-overfitting.html"><i class="fa fa-check"></i><b>1.8</b> Underfitting and Overfitting</a></li>
<li class="chapter" data-level="1.9" data-path="bias-and-variance.html"><a href="bias-and-variance.html"><i class="fa fa-check"></i><b>1.9</b> Bias and Variance</a></li>
<li class="chapter" data-level="1.10" data-path="SummaryIntro.html"><a href="SummaryIntro.html"><i class="fa fa-check"></i><b>1.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>2</b> Predicting Behavior with Classification Models</a><ul>
<li class="chapter" data-level="2.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html"><i class="fa fa-check"></i><b>2.1</b> <em>k</em>-nearest Neighbors</a><ul>
<li class="chapter" data-level="2.1.1" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html#indoor-location-with-wi-fi-signals"><i class="fa fa-check"></i><b>2.1.1</b> Indoor Location with Wi-Fi Signals</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="performance-metrics.html"><a href="performance-metrics.html"><i class="fa fa-check"></i><b>2.2</b> Performance Metrics</a></li>
<li class="chapter" data-level="2.3" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>2.3</b> Decision Trees</a><ul>
<li class="chapter" data-level="2.3.1" data-path="decision-trees.html"><a href="decision-trees.html#activityRecognition"><i class="fa fa-check"></i><b>2.3.1</b> Activity Recognition with Smartphones</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="dynamic-time-warping.html"><a href="dynamic-time-warping.html"><i class="fa fa-check"></i><b>2.4</b> Dynamic Time Warping</a><ul>
<li class="chapter" data-level="2.4.1" data-path="dynamic-time-warping.html"><a href="dynamic-time-warping.html#sechandgestures"><i class="fa fa-check"></i><b>2.4.1</b> Hand Gesture Recognition</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="summaryClassification.html"><a href="summaryClassification.html"><i class="fa fa-check"></i><b>2.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>3</b> Predicting Behavior with Ensemble Learning</a><ul>
<li class="chapter" data-level="3.1" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>3.1</b> Bagging</a><ul>
<li class="chapter" data-level="3.1.1" data-path="bagging.html"><a href="bagging.html#activity-recognition-with-bagging"><i class="fa fa-check"></i><b>3.1.1</b> Activity recognition with Bagging</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="random-forest.html"><a href="random-forest.html"><i class="fa fa-check"></i><b>3.2</b> Random Forest</a></li>
<li class="chapter" data-level="3.3" data-path="stacked-generalization.html"><a href="stacked-generalization.html"><i class="fa fa-check"></i><b>3.3</b> Stacked Generalization</a></li>
<li class="chapter" data-level="3.4" data-path="multiviewhometasks.html"><a href="multiviewhometasks.html"><i class="fa fa-check"></i><b>3.4</b> Multi-view Stacking for Home Tasks Recognition</a></li>
<li class="chapter" data-level="3.5" data-path="SummaryEnsemble.html"><a href="SummaryEnsemble.html"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="edavis.html"><a href="edavis.html"><i class="fa fa-check"></i><b>4</b> Exploring and Visualizing Behavioral Data</a><ul>
<li class="chapter" data-level="4.1" data-path="talking-with-field-experts.html"><a href="talking-with-field-experts.html"><i class="fa fa-check"></i><b>4.1</b> Talking with field experts</a></li>
<li class="chapter" data-level="4.2" data-path="summary-statistics.html"><a href="summary-statistics.html"><i class="fa fa-check"></i><b>4.2</b> Summary Statistics</a></li>
<li class="chapter" data-level="4.3" data-path="class-distributions.html"><a href="class-distributions.html"><i class="fa fa-check"></i><b>4.3</b> Class Distributions</a></li>
<li class="chapter" data-level="4.4" data-path="user-class-sparsity-matrix.html"><a href="user-class-sparsity-matrix.html"><i class="fa fa-check"></i><b>4.4</b> User-Class Sparsity Matrix</a></li>
<li class="chapter" data-level="4.5" data-path="boxplots.html"><a href="boxplots.html"><i class="fa fa-check"></i><b>4.5</b> Boxplots</a></li>
<li class="chapter" data-level="4.6" data-path="correlation-plots.html"><a href="correlation-plots.html"><i class="fa fa-check"></i><b>4.6</b> Correlation Plots</a><ul>
<li class="chapter" data-level="4.6.1" data-path="correlation-plots.html"><a href="correlation-plots.html#interactive-correlation-plots"><i class="fa fa-check"></i><b>4.6.1</b> Interactive Correlation Plots</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="timeseries.html"><a href="timeseries.html"><i class="fa fa-check"></i><b>4.7</b> Timeseries</a><ul>
<li class="chapter" data-level="4.7.1" data-path="timeseries.html"><a href="timeseries.html#interactive-timeseries"><i class="fa fa-check"></i><b>4.7.1</b> Interactive Timeseries</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="multidimensional-scaling-mds.html"><a href="multidimensional-scaling-mds.html"><i class="fa fa-check"></i><b>4.8</b> Multidimensional Scaling (MDS)</a></li>
<li class="chapter" data-level="4.9" data-path="heatmaps.html"><a href="heatmaps.html"><i class="fa fa-check"></i><b>4.9</b> Heatmaps</a></li>
<li class="chapter" data-level="4.10" data-path="automated-eda.html"><a href="automated-eda.html"><i class="fa fa-check"></i><b>4.10</b> Automated EDA</a></li>
<li class="chapter" data-level="4.11" data-path="SummaryExploratory.html"><a href="SummaryExploratory.html"><i class="fa fa-check"></i><b>4.11</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="preprocessing.html"><a href="preprocessing.html"><i class="fa fa-check"></i><b>5</b> Preprocessing Behavioral Data</a><ul>
<li class="chapter" data-level="5.1" data-path="missing-values.html"><a href="missing-values.html"><i class="fa fa-check"></i><b>5.1</b> Missing Values</a><ul>
<li class="chapter" data-level="5.1.1" data-path="missing-values.html"><a href="missing-values.html#imputation"><i class="fa fa-check"></i><b>5.1.1</b> Imputation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>5.2</b> Smoothing</a></li>
<li class="chapter" data-level="5.3" data-path="normalization.html"><a href="normalization.html"><i class="fa fa-check"></i><b>5.3</b> Normalization</a></li>
<li class="chapter" data-level="5.4" data-path="imbalanced-classes.html"><a href="imbalanced-classes.html"><i class="fa fa-check"></i><b>5.4</b> Imbalanced Classes</a><ul>
<li class="chapter" data-level="5.4.1" data-path="imbalanced-classes.html"><a href="imbalanced-classes.html#random-oversampling"><i class="fa fa-check"></i><b>5.4.1</b> Random oversampling</a></li>
<li class="chapter" data-level="5.4.2" data-path="imbalanced-classes.html"><a href="imbalanced-classes.html#smote"><i class="fa fa-check"></i><b>5.4.2</b> SMOTE</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="infoinjection.html"><a href="infoinjection.html"><i class="fa fa-check"></i><b>5.5</b> Information Injection</a></li>
<li class="chapter" data-level="5.6" data-path="one-hot-encoding.html"><a href="one-hot-encoding.html"><i class="fa fa-check"></i><b>5.6</b> One-hot Encoding</a></li>
<li class="chapter" data-level="5.7" data-path="SummaryPreprocessing.html"><a href="SummaryPreprocessing.html"><i class="fa fa-check"></i><b>5.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="unsupervised.html"><a href="unsupervised.html"><i class="fa fa-check"></i><b>6</b> Discovering Behaviors with Unsupervised Learning</a><ul>
<li class="chapter" data-level="6.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>6.1</b> K-means clustering</a><ul>
<li class="chapter" data-level="6.1.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#studentresponses"><i class="fa fa-check"></i><b>6.1.1</b> Grouping Student Responses</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="the-silhouette-index.html"><a href="the-silhouette-index.html"><i class="fa fa-check"></i><b>6.2</b> The Silhouette Index</a></li>
<li class="chapter" data-level="6.3" data-path="associationrules.html"><a href="associationrules.html"><i class="fa fa-check"></i><b>6.3</b> Mining Association Rules</a><ul>
<li class="chapter" data-level="6.3.1" data-path="associationrules.html"><a href="associationrules.html#finding-rules-for-criminal-behavior"><i class="fa fa-check"></i><b>6.3.1</b> Finding Rules for Criminal Behavior</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="SummaryUnsupervised.html"><a href="SummaryUnsupervised.html"><i class="fa fa-check"></i><b>6.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="representations.html"><a href="representations.html"><i class="fa fa-check"></i><b>7</b> Encoding Behavioral Data</a><ul>
<li class="chapter" data-level="7.1" data-path="feature-vectors.html"><a href="feature-vectors.html"><i class="fa fa-check"></i><b>7.1</b> Feature Vectors</a></li>
<li class="chapter" data-level="7.2" data-path="sectimeseries.html"><a href="sectimeseries.html"><i class="fa fa-check"></i><b>7.2</b> Timeseries</a></li>
<li class="chapter" data-level="7.3" data-path="transactions.html"><a href="transactions.html"><i class="fa fa-check"></i><b>7.3</b> Transactions</a></li>
<li class="chapter" data-level="7.4" data-path="images.html"><a href="images.html"><i class="fa fa-check"></i><b>7.4</b> Images</a></li>
<li class="chapter" data-level="7.5" data-path="recurrence-plots.html"><a href="recurrence-plots.html"><i class="fa fa-check"></i><b>7.5</b> Recurrence Plots</a><ul>
<li class="chapter" data-level="7.5.1" data-path="recurrence-plots.html"><a href="recurrence-plots.html#computing-recurence-plots"><i class="fa fa-check"></i><b>7.5.1</b> Computing Recurence Plots</a></li>
<li class="chapter" data-level="7.5.2" data-path="recurrence-plots.html"><a href="recurrence-plots.html#recurrence-plots-of-hand-gestures"><i class="fa fa-check"></i><b>7.5.2</b> Recurrence Plots of Hand Gestures</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="bag-of-words.html"><a href="bag-of-words.html"><i class="fa fa-check"></i><b>7.6</b> Bag-of-Words</a><ul>
<li class="chapter" data-level="7.6.1" data-path="bag-of-words.html"><a href="bag-of-words.html#bow-for-complex-activities."><i class="fa fa-check"></i><b>7.6.1</b> BoW for Complex Activities.</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="graphs.html"><a href="graphs.html"><i class="fa fa-check"></i><b>7.7</b> Graphs</a><ul>
<li class="chapter" data-level="7.7.1" data-path="graphs.html"><a href="graphs.html#complex-activities-as-graphs"><i class="fa fa-check"></i><b>7.7.1</b> Complex activities as graphs</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="SummaryRepresentations.html"><a href="SummaryRepresentations.html"><i class="fa fa-check"></i><b>7.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deeplearning.html"><a href="deeplearning.html"><i class="fa fa-check"></i><b>8</b> Predicting Behavior with Deep Learning</a><ul>
<li class="chapter" data-level="8.1" data-path="ann.html"><a href="ann.html"><i class="fa fa-check"></i><b>8.1</b> Introduction to Artificial Neural Networks</a><ul>
<li class="chapter" data-level="8.1.1" data-path="ann.html"><a href="ann.html#sigmoid-and-relu-units"><i class="fa fa-check"></i><b>8.1.1</b> Sigmoid and ReLU units</a></li>
<li class="chapter" data-level="8.1.2" data-path="ann.html"><a href="ann.html#assembling-units-into-layers"><i class="fa fa-check"></i><b>8.1.2</b> Assembling Units into Layers</a></li>
<li class="chapter" data-level="8.1.3" data-path="ann.html"><a href="ann.html#deep-neural-networks"><i class="fa fa-check"></i><b>8.1.3</b> Deep Neural Networks</a></li>
<li class="chapter" data-level="8.1.4" data-path="ann.html"><a href="ann.html#learning-the-parameters"><i class="fa fa-check"></i><b>8.1.4</b> Learning the Parameters</a></li>
<li class="chapter" data-level="8.1.5" data-path="ann.html"><a href="ann.html#parameter-learning-example-in-r"><i class="fa fa-check"></i><b>8.1.5</b> Parameter Learning Example in R</a></li>
<li class="chapter" data-level="8.1.6" data-path="ann.html"><a href="ann.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>8.1.6</b> Stochastic Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="keras-and-tensorflow-with-r.html"><a href="keras-and-tensorflow-with-r.html"><i class="fa fa-check"></i><b>8.2</b> Keras and TensorFlow with R</a><ul>
<li class="chapter" data-level="8.2.1" data-path="keras-and-tensorflow-with-r.html"><a href="keras-and-tensorflow-with-r.html#keras-example"><i class="fa fa-check"></i><b>8.2.1</b> Keras Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="classification-with-neural-networks.html"><a href="classification-with-neural-networks.html"><i class="fa fa-check"></i><b>8.3</b> Classification with Neural Networks</a><ul>
<li class="chapter" data-level="8.3.1" data-path="classification-with-neural-networks.html"><a href="classification-with-neural-networks.html#classification-of-electromyography-signals"><i class="fa fa-check"></i><b>8.3.1</b> Classification of Electromyography Signals</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>8.4</b> Overfitting</a><ul>
<li class="chapter" data-level="8.4.1" data-path="overfitting.html"><a href="overfitting.html#early-stopping"><i class="fa fa-check"></i><b>8.4.1</b> Early Stopping</a></li>
<li class="chapter" data-level="8.4.2" data-path="overfitting.html"><a href="overfitting.html#dropout"><i class="fa fa-check"></i><b>8.4.2</b> Dropout</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="fine-tuning-a-neural-network.html"><a href="fine-tuning-a-neural-network.html"><i class="fa fa-check"></i><b>8.5</b> Fine-Tuning a Neural Network</a></li>
<li class="chapter" data-level="8.6" data-path="cnns.html"><a href="cnns.html"><i class="fa fa-check"></i><b>8.6</b> Convolutional Neural Networks</a><ul>
<li class="chapter" data-level="8.6.1" data-path="cnns.html"><a href="cnns.html#convolutions"><i class="fa fa-check"></i><b>8.6.1</b> Convolutions</a></li>
<li class="chapter" data-level="8.6.2" data-path="cnns.html"><a href="cnns.html#pooling-operations"><i class="fa fa-check"></i><b>8.6.2</b> Pooling Operations</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="cnns-with-keras.html"><a href="cnns-with-keras.html"><i class="fa fa-check"></i><b>8.7</b> CNNs with Keras</a><ul>
<li class="chapter" data-level="8.7.1" data-path="cnns-with-keras.html"><a href="cnns-with-keras.html#example-1"><i class="fa fa-check"></i><b>8.7.1</b> Example 1</a></li>
<li class="chapter" data-level="8.7.2" data-path="cnns-with-keras.html"><a href="cnns-with-keras.html#example-2"><i class="fa fa-check"></i><b>8.7.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="cnnSmile.html"><a href="cnnSmile.html"><i class="fa fa-check"></i><b>8.8</b> Smiles Detection with a CNN</a></li>
<li class="chapter" data-level="8.9" data-path="SummaryDeepLearning.html"><a href="SummaryDeepLearning.html"><i class="fa fa-check"></i><b>8.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multiuser.html"><a href="multiuser.html"><i class="fa fa-check"></i><b>9</b> Multi-User Validation</a><ul>
<li class="chapter" data-level="9.1" data-path="mixed-models.html"><a href="mixed-models.html"><i class="fa fa-check"></i><b>9.1</b> Mixed Models</a><ul>
<li class="chapter" data-level="9.1.1" data-path="mixed-models.html"><a href="mixed-models.html#skeleton-action-recognition-with-mixed-models"><i class="fa fa-check"></i><b>9.1.1</b> Skeleton Action Recognition with Mixed Models</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="user-independent-models.html"><a href="user-independent-models.html"><i class="fa fa-check"></i><b>9.2</b> User-Independent Models</a></li>
<li class="chapter" data-level="9.3" data-path="user-dependent-models.html"><a href="user-dependent-models.html"><i class="fa fa-check"></i><b>9.3</b> User-Dependent Models</a></li>
<li class="chapter" data-level="9.4" data-path="user-adaptive-models.html"><a href="user-adaptive-models.html"><i class="fa fa-check"></i><b>9.4</b> User-Adaptive Models</a><ul>
<li class="chapter" data-level="9.4.1" data-path="user-adaptive-models.html"><a href="user-adaptive-models.html#transfer-learning"><i class="fa fa-check"></i><b>9.4.1</b> Transfer Learning</a></li>
<li class="chapter" data-level="9.4.2" data-path="user-adaptive-models.html"><a href="user-adaptive-models.html#a-user-adaptive-model-for-activity-recognition"><i class="fa fa-check"></i><b>9.4.2</b> A User-Adaptive Model for Activity Recognition</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="SummaryMultiUser.html"><a href="SummaryMultiUser.html"><i class="fa fa-check"></i><b>9.5</b> Summary</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendixInstall.html"><a href="appendixInstall.html"><i class="fa fa-check"></i><b>A</b> Setup Your Environment</a><ul>
<li class="chapter" data-level="A.1" data-path="installing-the-datasets.html"><a href="installing-the-datasets.html"><i class="fa fa-check"></i><b>A.1</b> Installing the Datasets</a></li>
<li class="chapter" data-level="A.2" data-path="installing-the-examples-source-code.html"><a href="installing-the-examples-source-code.html"><i class="fa fa-check"></i><b>A.2</b> Installing the Examples Source Code</a></li>
<li class="chapter" data-level="A.3" data-path="installing-keras-and-tensorflow-.html"><a href="installing-keras-and-tensorflow-.html"><i class="fa fa-check"></i><b>A.3</b> Installing Keras and TensorFlow.</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendixDatasets.html"><a href="appendixDatasets.html"><i class="fa fa-check"></i><b>B</b> Datasets</a><ul>
<li class="chapter" data-level="B.1" data-path="complex-activities.html"><a href="complex-activities.html"><i class="fa fa-check"></i><b>B.1</b> COMPLEX ACTIVITIES</a></li>
<li class="chapter" data-level="B.2" data-path="depresjon.html"><a href="depresjon.html"><i class="fa fa-check"></i><b>B.2</b> DEPRESJON</a></li>
<li class="chapter" data-level="B.3" data-path="electromyography.html"><a href="electromyography.html"><i class="fa fa-check"></i><b>B.3</b> ELECTROMYOGRAPHY</a></li>
<li class="chapter" data-level="B.4" data-path="hand-gestures.html"><a href="hand-gestures.html"><i class="fa fa-check"></i><b>B.4</b> HAND GESTURES</a></li>
<li class="chapter" data-level="B.5" data-path="home-tasks.html"><a href="home-tasks.html"><i class="fa fa-check"></i><b>B.5</b> HOME TASKS</a></li>
<li class="chapter" data-level="B.6" data-path="homicide-reports.html"><a href="homicide-reports.html"><i class="fa fa-check"></i><b>B.6</b> HOMICIDE REPORTS</a></li>
<li class="chapter" data-level="B.7" data-path="indoor-location.html"><a href="indoor-location.html"><i class="fa fa-check"></i><b>B.7</b> INDOOR LOCATION</a></li>
<li class="chapter" data-level="B.8" data-path="sheep-goats.html"><a href="sheep-goats.html"><i class="fa fa-check"></i><b>B.8</b> SHEEP GOATS</a></li>
<li class="chapter" data-level="B.9" data-path="skeleton-actions.html"><a href="skeleton-actions.html"><i class="fa fa-check"></i><b>B.9</b> SKELETON ACTIONS</a></li>
<li class="chapter" data-level="B.10" data-path="smartphone-activities.html"><a href="smartphone-activities.html"><i class="fa fa-check"></i><b>B.10</b> SMARTPHONE ACTIVITIES</a></li>
<li class="chapter" data-level="B.11" data-path="smiles.html"><a href="smiles.html"><i class="fa fa-check"></i><b>B.11</b> SMILES</a></li>
<li class="chapter" data-level="B.12" data-path="students-mental-health.html"><a href="students-mental-health.html"><i class="fa fa-check"></i><b>B.12</b> STUDENTS’ MENTAL HEALTH</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="citing-this-book.html"><a href="citing-this-book.html"><i class="fa fa-check"></i>Citing this Book</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Behavior Analysis with Machine Learning and R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple-classification-example" class="section level2">
<h2><span class="header-section-number">1.6</span> Simple Classification Example</h2>

<div class="rmdfolder">
simple_model.R
</div>

<p>So far, a lot of terminology and concepts have been introduced. In this section, we will work through a practical example that will demonstrate how most of those concepts fit together. Here, you will build (from scratch) your first classification and regression models! Furthermore, you will learn how to evaluate their generalization performance.</p>
<p>Suppose you have a dataset that contains information about felines including their maximum speed in km/hr and their specific type. For the sake of the example, suppose that these two variables are the only ones that we can observe. As for the types, consider that there are two possibilities: <em>‘tiger’</em> and <em>‘leopard’</em>. Figure <a href="simple-classification-example.html#fig:felinesTable">1.9</a> shows the first <span class="math inline">\(10\)</span> instances (rows) of the dataset.</p>
<div class="figure" style="text-align: center"><span id="fig:felinesTable"></span>
<img src="images/felines_table.png" alt="First 10 instances of felines dataset." width="15%" />
<p class="caption">
Figure 1.9: First 10 instances of felines dataset.
</p>
</div>
<p>This table has <span class="math inline">\(2\)</span> variables: <em>speed</em> and <em>class</em>. The first one is a numeric variable. The second one is a categorical variable. In this case, it can take two possible values: <em>‘tiger’</em> or <em>‘leopard’</em>.</p>
<p>This dataset was synthetically created for illustration purposes but I promise that after this, we will mostly use real datasets.</p>
<p>The code to reproduce this example is contained in the <em>Introduction</em> folder in the script file <code>simple_model.R</code>. The script contains the code used to generate the dataset. The dataset is stored in a data frame named <code>dataset</code>. Let’s start by doing a simple exploratory analysis of the dataset. More detailed exploratory analysis methods will be presented in chapter <a href="edavis.html#edavis">4</a>. First, we can print the data frame dimensions with the <code>dim()</code> function.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="simple-classification-example.html#cb2-1"></a><span class="co"># Print number of rows and columns.</span></span>
<span id="cb2-2"><a href="simple-classification-example.html#cb2-2"></a><span class="kw">dim</span>(dataset)</span>
<span id="cb2-3"><a href="simple-classification-example.html#cb2-3"></a><span class="co">#&gt; [1] 100   2</span></span></code></pre></div>
<p>The output tells us that the data frame has <span class="math inline">\(100\)</span> rows and <span class="math inline">\(2\)</span> columns. Now we may be interested to know from those <span class="math inline">\(100\)</span> instances, how many correspond to <em>tigers</em>. We can use the <code>table()</code> function to get that information.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="simple-classification-example.html#cb3-1"></a><span class="co"># Count instances in each class.</span></span>
<span id="cb3-2"><a href="simple-classification-example.html#cb3-2"></a><span class="kw">table</span>(dataset<span class="op">$</span>class)</span>
<span id="cb3-3"><a href="simple-classification-example.html#cb3-3"></a><span class="co">#&gt; leopard   tiger </span></span>
<span id="cb3-4"><a href="simple-classification-example.html#cb3-4"></a><span class="co">#&gt;      50      50 </span></span></code></pre></div>
<p>Here we can see that <span class="math inline">\(50\)</span> instances are of type <em>‘leopard’</em> and also <span class="math inline">\(50\)</span> instances are of type <em>‘tiger’</em>. In fact, this is how the dataset was intentionally generated. The next thing we can do is compute some summary statistics for each column. R already provides a very convenient function for that purpose. Yes, it is the <code>summary()</code> function.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="simple-classification-example.html#cb4-1"></a><span class="co"># Compute some summary statistics.</span></span>
<span id="cb4-2"><a href="simple-classification-example.html#cb4-2"></a><span class="kw">summary</span>(dataset)</span>
<span id="cb4-3"><a href="simple-classification-example.html#cb4-3"></a><span class="co">#&gt;      speed           class   </span></span>
<span id="cb4-4"><a href="simple-classification-example.html#cb4-4"></a><span class="co">#&gt;  Min.   :42.96   leopard:50  </span></span>
<span id="cb4-5"><a href="simple-classification-example.html#cb4-5"></a><span class="co">#&gt;  1st Qu.:48.41   tiger  :50  </span></span>
<span id="cb4-6"><a href="simple-classification-example.html#cb4-6"></a><span class="co">#&gt;  Median :51.12               </span></span>
<span id="cb4-7"><a href="simple-classification-example.html#cb4-7"></a><span class="co">#&gt;  Mean   :51.53               </span></span>
<span id="cb4-8"><a href="simple-classification-example.html#cb4-8"></a><span class="co">#&gt;  3rd Qu.:53.99               </span></span>
<span id="cb4-9"><a href="simple-classification-example.html#cb4-9"></a><span class="co">#&gt;  Max.   :61.65               </span></span></code></pre></div>
<p>Since <em>speed</em> is a numeric variable, <code>summary()</code> computes some statistics like the mean, min, max, etc. The <em>class</em> variable is a factor, thus, it returns row counts instead. In R, categorical variables are usually encoded as factors. It is similar to a string but R treats factors in a special way. We can already appreciate that with the previous code snippet when the summary function returned class counts.</p>
<p>There are many other ways in which you can explore a dataset, but for now, let’s assume we already feel comfortable and that we have a good understanding of the data. Since this dataset is very simple, we won’t need to do any further data cleaning or preprocessing.</p>
<p>Now, imagine that you are asked to build a model that is able to predict the type of feline based on observed attributes. In this case, the only thing we can observe is the <em>speed</em>. Our task is to build a function that maps speed measurements to classes. That is, we want to be able to predict what type of feline it is based on how fast it runs. Based on the terminology presented in section <a href="terminology.html#terminology">1.3</a>, <em>speed</em> would be a <strong>feature</strong> variable and <em>class</em> would be the <strong>class</strong> variable.</p>
<p>Based on the types of machine learning presented in section <a href="taxonomy.html#taxonomy">1.2</a>, this one corresponds to a <strong>supervised learning</strong> problem because, for each instance, we have its respective label or class which we can use to train a model. And specifically, since we want to predict a category, this is a <strong>classification</strong> problem.</p>
<p>Before building our classification model, it would be worth plotting the data. Let’s plot the speeds for both, tigers and leopards.</p>
<div class="figure" style="text-align: center"><span id="fig:felineSpeeds"></span>
<img src="images/felineSpeeds.png" alt="Feline speeds with vertical dashed lines at the means." width="100%" />
<p class="caption">
Figure 1.10: Feline speeds with vertical dashed lines at the means.
</p>
</div>
<p>Here, I omitted the code for building the plot but it is included in the script. I have also added vertical dashed lines at the mean speeds for the two classes. From this plot, it seems that leopards are faster than tigers (with some exceptions). One thing we can note is that the data points are grouped around the mean values of their corresponding classes. That is, most tiger data points are closer to the mean speed for tigers and the same can be observed for leopards. Of course, there are some exceptions in which an instance is closer to the mean of the opposite class. This could be because some tigers may be as fast as leopards. Some leopards may also be slower than the average maybe because they are newborns or they are old. Unfortunately, we do not have more information so the best we can do is use our single feature <em>speed</em>. We can use these insights to come up with a simple model that discriminates between the two classes based on this single feature variable.</p>
<p>One thing we can do for any new instance we want to classify is to compute its distance to the ‘center’ of each class and predict the class that is the closest one. In this case, the center is the mean value. We can formally define our model as the set of <span class="math inline">\(n\)</span> centrality measures where <span class="math inline">\(n\)</span> is the number of classes (<span class="math inline">\(2\)</span> in our example).</p>
<p><span class="math display" id="eq:simpleModel">\[\begin{equation}
  M = \{\mu_1,\dots ,\mu_n\}
  \tag{1.2}
\end{equation}\]</span></p>
<p>Those centrality measures (the class means in this particular case) are called the <em>parameters</em> of the model. Training a model consists of finding those optimal parameters that will allow us to achieve the best performance on new instances that were not part of the training data. In most cases, we will need an <strong>algorithm</strong> to find those parameters. In our example, the algorithm consists of simply computing the mean speed for each class. That is, for each class, sum all the speeds belonging to that class and divide them by the number of data points in that class.</p>
<p>Once those parameters are found, we can start making predictions on new data points. This is called <em>inference</em> or <em>prediction</em> time. In this case, when a new data point arrives, we can predict its class by computing its distance to each of the <span class="math inline">\(n\)</span> centrality measures in <span class="math inline">\(M\)</span> and returning the class of the closest one.</p>
<p>The following function implements the training part of our model.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="simple-classification-example.html#cb5-1"></a><span class="co"># Define a simple classifier that learns</span></span>
<span id="cb5-2"><a href="simple-classification-example.html#cb5-2"></a><span class="co"># a centrality measure for each class.</span></span>
<span id="cb5-3"><a href="simple-classification-example.html#cb5-3"></a>simple.model.train &lt;-<span class="st"> </span><span class="cf">function</span>(data, <span class="dt">centrality=</span>mean){</span>
<span id="cb5-4"><a href="simple-classification-example.html#cb5-4"></a>  </span>
<span id="cb5-5"><a href="simple-classification-example.html#cb5-5"></a>  <span class="co"># Store unique classes.</span></span>
<span id="cb5-6"><a href="simple-classification-example.html#cb5-6"></a>  classes &lt;-<span class="st"> </span><span class="kw">unique</span>(data<span class="op">$</span>class)</span>
<span id="cb5-7"><a href="simple-classification-example.html#cb5-7"></a>  </span>
<span id="cb5-8"><a href="simple-classification-example.html#cb5-8"></a>  <span class="co"># Define an array to store the learned parameters.</span></span>
<span id="cb5-9"><a href="simple-classification-example.html#cb5-9"></a>  params &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="kw">length</span>(classes))</span>
<span id="cb5-10"><a href="simple-classification-example.html#cb5-10"></a>  </span>
<span id="cb5-11"><a href="simple-classification-example.html#cb5-11"></a>  <span class="co"># Make this a named array.</span></span>
<span id="cb5-12"><a href="simple-classification-example.html#cb5-12"></a>  <span class="kw">names</span>(params) &lt;-<span class="st"> </span>classes</span>
<span id="cb5-13"><a href="simple-classification-example.html#cb5-13"></a>  </span>
<span id="cb5-14"><a href="simple-classification-example.html#cb5-14"></a>  <span class="co"># Iterate through each class and compute its centrality measure.</span></span>
<span id="cb5-15"><a href="simple-classification-example.html#cb5-15"></a>  <span class="cf">for</span>(c <span class="cf">in</span> classes){</span>
<span id="cb5-16"><a href="simple-classification-example.html#cb5-16"></a>    </span>
<span id="cb5-17"><a href="simple-classification-example.html#cb5-17"></a>    <span class="co"># Filter instances by class.</span></span>
<span id="cb5-18"><a href="simple-classification-example.html#cb5-18"></a>    tmp &lt;-<span class="st"> </span>data[<span class="kw">which</span>(data<span class="op">$</span>class <span class="op">==</span><span class="st"> </span>c),]</span>
<span id="cb5-19"><a href="simple-classification-example.html#cb5-19"></a>    </span>
<span id="cb5-20"><a href="simple-classification-example.html#cb5-20"></a>    <span class="co"># Compute the centrality measure.</span></span>
<span id="cb5-21"><a href="simple-classification-example.html#cb5-21"></a>    centrality.measure &lt;-<span class="st"> </span><span class="kw">centrality</span>(tmp<span class="op">$</span>speed)</span>
<span id="cb5-22"><a href="simple-classification-example.html#cb5-22"></a>    </span>
<span id="cb5-23"><a href="simple-classification-example.html#cb5-23"></a>    <span class="co"># Store the centrality measure for this class.</span></span>
<span id="cb5-24"><a href="simple-classification-example.html#cb5-24"></a>    params[c] &lt;-<span class="st"> </span>centrality.measure</span>
<span id="cb5-25"><a href="simple-classification-example.html#cb5-25"></a>  }</span>
<span id="cb5-26"><a href="simple-classification-example.html#cb5-26"></a>  </span>
<span id="cb5-27"><a href="simple-classification-example.html#cb5-27"></a>  <span class="kw">return</span>(params)</span>
<span id="cb5-28"><a href="simple-classification-example.html#cb5-28"></a>  </span>
<span id="cb5-29"><a href="simple-classification-example.html#cb5-29"></a>}</span></code></pre></div>
<p>The first argument is the training data and the second argument is the centrality function we want to use (the mean, by default). This function iterates each class, computes the centrality measure based on the speed, and stores the results in a named array called <code>params</code> which is then returned at the end.</p>
<p>Most of the time, training a model involves passing the training data and any additional <strong>hyperparameters</strong> specific to each model. In this case, the centrality measure is a hyperparameter and here, we want to use the mean.</p>

<div class="rmdinfo">
The difference between <strong>parameters</strong> and <strong>hyperparameters</strong> is that the former ones are learned during training. The <strong>hyperparameters</strong> are settings specific to each model that we can define before the actual training starts.
</div>

<p>Now that we have a function that performs the training, we need another one that performs the actual inference or prediction on new data points. Let’s call this one <code>simple.classifier.predict()</code>. Its first argument is a data frame with the instances we want to get predictions for. The second argument is the named vector of parameters learned during training. This function will return an array with the predicted class for each instance in <code>newdata</code>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="simple-classification-example.html#cb6-1"></a><span class="co"># Define a function that predicts a class</span></span>
<span id="cb6-2"><a href="simple-classification-example.html#cb6-2"></a><span class="co"># based on the learned parameters.</span></span>
<span id="cb6-3"><a href="simple-classification-example.html#cb6-3"></a>simple.classifier.predict &lt;-<span class="st"> </span><span class="cf">function</span>(newdata, params){</span>
<span id="cb6-4"><a href="simple-classification-example.html#cb6-4"></a>  </span>
<span id="cb6-5"><a href="simple-classification-example.html#cb6-5"></a>  <span class="co"># Variable to store the predictions of</span></span>
<span id="cb6-6"><a href="simple-classification-example.html#cb6-6"></a>  <span class="co"># each instance in newdata.</span></span>
<span id="cb6-7"><a href="simple-classification-example.html#cb6-7"></a>  predictions &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb6-8"><a href="simple-classification-example.html#cb6-8"></a>  </span>
<span id="cb6-9"><a href="simple-classification-example.html#cb6-9"></a>  <span class="co"># Iterate instances in newdata</span></span>
<span id="cb6-10"><a href="simple-classification-example.html#cb6-10"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(newdata)){</span>
<span id="cb6-11"><a href="simple-classification-example.html#cb6-11"></a>    </span>
<span id="cb6-12"><a href="simple-classification-example.html#cb6-12"></a>    instance &lt;-<span class="st"> </span>newdata[i,]</span>
<span id="cb6-13"><a href="simple-classification-example.html#cb6-13"></a>    </span>
<span id="cb6-14"><a href="simple-classification-example.html#cb6-14"></a>    <span class="co"># Predict the name of the class which</span></span>
<span id="cb6-15"><a href="simple-classification-example.html#cb6-15"></a>    <span class="co"># centrality measure is closest.</span></span>
<span id="cb6-16"><a href="simple-classification-example.html#cb6-16"></a>    pred &lt;-<span class="st"> </span><span class="kw">names</span>(<span class="kw">which.min</span>(<span class="kw">abs</span>(instance<span class="op">$</span>speed <span class="op">-</span><span class="st"> </span>params)))</span>
<span id="cb6-17"><a href="simple-classification-example.html#cb6-17"></a>    </span>
<span id="cb6-18"><a href="simple-classification-example.html#cb6-18"></a>    predictions &lt;-<span class="st"> </span><span class="kw">c</span>(predictions, pred)</span>
<span id="cb6-19"><a href="simple-classification-example.html#cb6-19"></a>  }</span>
<span id="cb6-20"><a href="simple-classification-example.html#cb6-20"></a>  </span>
<span id="cb6-21"><a href="simple-classification-example.html#cb6-21"></a>  <span class="kw">return</span>(predictions)</span>
<span id="cb6-22"><a href="simple-classification-example.html#cb6-22"></a>}</span></code></pre></div>
<p>This function iterates through each row and computes the distance to each centrality measure and returns the name of the class that was the closest one. The distance computation is done with the following line of code:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="simple-classification-example.html#cb7-1"></a>pred &lt;-<span class="st"> </span><span class="kw">names</span>(<span class="kw">which.min</span>(<span class="kw">abs</span>(instance<span class="op">$</span>speed <span class="op">-</span><span class="st"> </span>params)))</span></code></pre></div>
<p>First, it computes the absolute difference between the speed and each centrality measure stored in <code>params</code> and then, it returns the name of the one that was the minimum. Now that we have defined the training and prediction procedures, we are ready to test our classifier!</p>
<p>In section <a href="trainingeval.html#trainingeval">1.5</a> two evaluation methods were presented. <strong>Hold-out and k-fold cross-validation</strong>. These methods allow you to estimate how your model will perform on new data. Let’s first start with <strong>hold-out validation</strong>.</p>
<p>First, we need to split the data into two independent sets. We will use <span class="math inline">\(70\%\)</span> of the data to train our classifier and the remaining <span class="math inline">\(30\%\)</span> to test it. The following code splits <code>dataset</code> into a <code>trainset</code> and <code>testset</code>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="simple-classification-example.html#cb8-1"></a><span class="co"># Percent to be used as training data.</span></span>
<span id="cb8-2"><a href="simple-classification-example.html#cb8-2"></a>pctTrain &lt;-<span class="st"> </span><span class="fl">0.7</span></span>
<span id="cb8-3"><a href="simple-classification-example.html#cb8-3"></a></span>
<span id="cb8-4"><a href="simple-classification-example.html#cb8-4"></a><span class="co"># Set seed for reproducibility.</span></span>
<span id="cb8-5"><a href="simple-classification-example.html#cb8-5"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb8-6"><a href="simple-classification-example.html#cb8-6"></a></span>
<span id="cb8-7"><a href="simple-classification-example.html#cb8-7"></a>idxs &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(dataset),</span>
<span id="cb8-8"><a href="simple-classification-example.html#cb8-8"></a>               <span class="dt">size =</span> <span class="kw">nrow</span>(dataset) <span class="op">*</span><span class="st"> </span>pctTrain,</span>
<span id="cb8-9"><a href="simple-classification-example.html#cb8-9"></a>               <span class="dt">replace =</span> <span class="ot">FALSE</span>)</span>
<span id="cb8-10"><a href="simple-classification-example.html#cb8-10"></a></span>
<span id="cb8-11"><a href="simple-classification-example.html#cb8-11"></a>trainset &lt;-<span class="st"> </span>dataset[idxs,]</span>
<span id="cb8-12"><a href="simple-classification-example.html#cb8-12"></a></span>
<span id="cb8-13"><a href="simple-classification-example.html#cb8-13"></a>testset &lt;-<span class="st"> </span>dataset[<span class="op">-</span>idxs,]</span></code></pre></div>
<p>The <code>sample()</code> function was used to select integer numbers at random from <span class="math inline">\(1\)</span> to <span class="math inline">\(n\)</span>, where <span class="math inline">\(n\)</span> is the total number of data points in <code>dataset</code>. These randomly selected data points are the ones that will go to the train set. Thus, with the <code>size</code> argument we tell the function to return <span class="math inline">\(70\)</span> numbers which correspond to <span class="math inline">\(70\%\)</span> of the total since <code>dataset</code> has <span class="math inline">\(100\)</span> instances.</p>

<div class="rmdcaution">
The last argument <code>replace</code> is set to <code>FALSE</code> because we do not want repeated numbers. This ensures that any instance only belongs to either the train or the test set. <strong>We don’t want an instance to be copied into both sets.</strong>
</div>

<p>Now it’s time to test our functions. We can train our model using the trainset by calling our previously defined function <code>simple.model.train()</code>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="simple-classification-example.html#cb9-1"></a><span class="co"># Train the model using the trainset.</span></span>
<span id="cb9-2"><a href="simple-classification-example.html#cb9-2"></a>params &lt;-<span class="st"> </span><span class="kw">simple.model.train</span>(trainset, mean)</span>
<span id="cb9-3"><a href="simple-classification-example.html#cb9-3"></a></span>
<span id="cb9-4"><a href="simple-classification-example.html#cb9-4"></a><span class="co"># Print the learned parameters.</span></span>
<span id="cb9-5"><a href="simple-classification-example.html#cb9-5"></a><span class="kw">print</span>(params)</span>
<span id="cb9-6"><a href="simple-classification-example.html#cb9-6"></a><span class="co">#&gt;    tiger  leopard </span></span>
<span id="cb9-7"><a href="simple-classification-example.html#cb9-7"></a><span class="co">#&gt; 48.88246 54.58369</span></span></code></pre></div>
<p>After training the model, we print the learned parameters. In this case, the mean for <em>tiger</em> is <span class="math inline">\(48.88\)</span> and for <em>leopard</em>, it is <span class="math inline">\(54.58\)</span>. With these parameters, we can start making predictions on our test set! We pass the test set and the newly learned parameters to our function <code>simple.classifier.predict()</code>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="simple-classification-example.html#cb10-1"></a><span class="co"># Predict classes on the test set.</span></span>
<span id="cb10-2"><a href="simple-classification-example.html#cb10-2"></a>test.predictions &lt;-<span class="st"> </span><span class="kw">simple.classifier.predict</span>(testset, params)</span>
<span id="cb10-3"><a href="simple-classification-example.html#cb10-3"></a></span>
<span id="cb10-4"><a href="simple-classification-example.html#cb10-4"></a><span class="co"># Display first predictions.</span></span>
<span id="cb10-5"><a href="simple-classification-example.html#cb10-5"></a><span class="kw">head</span>(test.predictions)</span>
<span id="cb10-6"><a href="simple-classification-example.html#cb10-6"></a><span class="co">#&gt; [1] &quot;tiger&quot;   &quot;tiger&quot;   &quot;leopard&quot; &quot;tiger&quot;   &quot;tiger&quot;   &quot;leopard&quot;</span></span></code></pre></div>
<p>Our predict function returns predictions for each instance in the test set. We can use the <code>head()</code> function to print the first predictions. The first two instances were classified as tigers, the third one as leopard, and so on.</p>
<p>But how good are those predictions? Since we know what the true classes are (also known as <strong>ground truth</strong>) in our test set, we can compute the performance. In this case, we will compute the accuracy which is the percentage of correct classifications. Note that we did not use the class information when making predictions, we only used the <em>speed</em>. We pretended that we didn’t have the true class. We will use the true class only to evaluate the model’s performance.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="simple-classification-example.html#cb11-1"></a><span class="co"># Compute test accuracy.</span></span>
<span id="cb11-2"><a href="simple-classification-example.html#cb11-2"></a><span class="kw">sum</span>(test.predictions <span class="op">==</span><span class="st"> </span><span class="kw">as.character</span>(testset<span class="op">$</span>class)) <span class="op">/</span></span>
<span id="cb11-3"><a href="simple-classification-example.html#cb11-3"></a><span class="st">  </span><span class="kw">nrow</span>(testset)</span>
<span id="cb11-4"><a href="simple-classification-example.html#cb11-4"></a><span class="co">#&gt; [1] 0.8333333</span></span></code></pre></div>
<p>We can compute the accuracy by counting how many predictions were equal to the true classes and divide them by the total number of points in the test set. In this case, the test accuracy was <span class="math inline">\(83.0\%\)</span>. <strong>Congratulations! you have trained and evaluated your first classifier.</strong></p>
<p>It is also a good idea to compute the performance on the same train set that was used to train the model.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="simple-classification-example.html#cb12-1"></a><span class="co"># Compute train accuracy.</span></span>
<span id="cb12-2"><a href="simple-classification-example.html#cb12-2"></a>train.predictions &lt;-<span class="st"> </span><span class="kw">simple.classifier.predict</span>(trainset, params)</span>
<span id="cb12-3"><a href="simple-classification-example.html#cb12-3"></a><span class="kw">sum</span>(train.predictions <span class="op">==</span><span class="st"> </span><span class="kw">as.character</span>(trainset<span class="op">$</span>class)) <span class="op">/</span></span>
<span id="cb12-4"><a href="simple-classification-example.html#cb12-4"></a><span class="st">  </span><span class="kw">nrow</span>(trainset)</span>
<span id="cb12-5"><a href="simple-classification-example.html#cb12-5"></a><span class="co">#&gt; [1] 0.8571429</span></span></code></pre></div>
<p>The <em>train accuracy</em> was <span class="math inline">\(85.7\%\)</span>. As expected, this was higher than the <em>test accuracy</em>. Typically, what you report is the performance on the <em>test set</em> but we can use the performance on the <em>train set</em> to look for signs of over/under-fitting which will be covered in the following sections.</p>
<div id="k-fold-cross-validation-example" class="section level3">
<h3><span class="header-section-number">1.6.1</span> K-fold Cross-Validation Example</h3>
<p>Now, let’s see how k-fold cross-validation can be implemented to test our classifier. I will choose a <span class="math inline">\(k=5\)</span>. This means that <span class="math inline">\(5\)</span> independent sets are going to be generated and <span class="math inline">\(5\)</span> iterations will be run.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="simple-classification-example.html#cb13-1"></a><span class="co"># Number of folds.</span></span>
<span id="cb13-2"><a href="simple-classification-example.html#cb13-2"></a>k &lt;-<span class="st"> </span><span class="dv">5</span></span>
<span id="cb13-3"><a href="simple-classification-example.html#cb13-3"></a></span>
<span id="cb13-4"><a href="simple-classification-example.html#cb13-4"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb13-5"><a href="simple-classification-example.html#cb13-5"></a></span>
<span id="cb13-6"><a href="simple-classification-example.html#cb13-6"></a><span class="co"># Generate random folds.</span></span>
<span id="cb13-7"><a href="simple-classification-example.html#cb13-7"></a>folds &lt;-<span class="st"> </span><span class="kw">sample</span>(k, <span class="dt">size =</span> <span class="kw">nrow</span>(dataset), <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span>
<span id="cb13-8"><a href="simple-classification-example.html#cb13-8"></a></span>
<span id="cb13-9"><a href="simple-classification-example.html#cb13-9"></a><span class="co"># Print how many instances ended up in each fold.</span></span>
<span id="cb13-10"><a href="simple-classification-example.html#cb13-10"></a><span class="kw">table</span>(folds)</span>
<span id="cb13-11"><a href="simple-classification-example.html#cb13-11"></a><span class="co">#&gt; folds</span></span>
<span id="cb13-12"><a href="simple-classification-example.html#cb13-12"></a><span class="co">#&gt;  1  2  3  4  5 </span></span>
<span id="cb13-13"><a href="simple-classification-example.html#cb13-13"></a><span class="co">#&gt; 21 20 23 17 19 </span></span></code></pre></div>
<p>Again, we can use the <code>sample()</code> function. This time we want to select random integers between <span class="math inline">\(1\)</span> and <span class="math inline">\(k\)</span>. This time, the total number of integers will be equal to the total number of instances <span class="math inline">\(n\)</span> in the entire dataset. Note that this time we set <code>replace = TRUE</code> since <span class="math inline">\(k &lt; n\)</span> we need to pick repeated numbers. Each number will represent the fold to which each instance belongs to. As before, we need to make sure that each instance belongs only to one of the sets. Here, we are guaranteeing that by assigning each instance a single fold number. We can use the <code>table()</code> function to print how many instances ended up in each fold. Here, we see that the folds will contain between <span class="math inline">\(17\)</span> and <span class="math inline">\(23\)</span> instances.</p>
<p>K-fold cross-validation consists of iterating <span class="math inline">\(k\)</span> times. In each iteration, we select one of the folds to function as the test set and the remaining folds are used as the train set. We can then, train a model with the train set and evaluate it with the test set. In the end, we report the average accuracy across folds.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="simple-classification-example.html#cb14-1"></a><span class="co"># Variable to store accuracies on each fold.</span></span>
<span id="cb14-2"><a href="simple-classification-example.html#cb14-2"></a>test.accuracies &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb14-3"><a href="simple-classification-example.html#cb14-3"></a>train.accuracies &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb14-4"><a href="simple-classification-example.html#cb14-4"></a></span>
<span id="cb14-5"><a href="simple-classification-example.html#cb14-5"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k){</span>
<span id="cb14-6"><a href="simple-classification-example.html#cb14-6"></a>  testset &lt;-<span class="st"> </span>dataset[<span class="kw">which</span>(folds <span class="op">==</span><span class="st"> </span>i),]</span>
<span id="cb14-7"><a href="simple-classification-example.html#cb14-7"></a>  trainset &lt;-<span class="st"> </span>dataset[<span class="kw">which</span>(folds <span class="op">!=</span><span class="st"> </span>i),]</span>
<span id="cb14-8"><a href="simple-classification-example.html#cb14-8"></a>  </span>
<span id="cb14-9"><a href="simple-classification-example.html#cb14-9"></a>  params &lt;-<span class="st"> </span><span class="kw">simple.model.train</span>(trainset, mean)</span>
<span id="cb14-10"><a href="simple-classification-example.html#cb14-10"></a>  test.predictions &lt;-<span class="st"> </span><span class="kw">simple.classifier.predict</span>(testset, params)</span>
<span id="cb14-11"><a href="simple-classification-example.html#cb14-11"></a>  train.predictions &lt;-<span class="st"> </span><span class="kw">simple.classifier.predict</span>(trainset, params)</span>
<span id="cb14-12"><a href="simple-classification-example.html#cb14-12"></a>  </span>
<span id="cb14-13"><a href="simple-classification-example.html#cb14-13"></a>  <span class="co"># Accuracy on test set.</span></span>
<span id="cb14-14"><a href="simple-classification-example.html#cb14-14"></a>  acc &lt;-<span class="st"> </span><span class="kw">sum</span>(test.predictions <span class="op">==</span><span class="st"> </span></span>
<span id="cb14-15"><a href="simple-classification-example.html#cb14-15"></a><span class="st">               </span><span class="kw">as.character</span>(testset<span class="op">$</span>class)) <span class="op">/</span></span>
<span id="cb14-16"><a href="simple-classification-example.html#cb14-16"></a><span class="st">    </span><span class="kw">nrow</span>(testset)</span>
<span id="cb14-17"><a href="simple-classification-example.html#cb14-17"></a>  </span>
<span id="cb14-18"><a href="simple-classification-example.html#cb14-18"></a>  test.accuracies &lt;-<span class="st"> </span><span class="kw">c</span>(test.accuracies, acc)</span>
<span id="cb14-19"><a href="simple-classification-example.html#cb14-19"></a>  </span>
<span id="cb14-20"><a href="simple-classification-example.html#cb14-20"></a>  <span class="co"># Accuracy on train set.</span></span>
<span id="cb14-21"><a href="simple-classification-example.html#cb14-21"></a>  acc &lt;-<span class="st"> </span><span class="kw">sum</span>(train.predictions <span class="op">==</span><span class="st"> </span></span>
<span id="cb14-22"><a href="simple-classification-example.html#cb14-22"></a><span class="st">               </span><span class="kw">as.character</span>(trainset<span class="op">$</span>class)) <span class="op">/</span></span>
<span id="cb14-23"><a href="simple-classification-example.html#cb14-23"></a><span class="st">    </span><span class="kw">nrow</span>(trainset)</span>
<span id="cb14-24"><a href="simple-classification-example.html#cb14-24"></a>  </span>
<span id="cb14-25"><a href="simple-classification-example.html#cb14-25"></a>  train.accuracies &lt;-<span class="st"> </span><span class="kw">c</span>(train.accuracies, acc)</span>
<span id="cb14-26"><a href="simple-classification-example.html#cb14-26"></a>}</span>
<span id="cb14-27"><a href="simple-classification-example.html#cb14-27"></a></span>
<span id="cb14-28"><a href="simple-classification-example.html#cb14-28"></a><span class="co"># Print mean accuracy across folds on the test set.</span></span>
<span id="cb14-29"><a href="simple-classification-example.html#cb14-29"></a><span class="kw">mean</span>(test.accuracies)</span>
<span id="cb14-30"><a href="simple-classification-example.html#cb14-30"></a><span class="co">#&gt; [1] 0.829823</span></span>
<span id="cb14-31"><a href="simple-classification-example.html#cb14-31"></a></span>
<span id="cb14-32"><a href="simple-classification-example.html#cb14-32"></a><span class="co"># Print mean accuracy across folds on the train set.</span></span>
<span id="cb14-33"><a href="simple-classification-example.html#cb14-33"></a><span class="kw">mean</span>(train.accuracies)</span>
<span id="cb14-34"><a href="simple-classification-example.html#cb14-34"></a><span class="co">#&gt; [1] 0.8422414</span></span></code></pre></div>
<p>The test mean accuracy across the <span class="math inline">\(5\)</span> folds was <span class="math inline">\(\approx 83\%\)</span> which is very similar to the accuracy estimated by hold-out validation.</p>

<div class="rmdgoodpractice">
<p>Note that in section <a href="trainingeval.html#trainingeval">1.5</a> a <strong>validation set</strong> was also mentioned. This one is useful when you want to fine-tune a model and/or try different preprocessing methods on your data. In case you are using hold-out validation, you may want to split your data into three sets: train/validation/test sets. So you train your model using the train set and estimate its performance using the validation set. Then you can fine-tune your model. For example, here, instead of the mean as centrality measure, you can try to use the median and measure the performance again with the validation set. When you are pleased with your settings, you estimate the final performance of the model with the test set <em>only once</em>.</p>
In the case of k-fold cross-validation, you can set aside a test set at the beginning. Then you use the remaining data to perform cross-validation and fine-tune your model. Within each iteration, you test the performance with the validation data. Once you are sure you are not going to do any parameter tunning, you can train a model with the train and validation sets and test the generalization performance using the test set.
</div>


<div class="rmdinfo">
<p>One of the benefits of machine learning is that it allows us to find patterns based on data freeing us from having to program hard-coded rules. This means a more scalable and flexible code. If for some reason, now, instead of <span class="math inline">\(2\)</span> classes we needed to add another class, for example, a <em>jaguar</em> the only thing we need to do is update our database and retrain our model. We don’t need to modify the internals of the algorithms. They will update themselves based on the data.</p>
We can try this by adding a third class to the dataset. The <code>simple_model.R</code> script shows how to add a new class ‘jaguar’ to the dataset. It then trains the model as usual and performs predictions.
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="trainingeval.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="simple-regression-example.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
