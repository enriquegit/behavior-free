<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Predicting Behavior with Classification Models | Behavior Analysis with Machine Learning and R</title>
  <meta name="description" content="Chapter 2 Predicting Behavior with Classification Models | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Predicting Behavior with Classification Models | Behavior Analysis with Machine Learning and R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="Chapter 2 Predicting Behavior with Classification Models | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Predicting Behavior with Classification Models | Behavior Analysis with Machine Learning and R" />
  
  <meta name="twitter:description" content="Chapter 2 Predicting Behavior with Classification Models | Behavior Analysis with Machine Learning and R teaches you how to train machine learning models in the R programming language to make sense of behavioral data collected with sensors and stored in electronic records. This book introduces machine learning concepts and algorithms applied to a diverse set of behavior analysis problems by focusing on practical aspects. Some of the topics include how to: Build supervised models to predict indoor locations based on Wi-Fi signals, recognize physical activities from smartphone sensors, use unsupervised learning to discover criminal behavioral patterns, build deep learning models to analyze electromyography signals, CNNs to detect smiles in images and much more." />
  <meta name="twitter:image" content="images/cover.png" />

<meta name="author" content="Enrique Garcia Ceja" />


<meta name="date" content="2020-10-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="ensemble.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/d3-4.9.0/d3.min.js"></script>
<script src="libs/d3-tip-0.7.1/index-min.js"></script>
<link href="libs/d3panels-1.4.9/d3panels.min.css" rel="stylesheet" />
<script src="libs/d3panels-1.4.9/d3panels.min.js"></script>
<script src="libs/qtlcharts_iplotCorr-0.11.6/iplotCorr.js"></script>
<script src="libs/qtlcharts_iplotCorr-0.11.6/iplotCorr_noscat.js"></script>
<script src="libs/iplotCorr-binding-0.11.6/iplotCorr.js"></script>
<link href="libs/dygraphs-1.1.1/dygraph.css" rel="stylesheet" />
<script src="libs/dygraphs-1.1.1/dygraph-combined.js"></script>
<script src="libs/dygraphs-1.1.1/shapes.js"></script>
<script src="libs/moment-2.8.4/moment.js"></script>
<script src="libs/moment-timezone-0.2.5/moment-timezone-with-data.js"></script>
<script src="libs/moment-fquarter-1.0.0/moment-fquarter.min.js"></script>
<script src="libs/dygraphs-binding-1.1.1.6/dygraphs.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178679335-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178679335-1', { 'anonymize_ip': true });
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="#">Behavior Analysis with Machine Learning and R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#supplemental-material"><i class="fa fa-check"></i>Supplemental Material</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#conventions"><i class="fa fa-check"></i>Conventions</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#taxonomy"><i class="fa fa-check"></i><b>1.2</b> Types of Machine Learning</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#terminology"><i class="fa fa-check"></i><b>1.3</b> Terminology</a><ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#tables"><i class="fa fa-check"></i><b>1.3.1</b> Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#variable-types"><i class="fa fa-check"></i><b>1.3.2</b> Variable Types</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro.html"><a href="intro.html#predictive-models"><i class="fa fa-check"></i><b>1.3.3</b> Predictive Models</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#pipeline"><i class="fa fa-check"></i><b>1.4</b> Data Analysis Pipeline</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#trainingeval"><i class="fa fa-check"></i><b>1.5</b> Evaluating Predictive Models</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#simple-classification-example"><i class="fa fa-check"></i><b>1.6</b> Simple Classification Example</a><ul>
<li class="chapter" data-level="1.6.1" data-path="intro.html"><a href="intro.html#k-fold-cross-validation-example"><i class="fa fa-check"></i><b>1.6.1</b> K-fold Cross-Validation Example</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#simple-regression-example"><i class="fa fa-check"></i><b>1.7</b> Simple Regression Example</a></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#underfitting-and-overfitting"><i class="fa fa-check"></i><b>1.8</b> Underfitting and Overfitting</a></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a href="intro.html#bias-and-variance"><i class="fa fa-check"></i><b>1.9</b> Bias and Variance</a></li>
<li class="chapter" data-level="1.10" data-path="intro.html"><a href="intro.html#SummaryIntro"><i class="fa fa-check"></i><b>1.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>2</b> Predicting Behavior with Classification Models</a><ul>
<li class="chapter" data-level="2.1" data-path="classification.html"><a href="classification.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>2.1</b> <em>k</em>-nearest Neighbors</a><ul>
<li class="chapter" data-level="2.1.1" data-path="classification.html"><a href="classification.html#indoor-location-with-wi-fi-signals"><i class="fa fa-check"></i><b>2.1.1</b> Indoor Location with Wi-Fi Signals</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="classification.html"><a href="classification.html#performance-metrics"><i class="fa fa-check"></i><b>2.2</b> Performance Metrics</a></li>
<li class="chapter" data-level="2.3" data-path="classification.html"><a href="classification.html#decision-trees"><i class="fa fa-check"></i><b>2.3</b> Decision Trees</a><ul>
<li class="chapter" data-level="2.3.1" data-path="classification.html"><a href="classification.html#activityRecognition"><i class="fa fa-check"></i><b>2.3.1</b> Activity Recognition with Smartphones</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="classification.html"><a href="classification.html#naive-bayes"><i class="fa fa-check"></i><b>2.4</b> Naive Bayes</a><ul>
<li class="chapter" data-level="2.4.1" data-path="classification.html"><a href="classification.html#activity-recognition-with-naive-bayes"><i class="fa fa-check"></i><b>2.4.1</b> Activity Recognition with Naive Bayes</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="classification.html"><a href="classification.html#dynamic-time-warping"><i class="fa fa-check"></i><b>2.5</b> Dynamic Time Warping</a><ul>
<li class="chapter" data-level="2.5.1" data-path="classification.html"><a href="classification.html#sechandgestures"><i class="fa fa-check"></i><b>2.5.1</b> Hand Gesture Recognition</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="classification.html"><a href="classification.html#summaryClassification"><i class="fa fa-check"></i><b>2.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>3</b> Predicting Behavior with Ensemble Learning</a><ul>
<li class="chapter" data-level="3.1" data-path="ensemble.html"><a href="ensemble.html#bagging"><i class="fa fa-check"></i><b>3.1</b> Bagging</a><ul>
<li class="chapter" data-level="3.1.1" data-path="ensemble.html"><a href="ensemble.html#activity-recognition-with-bagging"><i class="fa fa-check"></i><b>3.1.1</b> Activity recognition with Bagging</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ensemble.html"><a href="ensemble.html#random-forest"><i class="fa fa-check"></i><b>3.2</b> Random Forest</a></li>
<li class="chapter" data-level="3.3" data-path="ensemble.html"><a href="ensemble.html#stacked-generalization"><i class="fa fa-check"></i><b>3.3</b> Stacked Generalization</a></li>
<li class="chapter" data-level="3.4" data-path="ensemble.html"><a href="ensemble.html#multiviewhometasks"><i class="fa fa-check"></i><b>3.4</b> Multi-view Stacking for Home Tasks Recognition</a></li>
<li class="chapter" data-level="3.5" data-path="ensemble.html"><a href="ensemble.html#SummaryEnsemble"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="edavis.html"><a href="edavis.html"><i class="fa fa-check"></i><b>4</b> Exploring and Visualizing Behavioral Data</a><ul>
<li class="chapter" data-level="4.1" data-path="edavis.html"><a href="edavis.html#talking-with-field-experts"><i class="fa fa-check"></i><b>4.1</b> Talking with Field Experts</a></li>
<li class="chapter" data-level="4.2" data-path="edavis.html"><a href="edavis.html#summary-statistics"><i class="fa fa-check"></i><b>4.2</b> Summary Statistics</a></li>
<li class="chapter" data-level="4.3" data-path="edavis.html"><a href="edavis.html#class-distributions"><i class="fa fa-check"></i><b>4.3</b> Class Distributions</a></li>
<li class="chapter" data-level="4.4" data-path="edavis.html"><a href="edavis.html#user-class-sparsity-matrix"><i class="fa fa-check"></i><b>4.4</b> User-Class Sparsity Matrix</a></li>
<li class="chapter" data-level="4.5" data-path="edavis.html"><a href="edavis.html#boxplots"><i class="fa fa-check"></i><b>4.5</b> Boxplots</a></li>
<li class="chapter" data-level="4.6" data-path="edavis.html"><a href="edavis.html#correlation-plots"><i class="fa fa-check"></i><b>4.6</b> Correlation Plots</a><ul>
<li class="chapter" data-level="4.6.1" data-path="edavis.html"><a href="edavis.html#interactive-correlation-plots"><i class="fa fa-check"></i><b>4.6.1</b> Interactive Correlation Plots</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="edavis.html"><a href="edavis.html#timeseries"><i class="fa fa-check"></i><b>4.7</b> Timeseries</a><ul>
<li class="chapter" data-level="4.7.1" data-path="edavis.html"><a href="edavis.html#interactive-timeseries"><i class="fa fa-check"></i><b>4.7.1</b> Interactive Timeseries</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="edavis.html"><a href="edavis.html#multidimensional-scaling-mds"><i class="fa fa-check"></i><b>4.8</b> Multidimensional Scaling (MDS)</a></li>
<li class="chapter" data-level="4.9" data-path="edavis.html"><a href="edavis.html#heatmaps"><i class="fa fa-check"></i><b>4.9</b> Heatmaps</a></li>
<li class="chapter" data-level="4.10" data-path="edavis.html"><a href="edavis.html#automated-eda"><i class="fa fa-check"></i><b>4.10</b> Automated EDA</a></li>
<li class="chapter" data-level="4.11" data-path="edavis.html"><a href="edavis.html#SummaryExploratory"><i class="fa fa-check"></i><b>4.11</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="preprocessing.html"><a href="preprocessing.html"><i class="fa fa-check"></i><b>5</b> Preprocessing Behavioral Data</a><ul>
<li class="chapter" data-level="5.1" data-path="preprocessing.html"><a href="preprocessing.html#missing-values"><i class="fa fa-check"></i><b>5.1</b> Missing Values</a><ul>
<li class="chapter" data-level="5.1.1" data-path="preprocessing.html"><a href="preprocessing.html#imputation"><i class="fa fa-check"></i><b>5.1.1</b> Imputation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="preprocessing.html"><a href="preprocessing.html#smoothing"><i class="fa fa-check"></i><b>5.2</b> Smoothing</a></li>
<li class="chapter" data-level="5.3" data-path="preprocessing.html"><a href="preprocessing.html#normalization"><i class="fa fa-check"></i><b>5.3</b> Normalization</a></li>
<li class="chapter" data-level="5.4" data-path="preprocessing.html"><a href="preprocessing.html#imbalanced-classes"><i class="fa fa-check"></i><b>5.4</b> Imbalanced Classes</a><ul>
<li class="chapter" data-level="5.4.1" data-path="preprocessing.html"><a href="preprocessing.html#random-oversampling"><i class="fa fa-check"></i><b>5.4.1</b> Random Oversampling</a></li>
<li class="chapter" data-level="5.4.2" data-path="preprocessing.html"><a href="preprocessing.html#smote"><i class="fa fa-check"></i><b>5.4.2</b> SMOTE</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="preprocessing.html"><a href="preprocessing.html#infoinjection"><i class="fa fa-check"></i><b>5.5</b> Information Injection</a></li>
<li class="chapter" data-level="5.6" data-path="preprocessing.html"><a href="preprocessing.html#one-hot-encoding"><i class="fa fa-check"></i><b>5.6</b> One-hot Encoding</a></li>
<li class="chapter" data-level="5.7" data-path="preprocessing.html"><a href="preprocessing.html#SummaryPreprocessing"><i class="fa fa-check"></i><b>5.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="unsupervised.html"><a href="unsupervised.html"><i class="fa fa-check"></i><b>6</b> Discovering Behaviors with Unsupervised Learning</a><ul>
<li class="chapter" data-level="6.1" data-path="unsupervised.html"><a href="unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>6.1</b> K-means clustering</a><ul>
<li class="chapter" data-level="6.1.1" data-path="unsupervised.html"><a href="unsupervised.html#studentresponses"><i class="fa fa-check"></i><b>6.1.1</b> Grouping Student Responses</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="unsupervised.html"><a href="unsupervised.html#the-silhouette-index"><i class="fa fa-check"></i><b>6.2</b> The Silhouette Index</a></li>
<li class="chapter" data-level="6.3" data-path="unsupervised.html"><a href="unsupervised.html#associationrules"><i class="fa fa-check"></i><b>6.3</b> Mining Association Rules</a><ul>
<li class="chapter" data-level="6.3.1" data-path="unsupervised.html"><a href="unsupervised.html#finding-rules-for-criminal-behavior"><i class="fa fa-check"></i><b>6.3.1</b> Finding Rules for Criminal Behavior</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="unsupervised.html"><a href="unsupervised.html#SummaryUnsupervised"><i class="fa fa-check"></i><b>6.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="representations.html"><a href="representations.html"><i class="fa fa-check"></i><b>7</b> Encoding Behavioral Data</a><ul>
<li class="chapter" data-level="7.1" data-path="representations.html"><a href="representations.html#feature-vectors"><i class="fa fa-check"></i><b>7.1</b> Feature Vectors</a></li>
<li class="chapter" data-level="7.2" data-path="representations.html"><a href="representations.html#sectimeseries"><i class="fa fa-check"></i><b>7.2</b> Timeseries</a></li>
<li class="chapter" data-level="7.3" data-path="representations.html"><a href="representations.html#transactions"><i class="fa fa-check"></i><b>7.3</b> Transactions</a></li>
<li class="chapter" data-level="7.4" data-path="representations.html"><a href="representations.html#images"><i class="fa fa-check"></i><b>7.4</b> Images</a></li>
<li class="chapter" data-level="7.5" data-path="representations.html"><a href="representations.html#recurrence-plots"><i class="fa fa-check"></i><b>7.5</b> Recurrence Plots</a><ul>
<li class="chapter" data-level="7.5.1" data-path="representations.html"><a href="representations.html#computing-recurence-plots"><i class="fa fa-check"></i><b>7.5.1</b> Computing Recurence Plots</a></li>
<li class="chapter" data-level="7.5.2" data-path="representations.html"><a href="representations.html#recurrence-plots-of-hand-gestures"><i class="fa fa-check"></i><b>7.5.2</b> Recurrence Plots of Hand Gestures</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="representations.html"><a href="representations.html#bag-of-words"><i class="fa fa-check"></i><b>7.6</b> Bag-of-Words</a><ul>
<li class="chapter" data-level="7.6.1" data-path="representations.html"><a href="representations.html#bow-for-complex-activities."><i class="fa fa-check"></i><b>7.6.1</b> BoW for Complex Activities.</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="representations.html"><a href="representations.html#graphs"><i class="fa fa-check"></i><b>7.7</b> Graphs</a><ul>
<li class="chapter" data-level="7.7.1" data-path="representations.html"><a href="representations.html#complex-activities-as-graphs"><i class="fa fa-check"></i><b>7.7.1</b> Complex Activities as Graphs</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="representations.html"><a href="representations.html#SummaryRepresentations"><i class="fa fa-check"></i><b>7.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deeplearning.html"><a href="deeplearning.html"><i class="fa fa-check"></i><b>8</b> Predicting Behavior with Deep Learning</a><ul>
<li class="chapter" data-level="8.1" data-path="deeplearning.html"><a href="deeplearning.html#ann"><i class="fa fa-check"></i><b>8.1</b> Introduction to Artificial Neural Networks</a><ul>
<li class="chapter" data-level="8.1.1" data-path="deeplearning.html"><a href="deeplearning.html#sigmoid-and-relu-units"><i class="fa fa-check"></i><b>8.1.1</b> Sigmoid and ReLU Units</a></li>
<li class="chapter" data-level="8.1.2" data-path="deeplearning.html"><a href="deeplearning.html#assembling-units-into-layers"><i class="fa fa-check"></i><b>8.1.2</b> Assembling Units into Layers</a></li>
<li class="chapter" data-level="8.1.3" data-path="deeplearning.html"><a href="deeplearning.html#deep-neural-networks"><i class="fa fa-check"></i><b>8.1.3</b> Deep Neural Networks</a></li>
<li class="chapter" data-level="8.1.4" data-path="deeplearning.html"><a href="deeplearning.html#learning-the-parameters"><i class="fa fa-check"></i><b>8.1.4</b> Learning the Parameters</a></li>
<li class="chapter" data-level="8.1.5" data-path="deeplearning.html"><a href="deeplearning.html#parameter-learning-example-in-r"><i class="fa fa-check"></i><b>8.1.5</b> Parameter Learning Example in R</a></li>
<li class="chapter" data-level="8.1.6" data-path="deeplearning.html"><a href="deeplearning.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>8.1.6</b> Stochastic Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="deeplearning.html"><a href="deeplearning.html#keras-and-tensorflow-with-r"><i class="fa fa-check"></i><b>8.2</b> Keras and TensorFlow with R</a><ul>
<li class="chapter" data-level="8.2.1" data-path="deeplearning.html"><a href="deeplearning.html#keras-example"><i class="fa fa-check"></i><b>8.2.1</b> Keras Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="deeplearning.html"><a href="deeplearning.html#classification-with-neural-networks"><i class="fa fa-check"></i><b>8.3</b> Classification with Neural Networks</a><ul>
<li class="chapter" data-level="8.3.1" data-path="deeplearning.html"><a href="deeplearning.html#classification-of-electromyography-signals"><i class="fa fa-check"></i><b>8.3.1</b> Classification of Electromyography Signals</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="deeplearning.html"><a href="deeplearning.html#overfitting"><i class="fa fa-check"></i><b>8.4</b> Overfitting</a><ul>
<li class="chapter" data-level="8.4.1" data-path="deeplearning.html"><a href="deeplearning.html#early-stopping"><i class="fa fa-check"></i><b>8.4.1</b> Early Stopping</a></li>
<li class="chapter" data-level="8.4.2" data-path="deeplearning.html"><a href="deeplearning.html#dropout"><i class="fa fa-check"></i><b>8.4.2</b> Dropout</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="deeplearning.html"><a href="deeplearning.html#fine-tuning-a-neural-network"><i class="fa fa-check"></i><b>8.5</b> Fine-Tuning a Neural Network</a></li>
<li class="chapter" data-level="8.6" data-path="deeplearning.html"><a href="deeplearning.html#cnns"><i class="fa fa-check"></i><b>8.6</b> Convolutional Neural Networks</a><ul>
<li class="chapter" data-level="8.6.1" data-path="deeplearning.html"><a href="deeplearning.html#convolutions"><i class="fa fa-check"></i><b>8.6.1</b> Convolutions</a></li>
<li class="chapter" data-level="8.6.2" data-path="deeplearning.html"><a href="deeplearning.html#pooling-operations"><i class="fa fa-check"></i><b>8.6.2</b> Pooling Operations</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="deeplearning.html"><a href="deeplearning.html#cnns-with-keras"><i class="fa fa-check"></i><b>8.7</b> CNNs with Keras</a><ul>
<li class="chapter" data-level="8.7.1" data-path="deeplearning.html"><a href="deeplearning.html#example-1"><i class="fa fa-check"></i><b>8.7.1</b> Example 1</a></li>
<li class="chapter" data-level="8.7.2" data-path="deeplearning.html"><a href="deeplearning.html#example-2"><i class="fa fa-check"></i><b>8.7.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="deeplearning.html"><a href="deeplearning.html#cnnSmile"><i class="fa fa-check"></i><b>8.8</b> Smiles Detection with a CNN</a></li>
<li class="chapter" data-level="8.9" data-path="deeplearning.html"><a href="deeplearning.html#SummaryDeepLearning"><i class="fa fa-check"></i><b>8.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multiuser.html"><a href="multiuser.html"><i class="fa fa-check"></i><b>9</b> Multi-User Validation</a><ul>
<li class="chapter" data-level="9.1" data-path="multiuser.html"><a href="multiuser.html#mixed-models"><i class="fa fa-check"></i><b>9.1</b> Mixed Models</a><ul>
<li class="chapter" data-level="9.1.1" data-path="multiuser.html"><a href="multiuser.html#skeleton-action-recognition-with-mixed-models"><i class="fa fa-check"></i><b>9.1.1</b> Skeleton Action Recognition with Mixed Models</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="multiuser.html"><a href="multiuser.html#user-independent-models"><i class="fa fa-check"></i><b>9.2</b> User-Independent Models</a></li>
<li class="chapter" data-level="9.3" data-path="multiuser.html"><a href="multiuser.html#user-dependent-models"><i class="fa fa-check"></i><b>9.3</b> User-Dependent Models</a></li>
<li class="chapter" data-level="9.4" data-path="multiuser.html"><a href="multiuser.html#user-adaptive-models"><i class="fa fa-check"></i><b>9.4</b> User-Adaptive Models</a><ul>
<li class="chapter" data-level="9.4.1" data-path="multiuser.html"><a href="multiuser.html#transfer-learning"><i class="fa fa-check"></i><b>9.4.1</b> Transfer Learning</a></li>
<li class="chapter" data-level="9.4.2" data-path="multiuser.html"><a href="multiuser.html#a-user-adaptive-model-for-activity-recognition"><i class="fa fa-check"></i><b>9.4.2</b> A User-Adaptive Model for Activity Recognition</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="multiuser.html"><a href="multiuser.html#SummaryMultiUser"><i class="fa fa-check"></i><b>9.5</b> Summary</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendixInstall.html"><a href="appendixInstall.html"><i class="fa fa-check"></i><b>A</b> Setup Your Environment</a><ul>
<li class="chapter" data-level="A.1" data-path="appendixInstall.html"><a href="appendixInstall.html#installing-the-datasets"><i class="fa fa-check"></i><b>A.1</b> Installing the Datasets</a></li>
<li class="chapter" data-level="A.2" data-path="appendixInstall.html"><a href="appendixInstall.html#installing-the-examples-source-code"><i class="fa fa-check"></i><b>A.2</b> Installing the Examples Source Code</a></li>
<li class="chapter" data-level="A.3" data-path="appendixInstall.html"><a href="appendixInstall.html#installing-keras-and-tensorflow."><i class="fa fa-check"></i><b>A.3</b> Installing Keras and TensorFlow.</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendixDatasets.html"><a href="appendixDatasets.html"><i class="fa fa-check"></i><b>B</b> Datasets</a><ul>
<li class="chapter" data-level="B.1" data-path="appendixDatasets.html"><a href="appendixDatasets.html#complex-activities"><i class="fa fa-check"></i><b>B.1</b> COMPLEX ACTIVITIES</a></li>
<li class="chapter" data-level="B.2" data-path="appendixDatasets.html"><a href="appendixDatasets.html#depresjon"><i class="fa fa-check"></i><b>B.2</b> DEPRESJON</a></li>
<li class="chapter" data-level="B.3" data-path="appendixDatasets.html"><a href="appendixDatasets.html#electromyography"><i class="fa fa-check"></i><b>B.3</b> ELECTROMYOGRAPHY</a></li>
<li class="chapter" data-level="B.4" data-path="appendixDatasets.html"><a href="appendixDatasets.html#hand-gestures"><i class="fa fa-check"></i><b>B.4</b> HAND GESTURES</a></li>
<li class="chapter" data-level="B.5" data-path="appendixDatasets.html"><a href="appendixDatasets.html#home-tasks"><i class="fa fa-check"></i><b>B.5</b> HOME TASKS</a></li>
<li class="chapter" data-level="B.6" data-path="appendixDatasets.html"><a href="appendixDatasets.html#homicide-reports"><i class="fa fa-check"></i><b>B.6</b> HOMICIDE REPORTS</a></li>
<li class="chapter" data-level="B.7" data-path="appendixDatasets.html"><a href="appendixDatasets.html#indoor-location"><i class="fa fa-check"></i><b>B.7</b> INDOOR LOCATION</a></li>
<li class="chapter" data-level="B.8" data-path="appendixDatasets.html"><a href="appendixDatasets.html#sheep-goats"><i class="fa fa-check"></i><b>B.8</b> SHEEP GOATS</a></li>
<li class="chapter" data-level="B.9" data-path="appendixDatasets.html"><a href="appendixDatasets.html#skeleton-actions"><i class="fa fa-check"></i><b>B.9</b> SKELETON ACTIONS</a></li>
<li class="chapter" data-level="B.10" data-path="appendixDatasets.html"><a href="appendixDatasets.html#smartphone-activities"><i class="fa fa-check"></i><b>B.10</b> SMARTPHONE ACTIVITIES</a></li>
<li class="chapter" data-level="B.11" data-path="appendixDatasets.html"><a href="appendixDatasets.html#smiles"><i class="fa fa-check"></i><b>B.11</b> SMILES</a></li>
<li class="chapter" data-level="B.12" data-path="appendixDatasets.html"><a href="appendixDatasets.html#students-mental-health"><i class="fa fa-check"></i><b>B.12</b> STUDENTS’ MENTAL HEALTH</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="citing-this-book.html"><a href="citing-this-book.html"><i class="fa fa-check"></i>Citing this Book</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Behavior Analysis with Machine Learning and R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Predicting Behavior with Classification Models</h1>
<p>In the previous chapter, the concept of <strong>classification</strong> was introduced along with a simple example (feline type classification). This chapter will cover more in depth concepts on classification methods and their application to behavior analysis tasks. Moreover, additional performance metrics will be introduced. This chapter begins with an introduction to <strong><span class="math inline">\(k\)</span>-nearest neighbors (<span class="math inline">\(k\)</span>-NN)</strong> which is one of the simplest classification algorithms. Then, an example of <span class="math inline">\(k\)</span>-NN applied to indoor location using Wi-Fi signals is presented. This chapter also covers <strong>Decision Trees</strong> and <strong>Naive Bayes</strong> classifiers and how they can be used for activity recognition based on smartphone accelerometer data. After that, <strong>Dynamic Time Warping (DTW)</strong> (a method for aligning time series) is introduced, and an example of how it can be used for hand gesture recognition is presented.</p>
<div id="k-nearest-neighbors" class="section level2">
<h2><span class="header-section-number">2.1</span> <em>k</em>-nearest Neighbors</h2>
<p><span class="math inline">\(k\)</span>-nearest neighbors (<span class="math inline">\(k\)</span>-NN) is one of the simplest classification algorithms. The predicted class for a given <em>query instance</em> is the most common class of its <em>k</em> nearest neighbors. A <em>query instance</em> is just the instance we want to make predictions on. In its most basic form, the algorithm consists of two steps:</p>
<ol style="list-style-type: decimal">
<li>Compute the distance between the <em>query instance</em> and all <em>training instances</em>.</li>
<li>Return the most common class among the <em>k</em> nearest training instances.</li>
</ol>
<p>This is a type of <em>lazy-learning</em> algorithm because all the computations take place at prediction time. There are no parameters to learn at training time! The training phase consists only of storing the training instances so they can be compared to the query instance at prediction time. The hyper-parameter <em>k</em> is usually specified by the user and will depend on each application. We also need to specify a <em>distance function</em> such that similar instances should have smaller distances between them in the feature space and dissimilar instances should have longer distances. For numeric features, the <strong>Euclidean distance</strong> is one of the most commonly used distance metrics. The Euclidean distance between two points can be computed as follows:</p>
<p><span class="math display" id="eq:euclideanDistance">\[\begin{equation}
  d\left(p,q\right) = \sqrt{\sum_{i=1}^n{\left(p_i-q_i\right)^2}}
  \tag{2.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> are <span class="math inline">\(n\)</span>-dimensional feature vectors and <span class="math inline">\(i\)</span> is the index to the vectors’ elements. Figure <a href="classification.html#fig:simpleKnn">2.1</a> shows the idea graphically. (Figure inspired from the k-nn article<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> in Wikipedia). The query instance is depicted with the ‘?’ symbol. If we choose <span class="math inline">\(k=3\)</span> (represented by the inner dashed circle) the predicted class is <em>‘square’</em> because there are two squares but only one circle. If <span class="math inline">\(k=5\)</span> (outer dotted circle), the predicted class is <em>‘circle’</em>.</p>
<div class="figure" style="text-align: center"><span id="fig:simpleKnn"></span>
<img src="images/knn.png" alt="k-NN example for k=3 (inner dashed circle) and k=5 (dotted outer circle)." width="40%" />
<p class="caption">
Figure 2.1: k-NN example for k=3 (inner dashed circle) and k=5 (dotted outer circle).
</p>
</div>
<p>Typical values for <span class="math inline">\(k\)</span> are small odd numbers like <span class="math inline">\(1,2,3,5\)</span>. The <span class="math inline">\(k\)</span>-nn algorithm can also be used for regression with a small modification: Instead of returning the majority class of the nearest neighbors, return the mean value of their response variable. Despite its simplicity, <span class="math inline">\(k\)</span>-nn has proved to perform really well in many tasks including time series classification <span class="citation">(Xi et al. <a href="#ref-xi2006" role="doc-biblioref">2006</a>)</span>.</p>
<div id="indoor-location-with-wi-fi-signals" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Indoor Location with Wi-Fi Signals</h3>

<div class="rmdfolder">
<code>indoor_classification.R</code> <code>indoor_auxiliary.R</code>
</div>

<p>It is possible that you might already have experienced some troubles with geolocation services when you are inside a building. Part of this is because GPS technologies do not provide good indoors accuracy due to several sources of interference. For some applications, it would be beneficial to have accurate location estimations inside buildings even at room-level. For example, in domotics and localization services in big public places like airports or shopping malls. Having good indoor location estimates can also be used in behavior analysis such as extracting trajectory patterns.</p>
<p>In this section, we will implement <span class="math inline">\(k\)</span>-NN to perform indoor location in a building based on Wi-Fi signals. For instance, we can use a smartphone to scan the nearby Wi-Fi access points and based on this information, determine our location at room-level. This can be formulated as a classification problem: Given a set of Wi-Fi signals as input, predict the location where the device is located.</p>
<p>For this classification problem, we will use the <em>INDOOR LOCATION</em> dataset (see Appendix <a href="appendixDatasets.html#appendixDatasets">B</a> for more info.) which was collected with an Android smartphone. The smartphone application scans the nearby access points and stores their information and label. The label is provided by the user and represents the room where the device is located. Several instances for every location were recorded. To generate each instance, the device scans and records the MAC address and signal strength of the nearby access points. A delay of <span class="math inline">\(500\)</span> ms is set between scans. For each location, approximately <span class="math inline">\(3\)</span> minutes of data were collected while the user walked around the specific room. Figure <a href="classification.html#fig:layoutHouse">2.2</a> depicts the layout of the building where the data was collected. The data has four different locations: <em>‘bedroomA’</em>, <em>‘beadroomB’</em>, <em>‘tvroom’</em>, and the <em>‘lobby’</em>. The lobby (not shown in the layout) is at the same level as bedroom A but on the first floor.</p>
<div class="figure" style="text-align: center"><span id="fig:layoutHouse"></span>
<img src="images/layout.png" alt="Layout of the apartments building." width="90%" />
<p class="caption">
Figure 2.2: Layout of the apartments building.
</p>
</div>
<p>Table <a href="classification.html#tab:headWifi">2.1</a> shows the first rows of the dataset. The first column is the class. <code>scanid</code> column is a unique identifier for the given Wi-Fi scan (instance). To preserve privacy, MAC addresses were converted into integer values. Every instance is composed of several rows. For example, the first instance with <code>scanid=1</code> has two rows (one row per mac address). Intuitively, same locations should have similar MAC addresses. From the table, we can see that at <em>bedroomA</em> access points with MAC address <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span> are usually found by the device.</p>
<table>
<caption><span id="tab:headWifi">Table 2.1: </span>First rows of wifi scans.</caption>
<thead>
<tr class="header">
<th align="left">locationid</th>
<th align="right">scanid</th>
<th align="right">mac</th>
<th align="right">signalstrength</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">bedroomA</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">-88.500</td>
</tr>
<tr class="even">
<td align="left">bedroomA</td>
<td align="right">1</td>
<td align="right">2</td>
<td align="right">-91.000</td>
</tr>
<tr class="odd">
<td align="left">bedroomA</td>
<td align="right">2</td>
<td align="right">1</td>
<td align="right">-88.000</td>
</tr>
<tr class="even">
<td align="left">bedroomA</td>
<td align="right">2</td>
<td align="right">2</td>
<td align="right">-90.000</td>
</tr>
<tr class="odd">
<td align="left">bedroomA</td>
<td align="right">3</td>
<td align="right">1</td>
<td align="right">-87.625</td>
</tr>
<tr class="even">
<td align="left">bedroomA</td>
<td align="right">3</td>
<td align="right">2</td>
<td align="right">-90.000</td>
</tr>
<tr class="odd">
<td align="left">bedroomA</td>
<td align="right">4</td>
<td align="right">2</td>
<td align="right">-90.250</td>
</tr>
<tr class="even">
<td align="left">bedroomA</td>
<td align="right">4</td>
<td align="right">1</td>
<td align="right">-90.000</td>
</tr>
<tr class="odd">
<td align="left">bedroomA</td>
<td align="right">4</td>
<td align="right">3</td>
<td align="right">-91.000</td>
</tr>
</tbody>
</table>
<p>Since each instance is composed of several rows, we will convert our data frame into a list of lists where each inner list represents a single instance with the class (<code>locationId</code>), a unique id, and a data frame with the corresponding access points. The example code can be found in the script <code>indoor_classification.R</code>.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="classification.html#cb20-1"></a><span class="co"># Read Wi-Fi data</span></span>
<span id="cb20-2"><a href="classification.html#cb20-2"></a>df &lt;-<span class="st"> </span><span class="kw">read.csv</span>(datapath, <span class="dt">stringsAsFactors =</span> F)</span>
<span id="cb20-3"><a href="classification.html#cb20-3"></a></span>
<span id="cb20-4"><a href="classification.html#cb20-4"></a><span class="co"># Convert data frame into a list of lists.</span></span>
<span id="cb20-5"><a href="classification.html#cb20-5"></a><span class="co"># Each inner list represents one instance.</span></span>
<span id="cb20-6"><a href="classification.html#cb20-6"></a>dataset &lt;-<span class="st"> </span><span class="kw">wifiScansToList</span>(df)</span>
<span id="cb20-7"><a href="classification.html#cb20-7"></a></span>
<span id="cb20-8"><a href="classification.html#cb20-8"></a><span class="co"># Print number of instances in the dataset.</span></span>
<span id="cb20-9"><a href="classification.html#cb20-9"></a><span class="kw">length</span>(dataset)</span>
<span id="cb20-10"><a href="classification.html#cb20-10"></a><span class="co">#&gt; [1] 365</span></span>
<span id="cb20-11"><a href="classification.html#cb20-11"></a></span>
<span id="cb20-12"><a href="classification.html#cb20-12"></a><span class="co"># Print the first instance.</span></span>
<span id="cb20-13"><a href="classification.html#cb20-13"></a>dataset[[<span class="dv">1</span>]]</span>
<span id="cb20-14"><a href="classification.html#cb20-14"></a><span class="co">#&gt; $locationId</span></span>
<span id="cb20-15"><a href="classification.html#cb20-15"></a><span class="co">#&gt; [1] &quot;bedroomA&quot;</span></span>
<span id="cb20-16"><a href="classification.html#cb20-16"></a><span class="co">#&gt; </span></span>
<span id="cb20-17"><a href="classification.html#cb20-17"></a><span class="co">#&gt; $scanId</span></span>
<span id="cb20-18"><a href="classification.html#cb20-18"></a><span class="co">#&gt; [1] 1</span></span>
<span id="cb20-19"><a href="classification.html#cb20-19"></a><span class="co">#&gt; </span></span>
<span id="cb20-20"><a href="classification.html#cb20-20"></a><span class="co">#&gt; $accessPoints</span></span>
<span id="cb20-21"><a href="classification.html#cb20-21"></a><span class="co">#&gt;   mac signalstrength</span></span>
<span id="cb20-22"><a href="classification.html#cb20-22"></a><span class="co">#&gt; 1   1          -88.5</span></span>
<span id="cb20-23"><a href="classification.html#cb20-23"></a><span class="co">#&gt; 2   2          -91.0</span></span></code></pre></div>
<p>First, we read the dataset from the csv file and store it in the data frame <code>df</code>. To make things easier, the data frame is converted into a list of lists using the auxiliary function <code>wifiScansToList()</code> which is defined in the script <code>indoor_auxiliary.R</code>. Next we print the number of instances in the dataset, that is, the number of lists in the dataset list. The dataset list contains <span class="math inline">\(365\)</span> instances. The <span class="math inline">\(365\)</span> was just a coincidence, the data was not collected every day during one year but in the same day. Next, we extract the first instance with <code>dataset[[1]]</code>. Here, we can see that each instance has three pieces of information. The class (locationId), a unique id (scanId), and a set of access points stored in a data frame. The first instance has two access points with MAC addresses <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span>. There is also information about the signal strength, though, this one will not be used.</p>
<p>Since we would expect that similar locations have similar MAC addresses and locations that are far away from each other have different MAC addresses, we need a distance measure that captures this notion of similarity. In this case, we cannot use the Euclidean distance on MAC addresses. Even though they were encoded as integer values, they do not represent magnitudes but unique identifiers. Each instance is composed of a set of <span class="math inline">\(n\)</span> MAC addresses stored in the <code>accessPoints</code> data frame. To compute the distance between two instances (two sets) we can use the <em>Jaccard distance</em>. This distance is based on element sets:</p>
<p><span class="math display" id="eq:jaccardDistance">\[\begin{equation}
  j\left(A,B\right)=\frac{\left|A\cup B\right|-\left|A\cap B\right|}{\left|A\cup B\right|}
  \tag{2.2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are sets of MAC addresses. A <strong>set</strong> is an unordered collection of elements. As an example, let’s say we have two sets, <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span>:</p>
<p><span class="math display">\[\begin{align*}
S_1&amp;=\{a,b,c,d,e\}\\
S_2&amp;=\{e,f,g,a\}
\end{align*}\]</span></p>
<p>The set <span class="math inline">\(S_1\)</span> has <span class="math inline">\(5\)</span> elements (letters) and <span class="math inline">\(S_2\)</span> has <span class="math inline">\(4\)</span> elements. <span class="math inline">\(A \cup B\)</span> means the <strong>union</strong> of the two sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> and its result is the set of all elements that are either in <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>. For instance, the union of <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> is <span class="math inline">\(S_1 \cup S_2 = \{a,b,c,d,e,f,g\}\)</span>. The <span class="math inline">\(A \cap B\)</span> denotes the <strong>intersection</strong> between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> which is the set of elements that are in both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. In our example, <span class="math inline">\(S_1 \cap S_2 = \{a,e\}\)</span>. Finally the vertical bars <span class="math inline">\(||\)</span> mean the <strong>cardinality</strong> of the set, that is, the number of elements. The cardinality of <span class="math inline">\(S_1\)</span> is <span class="math inline">\(|S_1|=5\)</span> because it has <span class="math inline">\(5\)</span> elements. The cardinality of the union of the two sets <span class="math inline">\(|S_1 \cup S_2|=7\)</span> because the set that results from the union of <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> has <span class="math inline">\(7\)</span> elements.</p>
<p>In R, we can implement the Jaccard distance as follows:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="classification.html#cb21-1"></a>jaccardDistance &lt;-<span class="st"> </span><span class="cf">function</span>(set1, set2){</span>
<span id="cb21-2"><a href="classification.html#cb21-2"></a>  lengthUnion &lt;-<span class="st"> </span><span class="kw">length</span>(<span class="kw">union</span>(set1, set2))</span>
<span id="cb21-3"><a href="classification.html#cb21-3"></a>  lengthIntersectoin &lt;-<span class="st"> </span><span class="kw">length</span>(<span class="kw">intersect</span>(set1, set2))</span>
<span id="cb21-4"><a href="classification.html#cb21-4"></a>  d &lt;-<span class="st"> </span>(lengthUnion <span class="op">-</span><span class="st"> </span>lengthIntersectoin)  <span class="op">/</span><span class="st"> </span>lengthUnion</span>
<span id="cb21-5"><a href="classification.html#cb21-5"></a>  <span class="kw">return</span>(d)</span>
<span id="cb21-6"><a href="classification.html#cb21-6"></a>}</span></code></pre></div>
<p>The implementation is in the script <code>indoor_auxiliary.R</code>. Now, we can try our function! Let’s compute the distance between two instances of the same class (<em>‘bedroomA’</em>).</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="classification.html#cb22-1"></a><span class="co"># Compute jaccard distance between instances with same class:</span></span>
<span id="cb22-2"><a href="classification.html#cb22-2"></a><span class="co"># (bedroomA)</span></span>
<span id="cb22-3"><a href="classification.html#cb22-3"></a><span class="kw">jaccardDistance</span>(dataset[[<span class="dv">1</span>]]<span class="op">$</span>accessPoints<span class="op">$</span>mac,</span>
<span id="cb22-4"><a href="classification.html#cb22-4"></a>                dataset[[<span class="dv">4</span>]]<span class="op">$</span>accessPoints<span class="op">$</span>mac)</span>
<span id="cb22-5"><a href="classification.html#cb22-5"></a><span class="co">#&gt; [1] 0.3333333</span></span></code></pre></div>
<p>Now let’s try to compute the distance between instances with different classes.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="classification.html#cb23-1"></a><span class="co"># Jaccard distance of instances with different class:</span></span>
<span id="cb23-2"><a href="classification.html#cb23-2"></a><span class="co"># (bedroomA and bedroomB)</span></span>
<span id="cb23-3"><a href="classification.html#cb23-3"></a><span class="kw">jaccardDistance</span>(dataset[[<span class="dv">1</span>]]<span class="op">$</span>accessPoints<span class="op">$</span>mac,</span>
<span id="cb23-4"><a href="classification.html#cb23-4"></a>                dataset[[<span class="dv">210</span>]]<span class="op">$</span>accessPoints<span class="op">$</span>mac)</span>
<span id="cb23-5"><a href="classification.html#cb23-5"></a><span class="co">#&gt; [1] 0.6666667</span></span></code></pre></div>
<p>The distance between instances of the same class was <span class="math inline">\(0.33\)</span> whereas the distance between instances of different class was <span class="math inline">\(0.66\)</span>. So, our function is working as expected.</p>
<p>In the extreme case when sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are the same, the distance will be <span class="math inline">\(0\)</span>. When there are no common elements in the sets, the distance will be <span class="math inline">\(1\)</span>. Armed with this distance metric, we can now implement the <span class="math inline">\(k\)</span>-nn function in R. The <code>knn_classifier()</code> implementation is in the script <code>indoor_auxiliary.R</code>. Its first argument is the dataset (the list of instances). The second argument <em>k</em>, is the number of nearest neighbors to use, and the last two arguments are the indices of the train and test instances, respectively. This indices are pointers to the elements in the <code>dataset</code> variable.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="classification.html#cb24-1"></a>knn_classifier &lt;-<span class="st"> </span><span class="cf">function</span>(dataset, k, trainSetIndices, testSetIndices){</span>
<span id="cb24-2"><a href="classification.html#cb24-2"></a>  </span>
<span id="cb24-3"><a href="classification.html#cb24-3"></a>  groundTruth &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb24-4"><a href="classification.html#cb24-4"></a>  predictions &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb24-5"><a href="classification.html#cb24-5"></a>  </span>
<span id="cb24-6"><a href="classification.html#cb24-6"></a>  <span class="cf">for</span>(queryInstance <span class="cf">in</span> testSetIndices){</span>
<span id="cb24-7"><a href="classification.html#cb24-7"></a>    distancesToQuery &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb24-8"><a href="classification.html#cb24-8"></a>    </span>
<span id="cb24-9"><a href="classification.html#cb24-9"></a>    <span class="cf">for</span>(trainInstance <span class="cf">in</span> trainSetIndices){</span>
<span id="cb24-10"><a href="classification.html#cb24-10"></a>      jd &lt;-<span class="st"> </span><span class="kw">jaccardDistance</span>(dataset[[queryInstance]]<span class="op">$</span>accessPoints<span class="op">$</span>mac,</span>
<span id="cb24-11"><a href="classification.html#cb24-11"></a>                            dataset[[trainInstance]]<span class="op">$</span>accessPoints<span class="op">$</span>mac)</span>
<span id="cb24-12"><a href="classification.html#cb24-12"></a>      distancesToQuery &lt;-<span class="st"> </span><span class="kw">c</span>(distancesToQuery, jd)</span>
<span id="cb24-13"><a href="classification.html#cb24-13"></a>    }</span>
<span id="cb24-14"><a href="classification.html#cb24-14"></a>    </span>
<span id="cb24-15"><a href="classification.html#cb24-15"></a>    indices &lt;-<span class="st"> </span><span class="kw">sort</span>(distancesToQuery, <span class="dt">index.return =</span> <span class="ot">TRUE</span>)<span class="op">$</span>ix</span>
<span id="cb24-16"><a href="classification.html#cb24-16"></a>    indices &lt;-<span class="st"> </span>indices[<span class="dv">1</span><span class="op">:</span>k]</span>
<span id="cb24-17"><a href="classification.html#cb24-17"></a>    <span class="co"># Indices of the k nearest neighbors</span></span>
<span id="cb24-18"><a href="classification.html#cb24-18"></a>    nnIndices &lt;-<span class="st"> </span>trainSetIndices[indices]</span>
<span id="cb24-19"><a href="classification.html#cb24-19"></a>    <span class="co"># Get the actual instances</span></span>
<span id="cb24-20"><a href="classification.html#cb24-20"></a>    nnInstances &lt;-<span class="st"> </span>dataset[nnIndices]</span>
<span id="cb24-21"><a href="classification.html#cb24-21"></a>    <span class="co"># Get their respective classes</span></span>
<span id="cb24-22"><a href="classification.html#cb24-22"></a>    nnClasses &lt;-<span class="st"> </span><span class="kw">sapply</span>(nnInstances, <span class="cf">function</span>(e){e[[<span class="dv">1</span>]]})</span>
<span id="cb24-23"><a href="classification.html#cb24-23"></a>    prediction &lt;-<span class="st"> </span><span class="kw">Mode</span>(nnClasses)</span>
<span id="cb24-24"><a href="classification.html#cb24-24"></a>    predictions &lt;-<span class="st"> </span><span class="kw">c</span>(predictions, prediction)</span>
<span id="cb24-25"><a href="classification.html#cb24-25"></a>    groundTruth &lt;-<span class="st"> </span><span class="kw">c</span>(groundTruth,</span>
<span id="cb24-26"><a href="classification.html#cb24-26"></a>                     dataset[[queryInstance]]<span class="op">$</span>locationId)</span>
<span id="cb24-27"><a href="classification.html#cb24-27"></a>  }</span>
<span id="cb24-28"><a href="classification.html#cb24-28"></a>  </span>
<span id="cb24-29"><a href="classification.html#cb24-29"></a>  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">predictions =</span> predictions,</span>
<span id="cb24-30"><a href="classification.html#cb24-30"></a>              <span class="dt">groundTruth =</span> groundTruth))</span>
<span id="cb24-31"><a href="classification.html#cb24-31"></a>}</span></code></pre></div>
<p>For each instance <code>queryInstance</code> in the test set, the <code>knn_classifier()</code> computes its jaccard distance to every other instance in the train set and stores those distances in <code>distancesToQuery</code>. Then, those distances are sorted in ascending order and the most common class among the first <span class="math inline">\(k\)</span> elements is returned as the predicted class. The function <code>Mode()</code> was used to return the most common element. Finally, <code>knn_classifier()</code> returns a list with the predictions for every instance in the test set and their respective ground truth class for evaluation.</p>
<p>Now, we can try our classifier. We will use <span class="math inline">\(70\%\)</span> of the dataset as train set and the remaining as the test set.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="classification.html#cb25-1"></a><span class="co"># Total number of instances</span></span>
<span id="cb25-2"><a href="classification.html#cb25-2"></a>numberInstances &lt;-<span class="st"> </span><span class="kw">length</span>(dataset)</span>
<span id="cb25-3"><a href="classification.html#cb25-3"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb25-4"><a href="classification.html#cb25-4"></a><span class="kw">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb25-5"><a href="classification.html#cb25-5"></a><span class="co"># Split into train and test sets.</span></span>
<span id="cb25-6"><a href="classification.html#cb25-6"></a>trainSetIndices &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>numberInstances,</span>
<span id="cb25-7"><a href="classification.html#cb25-7"></a>                          <span class="dt">size =</span> <span class="kw">round</span>(numberInstances <span class="op">*</span><span class="st"> </span><span class="fl">0.7</span>),</span>
<span id="cb25-8"><a href="classification.html#cb25-8"></a>                          <span class="dt">replace =</span> F)</span>
<span id="cb25-9"><a href="classification.html#cb25-9"></a>testSetIndices &lt;-<span class="st"> </span>(<span class="dv">1</span><span class="op">:</span>numberInstances)[<span class="op">-</span>trainSetIndices]</span></code></pre></div>
<p>The function <code>knn_classifier()</code> will predict the class for each test set instance and will return a list with their predictions and their ground truth classes. With this information, we can compute the accuracy on the test set which is the percentage of correctly classified instances. For this example, I will set <span class="math inline">\(k=3\)</span>.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="classification.html#cb26-1"></a><span class="co"># Obtain predictions on the test set.</span></span>
<span id="cb26-2"><a href="classification.html#cb26-2"></a>result &lt;-<span class="st"> </span><span class="kw">knn_classifier</span>(dataset,</span>
<span id="cb26-3"><a href="classification.html#cb26-3"></a>                         <span class="dt">k =</span> <span class="dv">3</span>,</span>
<span id="cb26-4"><a href="classification.html#cb26-4"></a>                         trainSetIndices,</span>
<span id="cb26-5"><a href="classification.html#cb26-5"></a>                         testSetIndices)</span>
<span id="cb26-6"><a href="classification.html#cb26-6"></a><span class="co"># Calculate and print accuracy.</span></span>
<span id="cb26-7"><a href="classification.html#cb26-7"></a><span class="kw">sum</span>(result<span class="op">$</span>predictions <span class="op">==</span><span class="st"> </span>result<span class="op">$</span>groundTruth) <span class="op">/</span></span>
<span id="cb26-8"><a href="classification.html#cb26-8"></a><span class="st">  </span><span class="kw">length</span>(result<span class="op">$</span>predictions)</span>
<span id="cb26-9"><a href="classification.html#cb26-9"></a><span class="co">#&gt; [1] 0.9454545</span></span></code></pre></div>
<p>Not bad! Our simple <span class="math inline">\(k\)</span>-nn algorithm achieved an accuracy of <span class="math inline">\(94.5\%\)</span>. Usually, it is a good idea to visualize the predictions to have a better understanding of the classifier’s behavior. <strong>Confusion matrices</strong> allow us to precisely do that. We can use the <code>confusionMatrix()</code> function from the <code>caret</code> package to generate a confusion matrix. Its first argument is a factor with the predictions and the second one is a factor with the corresponding true values. This function returns an object with several performance metrics (see next section) and the confusion matrix.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="classification.html#cb27-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb27-2"><a href="classification.html#cb27-2"></a>cm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="kw">factor</span>(result<span class="op">$</span>predictions),</span>
<span id="cb27-3"><a href="classification.html#cb27-3"></a>                      <span class="kw">factor</span>(result<span class="op">$</span>groundTruth))</span>
<span id="cb27-4"><a href="classification.html#cb27-4"></a>cm<span class="op">$</span>table <span class="co"># Access the confusion matrix.</span></span>
<span id="cb27-5"><a href="classification.html#cb27-5"></a><span class="co">#&gt; Reference</span></span>
<span id="cb27-6"><a href="classification.html#cb27-6"></a><span class="co">#&gt; Prediction bedroomA bedroomB lobby tvroom</span></span>
<span id="cb27-7"><a href="classification.html#cb27-7"></a><span class="co">#&gt; bedroomA       26        0     3      1</span></span>
<span id="cb27-8"><a href="classification.html#cb27-8"></a><span class="co">#&gt; bedroomB        0       17     0      1</span></span>
<span id="cb27-9"><a href="classification.html#cb27-9"></a><span class="co">#&gt; lobby           0        1    28      0</span></span>
<span id="cb27-10"><a href="classification.html#cb27-10"></a><span class="co">#&gt; tvroom          0        0     0     33</span></span></code></pre></div>
<p>The columns represent the true classes and rows the predictions. For example, from the total <span class="math inline">\(31\)</span> instances of type <em>‘lobby’</em>, <span class="math inline">\(28\)</span> of them were correctly classified as <em>‘lobby’</em> but <span class="math inline">\(3\)</span> of them were misclassified as <em>‘bedroomA’</em>. Something I find useful is to plot the confusion matrix as proportions instead of counts (Figure <a href="classification.html#fig:wifiCM">2.3</a>). From this confusion matrix we can see that for the class <em>‘bedroomB’</em>, <span class="math inline">\(94\%\)</span> of the instances were correctly classified and <span class="math inline">\(6\%\)</span> were mislabeled as <em>‘lobby’</em>. On the other hand, instances of type <em>‘bedroomA’</em> were always classified correctly.</p>
<div class="figure" style="text-align: center"><span id="fig:wifiCM"></span>
<img src="images/indoorCM.png" alt="Confusion matrix for location predictions." width="70%" />
<p class="caption">
Figure 2.3: Confusion matrix for location predictions.
</p>
</div>
<p>A confusion matrix is a good way to analyze classification results per class and spot weaknesses which we can use to improve the model, for example, by extracting additional features.</p>
</div>
</div>
<div id="performance-metrics" class="section level2">
<h2><span class="header-section-number">2.2</span> Performance Metrics</h2>
<p>Performance metrics allow us to measure the generalization performance of a model from several angles. The most common performance metric for classification is the accuracy:</p>
<p><span class="math display" id="eq:metricAccuracy">\[\begin{equation}
  accuracy = \frac{\# \textrm{ correctly classified instances}}{\textrm{total } \# \textrm{ instances}}
  \tag{2.3}
\end{equation}\]</span></p>
<p>In order to have a better understanding of the generalization performance of a model, it is a good practice to compute several performance metrics in addition to the accuracy. Accuracy also has some limitations, especially in highly imbalanced datasets. The following metrics provide different views of a model’s performance for the binary case (when there are only two classes). These metrics can be extended to the multi-class setting using a <em>one v.s. all</em> approach. That is, compare each class to the remaining classes.</p>
<p>Before introducing the other metrics, it is convenient to define some terms:</p>
<ul>
<li>True positives (TP): Positive examples classified as positives.</li>
<li>True negatives (TN): Negative examples classified as negatives.</li>
<li>False positives (FP): Negative examples misclassified as positives.</li>
<li>False negatives (FN): Positive examples misclassified as negatives.</li>
</ul>
<p>For binary classification, it is you who decides which is the positive class. For example, if your problem is about detecting falls and you have two classes: <em>‘fall’</em> and <em>‘nofall’</em>, then, considering <em>‘fall’</em> as the positive class makes sense since that is the one you are most interested in detecting. The following, is a list of commonly used metrics in classification:</p>
<p><strong>Recall:</strong> The proportion of positives that are classified as such. Alternative names for recall are: true positive rate, sensitivity, and hit rate. In fact, the diagonal of the confusion matrix with proportions of the indoor location example shows the recall for each class (Figure <a href="classification.html#fig:wifiCM">2.3</a>).</p>
<p><span class="math display" id="eq:metricRecall">\[\begin{equation}
  recall = \frac{\textrm{TP}}{\textrm{P}}
  \tag{2.4}
\end{equation}\]</span></p>
<p><strong>Specificity:</strong> The proportion of negatives classified as such. It is also called the true negative rate.</p>
<p><span class="math display" id="eq:metricSpecificity">\[\begin{equation}
  specificity = \frac{\textrm{TN}}{\textrm{N}}
  \tag{2.5}
\end{equation}\]</span></p>
<p><strong>Precision:</strong> The fraction of true positives among those classified as positives. Also known as the positive predictive value.</p>
<p><span class="math display" id="eq:metricPrecision">\[\begin{equation}
  precision = \frac{\textrm{TP}}{\textrm{TP + FP}}
  \tag{2.6}
\end{equation}\]</span></p>
<p><strong>F1-score:</strong> This is the harmonic mean of precision and recall.</p>
<p><span class="math display" id="eq:metricFscore">\[\begin{equation}
  \textit{F1-score} = 2 \cdot \frac{\textrm{precision} \cdot \textrm{recall}}{\textrm{precision + recall}}
  \tag{2.7}
\end{equation}\]</span></p>
<p>The <code>confusionMatrix()</code> function from the <code>caret</code> package computes several of those metrics. From our previous confusion matrix object, we can see those metrics by class.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="classification.html#cb28-1"></a>cm<span class="op">$</span>byClass[,<span class="kw">c</span>(<span class="st">&quot;Recall&quot;</span>, <span class="st">&quot;Specificity&quot;</span>, <span class="st">&quot;Precision&quot;</span>, <span class="st">&quot;F1&quot;</span>)]</span>
<span id="cb28-2"><a href="classification.html#cb28-2"></a><span class="co">#&gt;                    Recall Specificity Precision        F1</span></span>
<span id="cb28-3"><a href="classification.html#cb28-3"></a><span class="co">#&gt; Class: bedroomA 1.0000000   0.9523810 0.8666667 0.9285714</span></span>
<span id="cb28-4"><a href="classification.html#cb28-4"></a><span class="co">#&gt; Class: bedroomB 0.9444444   0.9891304 0.9444444 0.9444444</span></span>
<span id="cb28-5"><a href="classification.html#cb28-5"></a><span class="co">#&gt; Class: lobby    0.9032258   0.9873418 0.9655172 0.9333333</span></span>
<span id="cb28-6"><a href="classification.html#cb28-6"></a><span class="co">#&gt; Class: tvroom   0.9428571   1.0000000 1.0000000 0.9705882</span></span></code></pre></div>
<p>The mean of the metrics across all classes can be computed by taking the mean for each column of the returned object:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="classification.html#cb29-1"></a><span class="kw">colMeans</span>(cm<span class="op">$</span>byClass[,<span class="kw">c</span>(<span class="st">&quot;Recall&quot;</span>, <span class="st">&quot;Specificity&quot;</span>, <span class="st">&quot;Precision&quot;</span>, <span class="st">&quot;F1&quot;</span>)])</span>
<span id="cb29-2"><a href="classification.html#cb29-2"></a><span class="co">#&gt;    Recall Specificity   Precision          F1 </span></span>
<span id="cb29-3"><a href="classification.html#cb29-3"></a><span class="co">#&gt; 0.9476318   0.9822133   0.9441571   0.9442344</span></span></code></pre></div>
</div>
<div id="decision-trees" class="section level2">
<h2><span class="header-section-number">2.3</span> Decision Trees</h2>
<p>Decision trees are powerful predictive models (especially when combining several of them, see chapter <a href="ensemble.html#ensemble">3</a>) used for classification and regression tasks. Here, the focus will be on classification. Each node in a tree represents partial or final decisions based on a single feature. If a node is a leaf, then it leads to a final decision. A leaf is simply a terminal node, i.e, it has no children nodes. Given a feature vector representing an instance, the predicted class is obtained by testing the feature values and following the tree path until a leaf is reached. Figure <a href="classification.html#fig:treeExample">2.4</a> shows an example decision tree and a query instance with an unknown class. To get the final class, features are evaluated starting at the root. In this case <em>number_wheels</em> is <span class="math inline">\(4\)</span> in the query instance so we take the left path from the root. Now, we need to evaluate <em>weight</em>. This time the test is false since the weight is <span class="math inline">\(2300\)</span> and we take the right path. Since this is a leaf node the final predicted class is <em>‘truck’</em>. Usually, small trees are preferred (small depth) because they are easier to visualize and interpret and are less prone to overfitting. The example tree has a depth of 2. Should the number of wheels had been <span class="math inline">\(2\)</span> instead of <span class="math inline">\(4\)</span>, then testing the <em>weight</em> feature would not have been necessary.</p>
<div class="figure" style="text-align: center"><span id="fig:treeExample"></span>
<img src="images/treeExample.png" alt="Example decision tree. The query instance is classified as truck by this tree." width="90%" />
<p class="caption">
Figure 2.4: Example decision tree. The query instance is classified as truck by this tree.
</p>
</div>
<p>As shown in the example, decision trees are easy to interpret and the explanation of a final result can be obtained by just following the path. Now let’s see how these decision trees are learned from data. Consider the following artificial <em>cinema</em> dataset (Figure <a href="classification.html#fig:cinemaTable">2.5</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:cinemaTable"></span>
<img src="images/cinemaTable.png" alt="Cinema dataset." width="50%" />
<p class="caption">
Figure 2.5: Cinema dataset.
</p>
</div>
<p>The first <span class="math inline">\(4\)</span> variables are features and the last column is the class. The class is the decision whether or not we should go to the movies based on the other variables. In this case, all variables are binary except <em>Price</em> which has three possible values: <em>low</em>, <em>medium</em>, and <em>high</em>.</p>
<ul>
<li><em>Tired:</em> Indicates if the person is tired or not.</li>
<li><em>Rain:</em> Whether it is raining or not.</li>
<li><em>Comedy:</em> Indicates if the genre of the movie is <em>comedy</em>.</li>
<li><em>Price:</em> Ticket price.</li>
<li><em>Go:</em> The decision of whether to go to the movies or not.</li>
</ul>
<p>The main question when building a tree is which feature should be at the root (top). Once you answer this question, you may need to grow the tree by adding another feature (node) as one of the root’s children. To decide which new feature to add you need to answer the same first question: “What feature should be at the root of this subtree?”. This is a recursive definition! The tree keeps growing until you reach a leaf node, there are no more features to select from, or you have reached a predefined maximum depth.</p>
<p>For the <em>cinema</em> dataset we need to find which is the best variable to be placed at the root. Let’s suppose we need to choose between <em>Price</em> and <em>Comedy</em>. Figure <a href="classification.html#fig:treeAlgo1">2.6</a> shows these two possibilities.</p>
<div class="figure" style="text-align: center"><span id="fig:treeAlgo1"></span>
<img src="images/treeAlgo1.png" alt="Two example trees with one variable split by Price (left) and Comedy (right)." width="100%" />
<p class="caption">
Figure 2.6: Two example trees with one variable split by Price (left) and Comedy (right).
</p>
</div>
<p>If we select <em>Price</em>, there are three possible subnodes, one for each value: <em>low</em>, <em>medium</em>, and <em>high</em>. If <em>Price</em> is <em>low</em> then four instances fall into this subtree (the first four from the table). For all of them, the value of <em>Go</em> is <span class="math inline">\(1\)</span>. If <em>Price</em> is <em>high</em>, two instances fall into this category and their <em>Go</em> value is <span class="math inline">\(0\)</span>, thus if the price is high then you should not go to the movies according to this data. There are six instances for which the <em>Price</em> value is <em>medium</em>. From those, two of them have <em>Go=1</em> and the remaining four have <em>Go=0</em>. For cases when the price is <em>low</em> or <em>high</em> we can arrive at a solution. If the price is <em>low</em> then go to the cinema, if the price is <em>high</em> then do not go. However, if the price is <em>medium</em> it is still not clear what to do since this subnode is not <em>pure</em>. That is, the types of the instances are mixed: two with an output of <span class="math inline">\(1\)</span> and four with an output of <span class="math inline">\(0\)</span>. In this case we can try to use another feature to decide and grow the tree but first, let’s look at what happens if we decide to use <em>Comedy</em> as the first feature at the root. In this case, we end up with two subsets with six instances each. And for each subnode, what decision should we take is still not clear because the output is ‘mixed’ (Go: 3, NotGo: 3). At this point we would need to continue growing the tree below each subnode.</p>
<p>Intuitively, it seems like <em>Price</em> is a better feature since its subnodes are more <em>pure</em>. Then we can use another feature to split the instances whose <em>Price</em> is <em>medium</em>. For example, using the <em>Comedy</em> variable. Figure <a href="classification.html#fig:treeAlgo2">2.7</a> shows how this would look like. Since one of the subnodes of <em>Comedy</em> is still not pure we can further split it using the <em>Rain</em> variable, for example. At this point, we can not split any further. Note that the <em>Tired</em> variable was never used.</p>
<div class="figure" style="text-align: center"><span id="fig:treeAlgo2"></span>
<img src="images/treeAlgo2.png" alt="Tree splitting example. Left: tree splits. Right: Highlighted instances when splitting Comedy." width="100%" />
<p class="caption">
Figure 2.7: Tree splitting example. Left: tree splits. Right: Highlighted instances when splitting Comedy.
</p>
</div>
<p>So far, we have chosen the root variable based on which one looks more pure but to automate the process, we need a way to measure this <em>purity</em> in a quantitative manner. One way to do that is by using the <em>entropy</em>. <em>Entropy</em> is a measure of uncertainty from information theory. It is zero when there is no uncertainty and one when there is complete uncertainty. The entropy of a discrete variable <span class="math inline">\(X\)</span> with values <span class="math inline">\(x_1\dots x_n\)</span> and probability mass function <span class="math inline">\(P(X)\)</span> is:</p>
<p><span class="math display" id="eq:entropy">\[\begin{equation}
  H(X) = -\sum_{i=1}^n{P(x_i)log P(x_i)}
  \tag{2.8}
\end{equation}\]</span></p>
<p>Take for example a fair coin with probability of heads and tails = <span class="math inline">\(0.5\)</span> each. The entropy for that coin is:</p>
<p><span class="math display">\[\begin{equation*}
  H(X) = - (0.5)log(0.5) + (0.5)log(0.5) = 1
\end{equation*}\]</span></p>
<p>Since we do not know what will be the result when we drop the coin, the entropy is maximum. Now consider the extreme case when the coin is biased such that the probability of heads is <span class="math inline">\(1\)</span> and the probability of tails is <span class="math inline">\(0\)</span>. The entropy in this case is zero:</p>
<p><span class="math display">\[\begin{equation*}
  H(X) = - (1)log(1) + (0)log(0) = 0
\end{equation*}\]</span></p>
<p>If we know that the result is always going to be heads, then there is no uncertainty when the coin is dropped. The entropy of <span class="math inline">\(p\)</span> positive examples and <span class="math inline">\(n\)</span> negative examples is:</p>
<p><span class="math display" id="eq:binaryentropy">\[\begin{equation}
  H(p, n) = - (\frac{p}{p+n})log(\frac{p}{p+n}) + (\frac{n}{p+n})log(\frac{n}{p+n})
  \tag{2.9}
\end{equation}\]</span></p>
<p>Thus, the entropies for the three possible values of <em>Price</em> are:</p>
<p><span class="math display">\[\begin{equation*}
  H_{price=low}(4, 0) = - (\frac{4}{4+0})log(\frac{4}{4+0}) + (\frac{0}{4+0})log(\frac{0}{4+0}) = 0
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
  H_{price=medium}(2, 4) = - (\frac{2}{2+4})log(\frac{2}{2+4}) + (\frac{4}{2+4})log(\frac{4}{2+4}) = 0.918
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
  H_{price=high}(0, 2) = - (\frac{0}{0+2})log(\frac{0}{0+2}) + (\frac{2}{0+2})log(\frac{2}{0+2}) = 0
\end{equation*}\]</span></p>
<p>The average of those three can be calculated by taking into account the number of corresponding instances for each value and the total number of instances (<span class="math inline">\(12\)</span>):</p>
<p><span class="math display">\[\begin{equation*}
  meanH(price) = (4/12)(0) + (6/12)(0.918) + (2/12)(0) = 0.459
\end{equation*}\]</span></p>
<p>Before deciding to split on <em>Price</em> the entropy of the entire dataset is <span class="math inline">\(1\)</span> since there are six positive and negative examples:</p>
<p><span class="math display">\[\begin{equation*}
  H(6,6) = 1
\end{equation*}\]</span></p>
<p>thus, the information gain for <em>Price</em> is:</p>
<p><span class="math display">\[\begin{equation*}
  infoGain(Price) = 1 - meanH(Price) = 1 - 0.459 = 0.541
\end{equation*}\]</span></p>
<p>For the rest of the variables the information gain is:</p>
<p><span class="math inline">\(infoGain(Tired) = 0\)</span></p>
<p><span class="math inline">\(infoGain(Rain) = 0.020\)</span></p>
<p><span class="math inline">\(infoGain(Comedy) = 0\)</span></p>
<p>The highest information gain is produced by <em>Price</em>, thus, it is selected as the root node. Then, the process continues recursively for each branch but excluding <em>Price</em>. Since branches with values <em>low</em> and <em>high</em> are already done, we only need to further split <em>medium</em>. Sometimes it is not possible to have completely pure nodes like <em>low</em> and <em>high</em>. This can happen for example, when there are no more attributes left or when two or more instances have the same feature values but different labels. In those situations the final prediction is the most common label (majority vote).</p>
<p>There exist many implementations of decision trees. Some implementations compute variable importance using the entropy (as shown here) but others use the Gini index, for example. Each implementation also treats numeric variables in different ways. Pruning the tree using different techniques is also common in order to reduce its size.</p>
<p>Some of the most common implementations are C4.5 trees <span class="citation">(Quinlan <a href="#ref-quinlan2014" role="doc-biblioref">2014</a>)</span> and CART <span class="citation">(Steinberg and Colla <a href="#ref-steinberg2009" role="doc-biblioref">2009</a>)</span>. The later is implemented in the <code>rpart</code> R package <span class="citation">(Therneau and Atkinson <a href="#ref-rpart" role="doc-biblioref">2019</a>)</span> which will be used in the following section to build a model that predicts physical activities from smartphones sensor data.</p>
<div id="activityRecognition" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Activity Recognition with Smartphones</h3>

<div class="rmdfolder">
<code>smartphone_activities.R</code>
</div>

<p>As mentioned in the introduction, behavior can be an observable activity in a human. We can easily infer what <strong>physical activity</strong> someone is doing by looking at her/his body movements. Observing physical activities can provide useful behavioral and contextual information about someone. This can also be used as a proxy to, for example, infer someone’s health condition by detecting deviations in activity patterns.</p>
<p>Nowadays, most smartphones come with a tri-axial accelerometer sensor. This sensor measures gravitational forces from the <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, and <span class="math inline">\(z\)</span> axes. This information can be used to capture movement patterns from the user and automate the process of monitoring the type of physical activity being performed.</p>
<p>In this section, we will use decision trees to automatically classify physical activities from acceleration data. We will use the <em>SMARTPHONE ACTIVITIES</em> dataset that contains acceleration recordings that were collected with a smartphone. This dataset is called WISDM<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> and was made available by <span class="citation">Kwapisz, Weiss, and Moore (<a href="#ref-kwapisz2010" role="doc-biblioref">2010</a>)</span>. The dataset has <span class="math inline">\(6\)</span> different activities: <em>‘walking’</em>, <em>‘jogging’</em>, <em>‘walking upstairs’</em>, <em>‘walking downstairs’</em>, <em>‘sitting’</em> and <em>‘standing’</em>. The data was collected by <span class="math inline">\(36\)</span> volunteers with an Android phone located in their pant’s pocket and with a sampling rate of <span class="math inline">\(20Hz\)</span> (<span class="math inline">\(1\)</span> sample every <span class="math inline">\(50\)</span> milliseconds).</p>
<p>The dataset contains two types of files. One with the raw accelerometer data and the other one after feature extraction. Figure <a href="classification.html#fig:wisdmFirstLines">2.8</a> shows the first <span class="math inline">\(10\)</span> lines of the raw accelerometer values of the first file. The first column is the id of the user that collected the data and the second column is the class. The third column is the timestamp and the remaining columns are the <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, and <span class="math inline">\(z\)</span> accelerometer values, respectively.</p>
<div class="figure" style="text-align: center"><span id="fig:wisdmFirstLines"></span>
<img src="images/wisdmFirstLines.png" alt="First 10 lines of raw accelerometer data." width="50%" />
<p class="caption">
Figure 2.8: First 10 lines of raw accelerometer data.
</p>
</div>
<p>Usually, classification models are not trained with the raw data but with <em>feature vectors</em> extracted from the raw data. Feature vectors have the advantage of being more compact, thus, making the learning phase more efficient. For activity recognition, the feature extraction process consists of defining a moving window of size <span class="math inline">\(w\)</span> that starts at position <span class="math inline">\(i\)</span>. At the beginning, <span class="math inline">\(i\)</span> is the index pointing to the first accelerometer readings. Then, <span class="math inline">\(n\)</span> statistical features are computed on the elements covered by the window such as mean, standard deviation, <span class="math inline">\(0\)</span>-crossings, etc. This will produce a <span class="math inline">\(n\)</span>-dimensional feature vector and the process is repeated by moving the window <span class="math inline">\(s\)</span> steps forward. Typical values of <span class="math inline">\(s\)</span> are such that the overlap between the previous window position and the next one is about <span class="math inline">\(30\%\)</span> to <span class="math inline">\(50\%\)</span>. An overlap of <span class="math inline">\(0\)</span> is also typical, that is, <span class="math inline">\(s = w\)</span>. Figure <a href="classification.html#fig:featureExtraction">2.9</a> depicts the process.</p>
<div class="figure" style="text-align: center"><span id="fig:featureExtraction"></span>
<img src="images/featureExtraction.png" alt="Moving window for feature extraction." width="70%" />
<p class="caption">
Figure 2.9: Moving window for feature extraction.
</p>
</div>
<p>Once we have the set of feature vectors and their associated class labels, we can use them to train a classifier and make predictions on new data.</p>
<p><img src="images/activitiesPrediction.png" width="90%" style="display: block; margin: auto;" /></p>
<p>For this example, we will use the file with features already extracted. The authors used windows of <span class="math inline">\(10\)</span> seconds which is equivalent to <span class="math inline">\(200\)</span> observations given the <span class="math inline">\(20Hz\)</span> sampling rate and they used <span class="math inline">\(0\%\)</span> overlap. From each window, they extracted <span class="math inline">\(43\)</span> features such as the mean, standard deviation, absolute deviations, etc.</p>
<p>Let’s read and print the first rows of the dataset. The script for this section is <code>smartphone_activities.R</code>. The data frame has several columns, but we only print the first five features and the class which is stored in the last column.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="classification.html#cb30-1"></a><span class="co"># Read data.</span></span>
<span id="cb30-2"><a href="classification.html#cb30-2"></a>df &lt;-<span class="st"> </span><span class="kw">read.csv</span>(datapath,<span class="dt">stringsAsFactors =</span> F)</span>
<span id="cb30-3"><a href="classification.html#cb30-3"></a></span>
<span id="cb30-4"><a href="classification.html#cb30-4"></a><span class="co"># Some code to clean the dataset.</span></span>
<span id="cb30-5"><a href="classification.html#cb30-5"></a><span class="co"># (cleaning code not shown here).</span></span>
<span id="cb30-6"><a href="classification.html#cb30-6"></a></span>
<span id="cb30-7"><a href="classification.html#cb30-7"></a><span class="co"># Print first rows of the dataset.</span></span>
<span id="cb30-8"><a href="classification.html#cb30-8"></a><span class="kw">head</span>(df[,<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,<span class="dv">40</span>)])</span></code></pre></div>
<table>
<caption><span id="tab:activitiesTable">Table 2.2: </span>First rows of activities dataset.</caption>
<thead>
<tr class="header">
<th align="right">X0</th>
<th align="right">X1</th>
<th align="right">X2</th>
<th align="right">X3</th>
<th align="right">X4</th>
<th align="left">class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.04</td>
<td align="right">0.09</td>
<td align="right">0.14</td>
<td align="right">0.12</td>
<td align="right">0.11</td>
<td align="left">Jogging</td>
</tr>
<tr class="even">
<td align="right">0.12</td>
<td align="right">0.12</td>
<td align="right">0.06</td>
<td align="right">0.07</td>
<td align="right">0.11</td>
<td align="left">Jogging</td>
</tr>
<tr class="odd">
<td align="right">0.14</td>
<td align="right">0.09</td>
<td align="right">0.11</td>
<td align="right">0.09</td>
<td align="right">0.09</td>
<td align="left">Jogging</td>
</tr>
<tr class="even">
<td align="right">0.06</td>
<td align="right">0.10</td>
<td align="right">0.09</td>
<td align="right">0.09</td>
<td align="right">0.11</td>
<td align="left">Walking</td>
</tr>
<tr class="odd">
<td align="right">0.12</td>
<td align="right">0.11</td>
<td align="right">0.10</td>
<td align="right">0.08</td>
<td align="right">0.10</td>
<td align="left">Walking</td>
</tr>
<tr class="even">
<td align="right">0.09</td>
<td align="right">0.09</td>
<td align="right">0.10</td>
<td align="right">0.12</td>
<td align="right">0.08</td>
<td align="left">Walking</td>
</tr>
<tr class="odd">
<td align="right">0.12</td>
<td align="right">0.12</td>
<td align="right">0.12</td>
<td align="right">0.13</td>
<td align="right">0.15</td>
<td align="left">Upstairs</td>
</tr>
<tr class="even">
<td align="right">0.10</td>
<td align="right">0.10</td>
<td align="right">0.10</td>
<td align="right">0.10</td>
<td align="right">0.11</td>
<td align="left">Upstairs</td>
</tr>
<tr class="odd">
<td align="right">0.08</td>
<td align="right">0.07</td>
<td align="right">0.08</td>
<td align="right">0.08</td>
<td align="right">0.05</td>
<td align="left">Upstairs</td>
</tr>
</tbody>
</table>
<p>Our aim is to predict the class based on all the numeric features. We will use the <code>rpart</code> package <span class="citation">(Therneau and Atkinson <a href="#ref-rpart" role="doc-biblioref">2019</a>)</span> which implements classification and regression trees. We will assess the performance of the decision tree with <span class="math inline">\(10\)</span>-fold cross-validation. We can use the <code>sample()</code> function to generate the folds. This function will sample <span class="math inline">\(n\)</span> integers from <span class="math inline">\(1\)</span> to <span class="math inline">\(k\)</span> where <span class="math inline">\(n\)</span> is the number of rows in the data frame.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="classification.html#cb31-1"></a><span class="co"># Package with implementations of decision trees.</span></span>
<span id="cb31-2"><a href="classification.html#cb31-2"></a><span class="kw">library</span>(rpart)</span>
<span id="cb31-3"><a href="classification.html#cb31-3"></a></span>
<span id="cb31-4"><a href="classification.html#cb31-4"></a><span class="co"># Set seed for reproducibility.</span></span>
<span id="cb31-5"><a href="classification.html#cb31-5"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb31-6"><a href="classification.html#cb31-6"></a></span>
<span id="cb31-7"><a href="classification.html#cb31-7"></a><span class="co"># Define the number of folds.</span></span>
<span id="cb31-8"><a href="classification.html#cb31-8"></a>k &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb31-9"><a href="classification.html#cb31-9"></a></span>
<span id="cb31-10"><a href="classification.html#cb31-10"></a><span class="co"># Generate folds.</span></span>
<span id="cb31-11"><a href="classification.html#cb31-11"></a>folds &lt;-<span class="st"> </span><span class="kw">sample</span>(k, <span class="dt">size =</span> <span class="kw">nrow</span>(df), <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span>
<span id="cb31-12"><a href="classification.html#cb31-12"></a></span>
<span id="cb31-13"><a href="classification.html#cb31-13"></a><span class="co"># Print first 10 values.</span></span>
<span id="cb31-14"><a href="classification.html#cb31-14"></a><span class="kw">head</span>(folds)</span>
<span id="cb31-15"><a href="classification.html#cb31-15"></a><span class="co">#&gt; [1] 10  6  5  9  5  6</span></span></code></pre></div>
<p>The <code>folds</code> variable stores the fold each instance belongs to. For example, the first instance belongs to fold <span class="math inline">\(1\)</span>, the second instance belongs to fold <span class="math inline">\(6\)</span>, and so on. We can now generate our test and train sets. We will iterate <span class="math inline">\(k=10\)</span> times. For each iteration <span class="math inline">\(i\)</span>, the test set is built using the instances that belong to fold <span class="math inline">\(i\)</span> and the train set will be composed of the remaining instances (those that do not belong to fold <span class="math inline">\(i\)</span>). Next, the <code>rpart()</code> function is used to train the decision tree with the train set. By default, <code>rpart()</code> performs <span class="math inline">\(10\)</span>-fold cross-validation internally. To avoid this, we set the parameter <code>xval = 0</code>. Then, we can use the trained model to obtain the predictions on the test set with the generic <code>predict()</code> function. The ground truth classes and the predictions are stored so the performance metrics can be computed.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="classification.html#cb32-1"></a><span class="co"># Variable to store ground truth classes.</span></span>
<span id="cb32-2"><a href="classification.html#cb32-2"></a>groundTruth &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb32-3"><a href="classification.html#cb32-3"></a></span>
<span id="cb32-4"><a href="classification.html#cb32-4"></a><span class="co"># Variable to store the classifier&#39;s predictions.</span></span>
<span id="cb32-5"><a href="classification.html#cb32-5"></a>predictions &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb32-6"><a href="classification.html#cb32-6"></a></span>
<span id="cb32-7"><a href="classification.html#cb32-7"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k){</span>
<span id="cb32-8"><a href="classification.html#cb32-8"></a>  </span>
<span id="cb32-9"><a href="classification.html#cb32-9"></a>  trainSet &lt;-<span class="st"> </span>df[<span class="kw">which</span>(folds <span class="op">!=</span><span class="st"> </span>i), ]</span>
<span id="cb32-10"><a href="classification.html#cb32-10"></a>  testSet &lt;-<span class="st"> </span>df[<span class="kw">which</span>(folds <span class="op">==</span><span class="st"> </span>i), ]</span>
<span id="cb32-11"><a href="classification.html#cb32-11"></a>  </span>
<span id="cb32-12"><a href="classification.html#cb32-12"></a>  <span class="co"># Train the decision tree</span></span>
<span id="cb32-13"><a href="classification.html#cb32-13"></a>  treeClassifier &lt;-<span class="st"> </span><span class="kw">rpart</span>(class <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb32-14"><a href="classification.html#cb32-14"></a>                          trainSet, <span class="dt">xval=</span><span class="dv">0</span>)</span>
<span id="cb32-15"><a href="classification.html#cb32-15"></a>  </span>
<span id="cb32-16"><a href="classification.html#cb32-16"></a>  <span class="co"># Get predictions on the test set.</span></span>
<span id="cb32-17"><a href="classification.html#cb32-17"></a>  foldPredictions &lt;-<span class="st"> </span><span class="kw">predict</span>(treeClassifier,</span>
<span id="cb32-18"><a href="classification.html#cb32-18"></a>                             testSet, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb32-19"><a href="classification.html#cb32-19"></a>  </span>
<span id="cb32-20"><a href="classification.html#cb32-20"></a>  predictions &lt;-<span class="st"> </span><span class="kw">c</span>(predictions,</span>
<span id="cb32-21"><a href="classification.html#cb32-21"></a>                   <span class="kw">as.character</span>(foldPredictions))</span>
<span id="cb32-22"><a href="classification.html#cb32-22"></a>  </span>
<span id="cb32-23"><a href="classification.html#cb32-23"></a>  groundTruth &lt;-<span class="st"> </span><span class="kw">c</span>(groundTruth,</span>
<span id="cb32-24"><a href="classification.html#cb32-24"></a>                   <span class="kw">as.character</span>(testSet<span class="op">$</span>class))</span>
<span id="cb32-25"><a href="classification.html#cb32-25"></a>}</span></code></pre></div>
<p>Now, we use the <code>confusionMatrix()</code> function to compute the performance metrics and the confusion matrix.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="classification.html#cb33-1"></a>cm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="kw">as.factor</span>(predictions),</span>
<span id="cb33-2"><a href="classification.html#cb33-2"></a>                      <span class="kw">as.factor</span>(groundTruth))</span>
<span id="cb33-3"><a href="classification.html#cb33-3"></a></span>
<span id="cb33-4"><a href="classification.html#cb33-4"></a><span class="co"># Print accuracy</span></span>
<span id="cb33-5"><a href="classification.html#cb33-5"></a>cm<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span>
<span id="cb33-6"><a href="classification.html#cb33-6"></a><span class="co">#&gt; Accuracy </span></span>
<span id="cb33-7"><a href="classification.html#cb33-7"></a><span class="co">#&gt; 0.7895903 </span></span>
<span id="cb33-8"><a href="classification.html#cb33-8"></a></span>
<span id="cb33-9"><a href="classification.html#cb33-9"></a><span class="co"># Print performance metrics per class.</span></span>
<span id="cb33-10"><a href="classification.html#cb33-10"></a>cm<span class="op">$</span>byClass[,<span class="kw">c</span>(<span class="st">&quot;Recall&quot;</span>, <span class="st">&quot;Specificity&quot;</span>, <span class="st">&quot;Precision&quot;</span>, <span class="st">&quot;F1&quot;</span>)]</span>
<span id="cb33-11"><a href="classification.html#cb33-11"></a><span class="co">#&gt;                      Recall Specificity Precision        F1</span></span>
<span id="cb33-12"><a href="classification.html#cb33-12"></a><span class="co">#&gt; Class: Downstairs 0.2821970   0.9617587 0.4434524 0.3449074</span></span>
<span id="cb33-13"><a href="classification.html#cb33-13"></a><span class="co">#&gt; Class: Jogging    0.9612308   0.9601898 0.9118506 0.9358898</span></span>
<span id="cb33-14"><a href="classification.html#cb33-14"></a><span class="co">#&gt; Class: Sitting    0.8366013   0.9984351 0.9696970 0.8982456</span></span>
<span id="cb33-15"><a href="classification.html#cb33-15"></a><span class="co">#&gt; Class: Standing   0.8983740   0.9932328 0.8632812 0.8804781</span></span>
<span id="cb33-16"><a href="classification.html#cb33-16"></a><span class="co">#&gt; Class: Upstairs   0.2246835   0.9669870 0.4733333 0.3047210</span></span>
<span id="cb33-17"><a href="classification.html#cb33-17"></a><span class="co">#&gt; Class: Walking    0.9360884   0.8198981 0.7642213 0.8414687</span></span>
<span id="cb33-18"><a href="classification.html#cb33-18"></a></span>
<span id="cb33-19"><a href="classification.html#cb33-19"></a><span class="co"># Print overall metrics across classes.</span></span>
<span id="cb33-20"><a href="classification.html#cb33-20"></a><span class="kw">colMeans</span>(cm<span class="op">$</span>byClass[,<span class="kw">c</span>(<span class="st">&quot;Recall&quot;</span>, <span class="st">&quot;Specificity&quot;</span>,</span>
<span id="cb33-21"><a href="classification.html#cb33-21"></a>                       <span class="st">&quot;Precision&quot;</span>, <span class="st">&quot;F1&quot;</span>)])</span>
<span id="cb33-22"><a href="classification.html#cb33-22"></a><span class="co">#&gt;     Recall Specificity   Precision          F1 </span></span>
<span id="cb33-23"><a href="classification.html#cb33-23"></a><span class="co">#&gt;  0.6898625   0.9500836   0.7376393   0.7009518 </span></span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:activitiesTreeCM"></span>
<img src="images/activitiesTreeCM.png" alt="Confusion matrix for activities' predictions." width="70%" />
<p class="caption">
Figure 2.10: Confusion matrix for activities’ predictions.
</p>
</div>
<p>The overall accuracy was <span class="math inline">\(78\%\)</span> and by looking at the individual performance metrics, some classes had low scores like <em>‘walking downstairs’</em> and <em>‘walking upstairs’</em>. From the confusion matrix, it can be seen that those two activities were often confused with each other but also with the <em>‘walking’</em> activity. Package <code>rpart.plot</code> <span class="citation">(Milborrow <a href="#ref-rpartplot" role="doc-biblioref">2019</a>)</span> can be used to plot the resulting tree.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="classification.html#cb34-1"></a><span class="kw">library</span>(rpart.plot)</span>
<span id="cb34-2"><a href="classification.html#cb34-2"></a><span class="co"># Plot the tree from the last fold.</span></span>
<span id="cb34-3"><a href="classification.html#cb34-3"></a><span class="kw">rpart.plot</span>(treeClassifier, <span class="dt">fallen.leaves =</span> F,</span>
<span id="cb34-4"><a href="classification.html#cb34-4"></a>           <span class="dt">shadow.col =</span> <span class="st">&quot;gray&quot;</span>, <span class="dt">legend.y =</span> <span class="dv">1</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:activitiesTree"></span>
<img src="images/activitiesTree.png" alt="Resulting decision tree." width="100%" />
<p class="caption">
Figure 2.11: Resulting decision tree.
</p>
</div>
<p>The <code>fallen.leaves = F</code> argument prevents the leaves to be plotted at the bottom. This is useful if the tree has many nodes. Each node shows the predicted class, the predicted probability of each class and the percentage of observations in the node. The plot also shows the feature used for each split. We can see that the <em>YABSOLDEV</em> variable is at the root thus, it had the highest information gain with the initial set of instances. At the root of the tree, before looking at any of the features, the predicted class is <em>‘Walking’</em>. This is because its prior probability is the highest one (<span class="math inline">\(\approx 0.39\)</span>), that is, it’s the most common activity present in the dataset. So, if we didn’t have any other information, our best bet would be to predict the most frequent activity.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="classification.html#cb35-1"></a><span class="co"># Prior probabilities.</span></span>
<span id="cb35-2"><a href="classification.html#cb35-2"></a><span class="kw">table</span>(trainSet<span class="op">$</span>class) <span class="op">/</span><span class="st"> </span><span class="kw">nrow</span>(trainSet)</span>
<span id="cb35-3"><a href="classification.html#cb35-3"></a><span class="co">#&gt; Downstairs    Jogging    Sitting   Standing   Upstairs    Walking </span></span>
<span id="cb35-4"><a href="classification.html#cb35-4"></a><span class="co">#&gt; 0.09882885 0.29607561 0.05506472 0.04705157 0.11793713 0.38504212</span></span></code></pre></div>
<p>These results look promising, but they can still be improved. In the next chapter, I will show you how to improve these results with <em>Ensemble Learning</em> which is a method that is used to aggregate many models.</p>
</div>
</div>
<div id="naive-bayes" class="section level2">
<h2><span class="header-section-number">2.4</span> Naive Bayes</h2>
<p>Naive Bayes is yet another type of classifier. This one is based on Bayes’ rule. The name <em>Naive</em> is because this method assumes that the features are independent. In the previous section we learned that decision trees are built recursively. Trees are built by first selecting a feature to be at the root and then, the root is split into subnodes and so on. How those subnodes are chosen depends on their parent node. With Naive Bayes, features don’t need information about other features, thus, the parameters for each feature can be learned in parallel.</p>
<p>To demonstrate how Naive Bayes works I will use the <em>SMARTPHONE ACTIVITIES</em> dataset as in the previous section. For any given <em>query instance</em>, the aim is to <strong>predict its most likely class</strong> based on the accelerometer features. For a new <em>query instance</em>, we want to estimate its class based on the features that we have observed. Let’s say we want to know what is the probability that the query instance belongs to the class <em>‘Walking’</em>. This can be formulated as follows:</p>
<p><span class="math display">\[\begin{equation*}
  P(C=\textit{Walking} | f_1,\dots ,f_n).
\end{equation*}\]</span></p>
<p>This reads as the conditional probability that the class is <em>‘Walking’</em> <strong>given</strong> the observed evidence. For each instance, the evidence that we can observe are its features <span class="math inline">\(f_1, \dots ,f_n\)</span>. In this dataset, each instance has <span class="math inline">\(39\)</span> features. If we want to estimate what is the most likely class all we need to do is to compute the conditional probability for each class and return the highest one:</p>
<p><span class="math display" id="eq:bayesclassifier">\[\begin{equation}
  y = \operatorname*{arg max}_{k \in \{1, \dots ,K\}} P(C_k | f_1,\dots ,f_n)
  \tag{2.10}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(K\)</span> is the total number of possible classes. The <span class="math inline">\(\text{arg max}\)</span> notation means: Evaluate the right hand expression for every class <span class="math inline">\(k\)</span> and return the <span class="math inline">\(k\)</span> that resulted with the maximum probability. If instead of <span class="math inline">\(\text{arg max}\)</span> we had <span class="math inline">\(\text{max}\)</span> (without the <span class="math inline">\(\text{arg}\)</span>) that would mean to return the actual maximum probability instead of the class <span class="math inline">\(k\)</span>.</p>
<p>Now let’s see how we can compute <span class="math inline">\(P(C_k | f_1,\dots ,f_n)\)</span>. To compute a conditional probability we can use Bayes’ rule:</p>
<p><span class="math display" id="eq:bayesrule">\[\begin{equation}
  P(H|E) = \frac{P(H)P(E|H)}{P(E)}
  \tag{2.11}
\end{equation}\]</span></p>
<p>Let’s dissect that formula:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(P(H|E)\)</span> is called the <strong>posterior</strong> and it is the probability of the hypothesis <span class="math inline">\(H\)</span> given the observed evidence <span class="math inline">\(E\)</span>. In our example, the hypothesis can be that <span class="math inline">\(C=Walking\)</span> and the evidence consists of the measured features. This is the probability that ultimately we want to estimate for each class and pick the class with the highest probability.</p></li>
<li><p><span class="math inline">\(P(H)\)</span> is called the <strong>prior</strong>. This is the probability of a hypothesis happening without having any evidence. In our example, this translates into the probability that an instance belongs to a particular class without looking at its features. In practice, this is estimated from the class counts in the training set. Suppose the training set consists of <span class="math inline">\(100\)</span> instances and from those, <span class="math inline">\(80\)</span> are of type <em>‘Walking’</em> and <span class="math inline">\(20\)</span> are of type <em>‘Jogging’</em>. Then, the prior probability for <em>‘Walking’</em> is <span class="math inline">\(P(C=Walking)=80/100=0.8\)</span> and the prior for <em>‘Jogging’</em> is <span class="math inline">\(P(C=Jogging)=20/100=0.2\)</span>.</p></li>
<li><p><span class="math inline">\(P(E)\)</span> is the probability of the evidence. Since this one doesn’t depend on the class we don’t need to compute it. This can be thought of as a normalization factor. When choosing the final class we only need to select the one with the highest score, so there is no need to normalize them into proper probabilities between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</p></li>
<li><p><span class="math inline">\(P(E|H)\)</span> is called the <strong>likelihood</strong>. For numerical variables we can estimate this using a <em>Gaussian probability density function</em>. This sounds intimidating! but all we need to do is to compute the <em>mean</em> and <em>standard deviation</em> for each feature-class pair and plug them in the probability density function (pdf). The formula for a Gaussian (also called normal) pdf is:</p></li>
</ol>
<p><span class="math display" id="eq:gaussianpdf">\[\begin{equation}
  f(x) = \frac{1}{{\sigma \sqrt {2\pi } }}e^{ - \left( {x - \mu } \right)^2 / 2 \sigma ^2 }
  \tag{2.12}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is the mean and <span class="math inline">\(\sigma\)</span> is the standard deviation.</p>
<p>Suppose that for some feature <span class="math inline">\(f1\)</span> and when the class is <em>‘Walking’</em>, its mean is <span class="math inline">\(5\)</span> and its standard deviation is <span class="math inline">\(3\)</span>. That is, we filter the train set and only select those instances with class <em>‘Walking’</em> and compute the mean and standard deviation for feature <span class="math inline">\(f1\)</span>. Figure <a href="classification.html#fig:pdf1">2.12</a> shows how its pdf looks like.</p>
<div class="figure" style="text-align: center"><span id="fig:pdf1"></span>
<img src="images/pdf1.png" alt="Gaussian probability density function with mean 5 and standard deviation 3." width="90%" />
<p class="caption">
Figure 2.12: Gaussian probability density function with mean 5 and standard deviation 3.
</p>
</div>
<p>If we have a query instance with a feature <span class="math inline">\(f_1 = 1.7\)</span> we can compute its likelihood given the <em>‘Walking’</em> class <span class="math inline">\(P(f_1=1.7|C=Walking)\)</span> with equation <a href="classification.html#eq:gaussianpdf">(2.12)</a> by plugging <span class="math inline">\(x=1.7\)</span>, <span class="math inline">\(\mu=5\)</span> and <span class="math inline">\(\sigma=3\)</span>. In R, the function <code>dnorm()</code> implements the normal pdf.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="classification.html#cb36-1"></a><span class="kw">dnorm</span>(<span class="dt">x=</span><span class="fl">1.7</span>, <span class="dt">mean =</span> <span class="dv">5</span>, <span class="dt">sd =</span> <span class="dv">3</span>)</span>
<span id="cb36-2"><a href="classification.html#cb36-2"></a><span class="co">#&gt; [1] 0.07261739</span></span></code></pre></div>
<p>Figure <a href="classification.html#fig:pdf2">2.13</a> shows the likelihood with a solid circle when <span class="math inline">\(x=1.7\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:pdf2"></span>
<img src="images/pdf2.png" alt="Likelihood (0.072) when x=1.7." width="90%" />
<p class="caption">
Figure 2.13: Likelihood (0.072) when x=1.7.
</p>
</div>
<p>If we have more than one feature we need to compute the likelihood for each and take their <strong>product</strong>: <span class="math inline">\(P(f_1|C=Walking)*P(f_2|C=Walking)*\dots*P(f_n|C=Walking)\)</span>. Each feature and class pair has its own <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> parameters. Thus, Naive Bayes requires to learn <span class="math inline">\(K*F*2\)</span> parameters for the <span class="math inline">\(P(E|H)\)</span> part plus <span class="math inline">\(K\)</span> parameters for the priors <span class="math inline">\(P(H)\)</span>. <span class="math inline">\(K\)</span> is the number of classes, <span class="math inline">\(F\)</span> is the number of features and the <span class="math inline">\(2\)</span> stands for the mean and standard deviation.</p>
<p>We have seen how we can compute <span class="math inline">\(P(C_k|f_1, \dots ,f_n)\)</span> using Baye’s rule by calculating the prior <span class="math inline">\(P(H)\)</span> and <span class="math inline">\(P(E|H)\)</span> which is the product of the likelihoods for each feature. If we substitute Bayes’s rule (omitting the denominator) in equation <a href="classification.html#eq:bayesclassifier">(2.10)</a> we get our Naive Bayes classifier:</p>
<p><span class="math display" id="eq:bayesclassifier2">\[\begin{equation}
  y = \operatorname*{arg max}_{k \in \{1, \dots ,K\}} P(C_k) \prod_{i=1}^{F} P(f_i | C_k)
  \tag{2.13}
\end{equation}\]</span></p>
<p>In the following section we will implement our own Naive Bayes algorithm in R and test it on the <em>SMARTPHONE ACTIVITIES</em> dataset. Then, we will compare our implementation with that of the well known <code>e1071</code> package <span class="citation">(Meyer et al. <a href="#ref-e1071" role="doc-biblioref">2019</a>)</span>.</p>

<div class="rmdgoodpractice">
Naive Bayes works well with missing values since the features are independent. At prediction time, if an instance has one or more missing values then, those features are just ignored and the posterior probability is computed based only on the available variabels. Another advantage of the feature independence assumption is that feature selection algorithms run very fast with Naive Bayes. When building a predictive model, not all features may provide useful information and some features may even degrade the performance. Feature selection algorithms aim to find the best set of features and some of them need to try a huge number of feature combinations. With Naive Bayes, the parameters only need to be learned once and then different combinations of features can be evaluated by ommiting the ones that are not used. With decision trees, for example, we would need to build entire new trees every time we want to try different input features.
</div>


<div class="rmdinfo">
Here, we have shown how we can use a Gaussian pdf to compute the likelihood <span class="math inline">\(P(E|H)\)</span> when the features are numeric. This assumes that the features have a normal distribution however, this is not always the case. In practice, Naive Bayes can work really well even if that assumption is not met. Furthermore, nothing prevents us from using another distribution to estimate the likelihood or even defining a specific distribution for each feature. For categorical variables, <span class="math inline">\(P(E|H)\)</span> is estimated using the feature values frequencies.
</div>

<div id="activity-recognition-with-naive-bayes" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Activity Recognition with Naive Bayes</h3>

<div class="rmdfolder">
<code>naive_bayes.R</code>
</div>

<p>It’s time to implement Naive Bayes. To keep it simple, first we will go through a step by step example using a single feature. Then, we will implement a function to train a Naive Bayes classifier for the case of multiple features.</p>
<p>Let’s assume we have already split the data into train and test sets. The complete code is in the script <code>naive_bayes.R</code>. We will only use the feature <em>RESULTANT</em> which corresponds to the acceleration magnitude of the three axes of the accelerometer sensor. The following code snippet prints the first rows of the train set. The <em>RESULTANT</em> feature is in column <span class="math inline">\(39\)</span> and the class is the last column (<span class="math inline">\(40\)</span>).</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="classification.html#cb37-1"></a><span class="kw">head</span>(trainset[,<span class="kw">c</span>(<span class="dv">39</span><span class="op">:</span><span class="dv">40</span>)])</span>
<span id="cb37-2"><a href="classification.html#cb37-2"></a><span class="co">#&gt;      RESULTANT    class</span></span>
<span id="cb37-3"><a href="classification.html#cb37-3"></a><span class="co">#&gt; 1004     11.14  Walking</span></span>
<span id="cb37-4"><a href="classification.html#cb37-4"></a><span class="co">#&gt; 623       1.24 Upstairs</span></span>
<span id="cb37-5"><a href="classification.html#cb37-5"></a><span class="co">#&gt; 2693      9.90 Standing</span></span>
<span id="cb37-6"><a href="classification.html#cb37-6"></a><span class="co">#&gt; 934      10.44 Upstairs</span></span>
<span id="cb37-7"><a href="classification.html#cb37-7"></a><span class="co">#&gt; 4496     10.43  Walking</span></span>
<span id="cb37-8"><a href="classification.html#cb37-8"></a><span class="co">#&gt; 2948     15.28  Jogging</span></span></code></pre></div>
<p>First, we compute the prior probabilities for each class in the train set and store them in the variable <code>priors</code>. This corresponds to the <span class="math inline">\(P(C_k)\)</span> part in equation <a href="classification.html#eq:bayesclassifier2">(2.13)</a>.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="classification.html#cb38-1"></a><span class="co"># Compute prior probabilities.</span></span>
<span id="cb38-2"><a href="classification.html#cb38-2"></a>priors &lt;-<span class="st"> </span><span class="kw">table</span>(trainset<span class="op">$</span>class) <span class="op">/</span><span class="st"> </span><span class="kw">nrow</span>(trainset)</span>
<span id="cb38-3"><a href="classification.html#cb38-3"></a></span>
<span id="cb38-4"><a href="classification.html#cb38-4"></a><span class="co"># Print the table of priors.</span></span>
<span id="cb38-5"><a href="classification.html#cb38-5"></a>priors</span>
<span id="cb38-6"><a href="classification.html#cb38-6"></a></span>
<span id="cb38-7"><a href="classification.html#cb38-7"></a><span class="co">#&gt; Downstairs    Jogging    Sitting   Standing   Upstairs </span></span>
<span id="cb38-8"><a href="classification.html#cb38-8"></a><span class="co">#&gt; 0.09622990 0.30266280 0.05721065 0.04640127 0.11521223 </span></span>
<span id="cb38-9"><a href="classification.html#cb38-9"></a><span class="co">#&gt;    Walking </span></span>
<span id="cb38-10"><a href="classification.html#cb38-10"></a><span class="co">#&gt; 0.38228315 </span></span></code></pre></div>
<p>We can access each prior by name like this:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="classification.html#cb39-1"></a><span class="co"># Get the prior for &quot;Jogging&quot;.</span></span>
<span id="cb39-2"><a href="classification.html#cb39-2"></a>priors[<span class="st">&quot;Jogging&quot;</span>]</span>
<span id="cb39-3"><a href="classification.html#cb39-3"></a><span class="co">#&gt;   Jogging </span></span>
<span id="cb39-4"><a href="classification.html#cb39-4"></a><span class="co">#&gt; 0.3026628 </span></span></code></pre></div>
<p>This means that <span class="math inline">\(30\%\)</span> of the instances in the train set are of type <em>‘Jogging’</em>. Now we need to compute the <span class="math inline">\(P(f_i|C_k)\)</span> part from equation <a href="classification.html#eq:bayesclassifier2">(2.13)</a>. We can define a method to compute the probability density function in equation <a href="classification.html#eq:gaussianpdf">(2.12)</a>:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="classification.html#cb40-1"></a><span class="co"># Probability density function of normal distribution.</span></span>
<span id="cb40-2"><a href="classification.html#cb40-2"></a>f &lt;-<span class="st"> </span><span class="cf">function</span>(x, m, s){</span>
<span id="cb40-3"><a href="classification.html#cb40-3"></a>  (<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="kw">sqrt</span>(<span class="dv">2</span><span class="op">*</span>pi)<span class="op">*</span>s)) <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>((x<span class="op">-</span>m)<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>s<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb40-4"><a href="classification.html#cb40-4"></a>}</span></code></pre></div>
<p>It’s first argument <code>x</code> is the input value. The second argument <code>m</code> is the mean and the last argument <code>s</code> is the standard deviation. For illustration purposes we are defining this function manually but remember that this pdf is already implemented with the base <code>dnorm()</code> function.</p>
<p>According to equation <a href="classification.html#eq:bayesclassifier2">(2.13)</a> we need to compute <span class="math inline">\(P(f_i|C_k)\)</span> for each feature <span class="math inline">\(i\)</span> and class <span class="math inline">\(k\)</span>. Let’s assume there are only two classes, <em>‘Walking’</em> and <em>‘Jogging’</em> thus, we need the mean and standard deviation for each and for the feature <em>RESULTANT</em> (column <span class="math inline">\(39\)</span>).</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="classification.html#cb41-1"></a><span class="co"># Compute the mean and sd of</span></span>
<span id="cb41-2"><a href="classification.html#cb41-2"></a><span class="co"># the feature RESULTANT (column 39)</span></span>
<span id="cb41-3"><a href="classification.html#cb41-3"></a><span class="co"># when the class = &quot;Standing&quot;.</span></span>
<span id="cb41-4"><a href="classification.html#cb41-4"></a>mean.standing &lt;-<span class="st"> </span><span class="kw">mean</span>(trainset[<span class="kw">which</span>(trainset<span class="op">$</span>class <span class="op">==</span><span class="st"> &quot;Standing&quot;</span>), <span class="dv">39</span>])</span>
<span id="cb41-5"><a href="classification.html#cb41-5"></a>sd.standing &lt;-<span class="st"> </span><span class="kw">sd</span>(trainset[<span class="kw">which</span>(trainset<span class="op">$</span>class <span class="op">==</span><span class="st"> &quot;Standing&quot;</span>), <span class="dv">39</span>])</span>
<span id="cb41-6"><a href="classification.html#cb41-6"></a></span>
<span id="cb41-7"><a href="classification.html#cb41-7"></a><span class="co"># Compute mean and sd when</span></span>
<span id="cb41-8"><a href="classification.html#cb41-8"></a><span class="co"># the class = &quot;Jogging&quot;.</span></span>
<span id="cb41-9"><a href="classification.html#cb41-9"></a>mean.jogging &lt;-<span class="st"> </span><span class="kw">mean</span>(trainset[<span class="kw">which</span>(trainset<span class="op">$</span>class <span class="op">==</span><span class="st"> &quot;Jogging&quot;</span>), <span class="dv">39</span>])</span>
<span id="cb41-10"><a href="classification.html#cb41-10"></a>sd.jogging &lt;-<span class="st"> </span><span class="kw">sd</span>(trainset[<span class="kw">which</span>(trainset<span class="op">$</span>class <span class="op">==</span><span class="st"> &quot;Jogging&quot;</span>), <span class="dv">39</span>])</span></code></pre></div>
<p>We can print the means:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="classification.html#cb42-1"></a>mean.standing</span>
<span id="cb42-2"><a href="classification.html#cb42-2"></a><span class="co">#&gt; [1] 9.405795</span></span>
<span id="cb42-3"><a href="classification.html#cb42-3"></a>mean.jogging</span>
<span id="cb42-4"><a href="classification.html#cb42-4"></a><span class="co">#&gt; [1] 13.70145</span></span></code></pre></div>
<p>We can see that the mean value for <em>‘Jogging’</em> is higher for this feature. This was expected since this feature captures the overall movement across all axes. Now we have everything we need to start making predictions on new instances. We have the priors and we have the means and standard deviations for each feature-class pair.</p>
<p>Let’s select the first instance from the test set and try to predict its class.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="classification.html#cb43-1"></a><span class="co"># Select a query instance from the test set.</span></span>
<span id="cb43-2"><a href="classification.html#cb43-2"></a>query &lt;-<span class="st"> </span>testset[<span class="dv">1</span>,] <span class="co"># Select the first one.</span></span></code></pre></div>
<p>Now we compute the posterior probability for each class using the learned means and standard deviations:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="classification.html#cb44-1"></a><span class="co"># Compute P(Standing)P(RESULTANT|Standing)</span></span>
<span id="cb44-2"><a href="classification.html#cb44-2"></a>priors[<span class="st">&quot;Standing&quot;</span>] <span class="op">*</span><span class="st"> </span><span class="kw">f</span>(query<span class="op">$</span>RESULTANT, mean.standing, sd.standing)</span>
<span id="cb44-3"><a href="classification.html#cb44-3"></a><span class="co">#&gt; 0.003169748</span></span>
<span id="cb44-4"><a href="classification.html#cb44-4"></a></span>
<span id="cb44-5"><a href="classification.html#cb44-5"></a><span class="co"># Compute P(Jogging)P(RESULTANT|Jogging)</span></span>
<span id="cb44-6"><a href="classification.html#cb44-6"></a>priors[<span class="st">&quot;Jogging&quot;</span>] <span class="op">*</span><span class="st"> </span><span class="kw">f</span>(query<span class="op">$</span>RESULTANT, mean.jogging, sd.jogging)</span>
<span id="cb44-7"><a href="classification.html#cb44-7"></a><span class="co">#&gt; 0.03884481</span></span></code></pre></div>
<p>The posterior for <em>‘Jogging’</em> was higher (<span class="math inline">\(0.038\)</span>) so we classify the query instance as <em>‘Jogging’</em>. If we check the true class we see that it was correctly classified!</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="classification.html#cb45-1"></a><span class="co"># Inspect the true class of the query instance.</span></span>
<span id="cb45-2"><a href="classification.html#cb45-2"></a>query<span class="op">$</span>class</span>
<span id="cb45-3"><a href="classification.html#cb45-3"></a><span class="co">#&gt; [1] &quot;Jogging&quot;</span></span></code></pre></div>
<p>In this example we assumed that there was only one feature and we computed each step manually. However, this can easily be extended to deal with more features. So let’s just do that. We can write two functions, one for training the classifier and the other one for making predictions.</p>
<p>The following function will be used to train the classifier. It takes as input a data frame with <span class="math inline">\(n\)</span> features. This function assumes that the class is the last column. The function returns a list with the learned priors, means, and standard deviations.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="classification.html#cb46-1"></a><span class="co"># Function to learn the parameters of </span></span>
<span id="cb46-2"><a href="classification.html#cb46-2"></a><span class="co"># a Naive Bayes classifier.</span></span>
<span id="cb46-3"><a href="classification.html#cb46-3"></a><span class="co"># Assumes that the last column of data is the class.</span></span>
<span id="cb46-4"><a href="classification.html#cb46-4"></a>naive.bayes.train &lt;-<span class="st"> </span><span class="cf">function</span>(data){</span>
<span id="cb46-5"><a href="classification.html#cb46-5"></a></span>
<span id="cb46-6"><a href="classification.html#cb46-6"></a>  <span class="co"># Unique classes.</span></span>
<span id="cb46-7"><a href="classification.html#cb46-7"></a>  classes &lt;-<span class="st"> </span><span class="kw">unique</span>(data<span class="op">$</span>class)</span>
<span id="cb46-8"><a href="classification.html#cb46-8"></a></span>
<span id="cb46-9"><a href="classification.html#cb46-9"></a>  <span class="co"># Number of features.</span></span>
<span id="cb46-10"><a href="classification.html#cb46-10"></a>  nfeatures &lt;-<span class="st"> </span><span class="kw">ncol</span>(data) <span class="op">-</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb46-11"><a href="classification.html#cb46-11"></a>  </span>
<span id="cb46-12"><a href="classification.html#cb46-12"></a>  <span class="co"># List to store the learned means and sds.</span></span>
<span id="cb46-13"><a href="classification.html#cb46-13"></a>  list.means.sds &lt;-<span class="st"> </span><span class="kw">list</span>()</span>
<span id="cb46-14"><a href="classification.html#cb46-14"></a>  </span>
<span id="cb46-15"><a href="classification.html#cb46-15"></a>  <span class="cf">for</span>(c <span class="cf">in</span> classes){</span>
<span id="cb46-16"><a href="classification.html#cb46-16"></a>    </span>
<span id="cb46-17"><a href="classification.html#cb46-17"></a>    <span class="co"># Matrix to store the mean and sd for each feature.</span></span>
<span id="cb46-18"><a href="classification.html#cb46-18"></a>    <span class="co"># First column stores the mean and second column</span></span>
<span id="cb46-19"><a href="classification.html#cb46-19"></a>    <span class="co"># stores the sd.</span></span>
<span id="cb46-20"><a href="classification.html#cb46-20"></a>    M &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> nfeatures, <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb46-21"><a href="classification.html#cb46-21"></a>    </span>
<span id="cb46-22"><a href="classification.html#cb46-22"></a>    <span class="co"># Populate matrix.</span></span>
<span id="cb46-23"><a href="classification.html#cb46-23"></a>    <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nfeatures){</span>
<span id="cb46-24"><a href="classification.html#cb46-24"></a>      feature.values &lt;-<span class="st"> </span>data[<span class="kw">which</span>(data<span class="op">$</span>class <span class="op">==</span><span class="st"> </span>c),i]</span>
<span id="cb46-25"><a href="classification.html#cb46-25"></a>      M[i,<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">mean</span>(feature.values)</span>
<span id="cb46-26"><a href="classification.html#cb46-26"></a>      M[i,<span class="dv">2</span>] &lt;-<span class="st"> </span><span class="kw">sd</span>(feature.values)</span>
<span id="cb46-27"><a href="classification.html#cb46-27"></a>    }</span>
<span id="cb46-28"><a href="classification.html#cb46-28"></a>    </span>
<span id="cb46-29"><a href="classification.html#cb46-29"></a>    list.means.sds[c] &lt;-<span class="st"> </span><span class="kw">list</span>(M)</span>
<span id="cb46-30"><a href="classification.html#cb46-30"></a>  }</span>
<span id="cb46-31"><a href="classification.html#cb46-31"></a></span>
<span id="cb46-32"><a href="classification.html#cb46-32"></a>  <span class="co"># Compute prior probabilities.</span></span>
<span id="cb46-33"><a href="classification.html#cb46-33"></a>  priors &lt;-<span class="st"> </span><span class="kw">table</span>(data<span class="op">$</span>class) <span class="op">/</span><span class="st"> </span><span class="kw">nrow</span>(data)</span>
<span id="cb46-34"><a href="classification.html#cb46-34"></a>  </span>
<span id="cb46-35"><a href="classification.html#cb46-35"></a>  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">list.means.sds=</span>list.means.sds,</span>
<span id="cb46-36"><a href="classification.html#cb46-36"></a>              <span class="dt">priors=</span>priors))</span>
<span id="cb46-37"><a href="classification.html#cb46-37"></a>}</span></code></pre></div>
<p>The function iterates through each class and for each, it creates a matrix <code>M</code> with <span class="math inline">\(F\)</span> rows and <span class="math inline">\(2\)</span> columns where <span class="math inline">\(F\)</span> is the number of features. The first column stores the means and the second the standard deviations. Those matrices are saved in a list indexed by the class name so at prediction time we can retrieve each matrix individually. At the end, the prior probabilities are computed. Finally, a list is returned. The first element of the list is the list of matrices and the second element are the priors.</p>
<p>The next function will make predictions based on the learned parameters. Its first argument is the learned parameters and the second parameter is a data frame with the instances we want to make predictions for.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="classification.html#cb47-1"></a><span class="co"># Function to make predictions using</span></span>
<span id="cb47-2"><a href="classification.html#cb47-2"></a><span class="co"># the learned parameters.</span></span>
<span id="cb47-3"><a href="classification.html#cb47-3"></a>naive.bayes.predict &lt;-<span class="st"> </span><span class="cf">function</span>(params, data){</span>
<span id="cb47-4"><a href="classification.html#cb47-4"></a>  </span>
<span id="cb47-5"><a href="classification.html#cb47-5"></a>  <span class="co"># Variable to store the prediction for each instance.</span></span>
<span id="cb47-6"><a href="classification.html#cb47-6"></a>  predictions &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb47-7"><a href="classification.html#cb47-7"></a>  </span>
<span id="cb47-8"><a href="classification.html#cb47-8"></a>  n &lt;-<span class="st"> </span><span class="kw">nrow</span>(data)</span>
<span id="cb47-9"><a href="classification.html#cb47-9"></a>  </span>
<span id="cb47-10"><a href="classification.html#cb47-10"></a>  <span class="co"># Get class names.</span></span>
<span id="cb47-11"><a href="classification.html#cb47-11"></a>  classes &lt;-<span class="st"> </span><span class="kw">names</span>(params<span class="op">$</span>priors)</span>
<span id="cb47-12"><a href="classification.html#cb47-12"></a>  </span>
<span id="cb47-13"><a href="classification.html#cb47-13"></a>  <span class="co"># Get number of features.</span></span>
<span id="cb47-14"><a href="classification.html#cb47-14"></a>  nfeatures &lt;-<span class="st"> </span><span class="kw">nrow</span>(params<span class="op">$</span>list.means.sds[[<span class="dv">1</span>]])</span>
<span id="cb47-15"><a href="classification.html#cb47-15"></a>  </span>
<span id="cb47-16"><a href="classification.html#cb47-16"></a>  <span class="co"># Iterate instances.</span></span>
<span id="cb47-17"><a href="classification.html#cb47-17"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n){</span>
<span id="cb47-18"><a href="classification.html#cb47-18"></a>    </span>
<span id="cb47-19"><a href="classification.html#cb47-19"></a>    query &lt;-<span class="st"> </span>data[i,]</span>
<span id="cb47-20"><a href="classification.html#cb47-20"></a>    </span>
<span id="cb47-21"><a href="classification.html#cb47-21"></a>    max.probability &lt;-<span class="st"> </span><span class="op">-</span><span class="ot">Inf</span></span>
<span id="cb47-22"><a href="classification.html#cb47-22"></a>    </span>
<span id="cb47-23"><a href="classification.html#cb47-23"></a>    predicted.class &lt;-<span class="st"> &quot;&quot;</span></span>
<span id="cb47-24"><a href="classification.html#cb47-24"></a>    </span>
<span id="cb47-25"><a href="classification.html#cb47-25"></a>    <span class="co"># Find the class with highest probability.</span></span>
<span id="cb47-26"><a href="classification.html#cb47-26"></a>    <span class="cf">for</span>(c <span class="cf">in</span> classes){</span>
<span id="cb47-27"><a href="classification.html#cb47-27"></a></span>
<span id="cb47-28"><a href="classification.html#cb47-28"></a>      <span class="co"># Get the prior probability for class c.</span></span>
<span id="cb47-29"><a href="classification.html#cb47-29"></a>      acum.prob &lt;-<span class="st"> </span>params<span class="op">$</span>priors[c]</span>
<span id="cb47-30"><a href="classification.html#cb47-30"></a>      </span>
<span id="cb47-31"><a href="classification.html#cb47-31"></a>      <span class="co"># Iterate features.</span></span>
<span id="cb47-32"><a href="classification.html#cb47-32"></a>      <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nfeatures){</span>
<span id="cb47-33"><a href="classification.html#cb47-33"></a>        </span>
<span id="cb47-34"><a href="classification.html#cb47-34"></a>        <span class="co"># Compute P(feature|class)</span></span>
<span id="cb47-35"><a href="classification.html#cb47-35"></a>        tmp &lt;-<span class="st"> </span><span class="kw">f</span>(query[,j],</span>
<span id="cb47-36"><a href="classification.html#cb47-36"></a>          params<span class="op">$</span>list.means.sds[[c]][j,<span class="dv">1</span>],</span>
<span id="cb47-37"><a href="classification.html#cb47-37"></a>          params<span class="op">$</span>list.means.sds[[c]][j,<span class="dv">2</span>])</span>
<span id="cb47-38"><a href="classification.html#cb47-38"></a>        </span>
<span id="cb47-39"><a href="classification.html#cb47-39"></a>        <span class="co"># Accumulate result.</span></span>
<span id="cb47-40"><a href="classification.html#cb47-40"></a>        acum.prob &lt;-<span class="st"> </span>acum.prob <span class="op">*</span><span class="st"> </span>tmp</span>
<span id="cb47-41"><a href="classification.html#cb47-41"></a>      }</span>
<span id="cb47-42"><a href="classification.html#cb47-42"></a>      </span>
<span id="cb47-43"><a href="classification.html#cb47-43"></a>      <span class="cf">if</span>(acum.prob <span class="op">&gt;</span><span class="st"> </span>max.probability){</span>
<span id="cb47-44"><a href="classification.html#cb47-44"></a>        max.probability &lt;-<span class="st"> </span>acum.prob</span>
<span id="cb47-45"><a href="classification.html#cb47-45"></a>        predicted.class &lt;-<span class="st"> </span>c</span>
<span id="cb47-46"><a href="classification.html#cb47-46"></a>      }</span>
<span id="cb47-47"><a href="classification.html#cb47-47"></a>    }</span>
<span id="cb47-48"><a href="classification.html#cb47-48"></a>    </span>
<span id="cb47-49"><a href="classification.html#cb47-49"></a>    predictions &lt;-<span class="st"> </span><span class="kw">c</span>(predictions, predicted.class)</span>
<span id="cb47-50"><a href="classification.html#cb47-50"></a>  }</span>
<span id="cb47-51"><a href="classification.html#cb47-51"></a>  </span>
<span id="cb47-52"><a href="classification.html#cb47-52"></a>  <span class="kw">return</span>(predictions)</span>
<span id="cb47-53"><a href="classification.html#cb47-53"></a>}</span></code></pre></div>
<p>This function iterates through each instance and computes the posterior for each class and stores the one that achieved the highest value as the prediction. Finally, it returns the list with all predictions.</p>
<p>We are ready to train our Naive Bayes classifier. All we need to do is call the function <code>naive.bayes.train()</code> and pass the train set.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="classification.html#cb48-1"></a><span class="co"># Learn Naive Bayes parameters.</span></span>
<span id="cb48-2"><a href="classification.html#cb48-2"></a>nb.model &lt;-<span class="st"> </span><span class="kw">naive.bayes.train</span>(trainset)</span></code></pre></div>
<p>The learned parameters are stored in <code>nb.model</code> and we can make predictions with the <code>naive.bayes.predict()</code> function by passing the <code>nb.model</code> and a test set.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="classification.html#cb49-1"></a><span class="co"># Make predictions.</span></span>
<span id="cb49-2"><a href="classification.html#cb49-2"></a>predictions &lt;-<span class="st"> </span><span class="kw">naive.bayes.predict</span>(nb.model, testset)</span></code></pre></div>
<p>Then, we assess the performance of the model by computing the confusion matrix.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="classification.html#cb50-1"></a><span class="co"># Compute confusion matrix and other performance metrics.</span></span>
<span id="cb50-2"><a href="classification.html#cb50-2"></a>groundTruth &lt;-<span class="st"> </span>testset<span class="op">$</span>class</span>
<span id="cb50-3"><a href="classification.html#cb50-3"></a></span>
<span id="cb50-4"><a href="classification.html#cb50-4"></a>cm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="kw">as.factor</span>(predictions),</span>
<span id="cb50-5"><a href="classification.html#cb50-5"></a>                      <span class="kw">as.factor</span>(groundTruth))</span>
<span id="cb50-6"><a href="classification.html#cb50-6"></a></span>
<span id="cb50-7"><a href="classification.html#cb50-7"></a><span class="co"># Print accuracy</span></span>
<span id="cb50-8"><a href="classification.html#cb50-8"></a>cm<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span>
<span id="cb50-9"><a href="classification.html#cb50-9"></a><span class="co">#&gt;  Accuracy </span></span>
<span id="cb50-10"><a href="classification.html#cb50-10"></a><span class="co">#&gt; 0.7501538</span></span>
<span id="cb50-11"><a href="classification.html#cb50-11"></a></span>
<span id="cb50-12"><a href="classification.html#cb50-12"></a><span class="co"># Print overall metrics across classes.</span></span>
<span id="cb50-13"><a href="classification.html#cb50-13"></a><span class="kw">colMeans</span>(cm<span class="op">$</span>byClass[,<span class="kw">c</span>(<span class="st">&quot;Recall&quot;</span>, <span class="st">&quot;Specificity&quot;</span>,</span>
<span id="cb50-14"><a href="classification.html#cb50-14"></a>                       <span class="st">&quot;Precision&quot;</span>, <span class="st">&quot;F1&quot;</span>)])</span>
<span id="cb50-15"><a href="classification.html#cb50-15"></a><span class="co">#&gt;      Recall Specificity   Precision          F1 </span></span>
<span id="cb50-16"><a href="classification.html#cb50-16"></a><span class="co">#&gt;   0.6621381   0.9423729   0.6468372   0.6433231</span></span></code></pre></div>
<p>The accuracy was <span class="math inline">\(75\%\)</span>. In the previous section we obtained an accuracy of <span class="math inline">\(78\%\)</span> with decision trees. However, this does not necessarily mean that decision trees are better. Moreover, in the previous section we used cross-validation and here we used hold-out validation.</p>

<div class="rmdcaution">
One important thing to note is that computing the posterior may cause a loss of numeric precision specially, when there are many features. This is because we are multiplying the likelihoods for each feature (see equation <a href="classification.html#eq:bayesclassifier2">(2.13)</a>) and those likelihoods are small numbers. One way to fix that is to use logarithms. In <code>navie.bayes.predict()</code> we can change <code>acum.prob &lt;- params$priors[c]</code> with <code>acum.prob &lt;- log(params$priors[c])</code> and <code>acum.prob &lt;- acum.prob * tmp</code> with <code>acum.prob &lt;- acum.prob + log(tmp)</code>. If you try those changes you should get the same result as before.
</div>

<p>There is already a popular R package (<code>e1071</code>) for training Naive Bayes classifiers. The following code trains a classifier using this package.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="classification.html#cb51-1"></a><span class="co">#### Use Naive Bayes implementation from package e1071 ####</span></span>
<span id="cb51-2"><a href="classification.html#cb51-2"></a><span class="kw">library</span>(e1071)</span>
<span id="cb51-3"><a href="classification.html#cb51-3"></a></span>
<span id="cb51-4"><a href="classification.html#cb51-4"></a><span class="co"># We need to convert the class into a factor.</span></span>
<span id="cb51-5"><a href="classification.html#cb51-5"></a>trainset<span class="op">$</span>class &lt;-<span class="st"> </span><span class="kw">as.factor</span>(trainset<span class="op">$</span>class)</span>
<span id="cb51-6"><a href="classification.html#cb51-6"></a></span>
<span id="cb51-7"><a href="classification.html#cb51-7"></a>nb.model2 &lt;-<span class="st"> </span><span class="kw">naiveBayes</span>(class <span class="op">~</span>., trainset)</span>
<span id="cb51-8"><a href="classification.html#cb51-8"></a></span>
<span id="cb51-9"><a href="classification.html#cb51-9"></a>predictions2 &lt;-<span class="st"> </span><span class="kw">predict</span>(nb.model2, testset)</span>
<span id="cb51-10"><a href="classification.html#cb51-10"></a></span>
<span id="cb51-11"><a href="classification.html#cb51-11"></a>cm2 &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="kw">as.factor</span>(predictions2),</span>
<span id="cb51-12"><a href="classification.html#cb51-12"></a>                      <span class="kw">as.factor</span>(groundTruth))</span>
<span id="cb51-13"><a href="classification.html#cb51-13"></a></span>
<span id="cb51-14"><a href="classification.html#cb51-14"></a><span class="co"># Print accuracy</span></span>
<span id="cb51-15"><a href="classification.html#cb51-15"></a>cm2<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]</span>
<span id="cb51-16"><a href="classification.html#cb51-16"></a><span class="co">#&gt;  Accuracy </span></span>
<span id="cb51-17"><a href="classification.html#cb51-17"></a><span class="co">#&gt; 0.7501538</span></span></code></pre></div>
<p>As you can see, the result was the same as the one obtained with our implementation! We implemented our own for illustrative purposes but it is advisable to use already tested and proven packages. Furthermore, this one also supports categorical variables.</p>
</div>
</div>
<div id="dynamic-time-warping" class="section level2">
<h2><span class="header-section-number">2.5</span> Dynamic Time Warping</h2>

<div class="rmdfolder">
<code>dtw_example.R</code>
</div>

<p>In the previous activity recognition example, we used the extracted features represented as feature vectors to train the classifiers instead of using the raw data. This can lead to temporal information loss. In the previous example, we could classify the activities with reasonable accuracy since the extracted features were able to retain enough information from the raw data. However, in some cases, the need to retain temporal information is crucial. For example, in hand signature recognition, one needs to check if a query signature matches one of the signatures from a database. The signatures need to have an almost exact match to authenticate a user, i.e, they need to look very similar. If we represent each signature as a feature vector, it can turn out that two signatures have very similar feature vectors even though they look completely different. For example, Figure <a href="classification.html#fig:correlations">2.14</a> shows four datasets. They look very different but they all have the same correlation of <span class="math inline">\(0.816\)</span><a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>.</p>
<div class="figure" style="text-align: center"><span id="fig:correlations"></span>
<img src="images/correlations.png" alt="Four datasets with the same correlation of 0.816." width="90%" />
<p class="caption">
Figure 2.14: Four datasets with the same correlation of 0.816.
</p>
</div>
<p>To avoid this potential issue, we can also include time-dependent information into our models by keeping the order of the data points. Another issue is that two time series that belong to the same class will still have some differences. Every time the same person signs a document the signature will vary a bit. In the same way, when we pronounce a word, sometimes we emphasize some letters or speak at different speeds. Figure <a href="classification.html#fig:verygood">2.15</a> shows two versions of the sentence “very good”. In the second one (bottom) the speaker emphasizes the “e” and as a result, the two sentences are not aligned in time anymore but they have the same meaning.</p>
<div class="figure" style="text-align: center"><span id="fig:verygood"></span>
<img src="images/verygood.png" alt="Time shift example between two sentences." width="30%" />
<p class="caption">
Figure 2.15: Time shift example between two sentences.
</p>
</div>
<p>If we wanted to compare two sequences we could use the well known Euclidean distance. However since the two sequences may not be aligned in time, the result could be misleading. To account for this “time-shift” effect in time series data we can use <em>Dynamic Time Warping</em> (DTW) <span class="citation">(Sakoe et al. <a href="#ref-sakoe1990dynamic" role="doc-biblioref">1990</a>)</span> when comparing two time series. DTW is a method that:</p>
<ul>
<li>Finds an optimal match between two given time-dependent sequences.</li>
<li>Computes the dissimilarity between the sequences.</li>
<li>Finds the optimal deformation of one of the sequences onto the other.</li>
</ul>
<p>Another advantage of DTW is that the time series do not need to be of the same length. Suppose we have two time series, a <em>query</em>, and a <em>reference</em> we want to compare with:</p>
<p><span class="math display">\[\begin{align*}
query&amp;=(2,2,2,4,4,3)\\
ref&amp;=(2,2,3,3,2)
\end{align*}\]</span></p>
<p>The first thing to note is that both sequences differ in length. Figure <a href="classification.html#fig:queryref">2.16</a> shows a plot with the two sequences. The <em>query</em> is the solid line. It can be seen that the <em>query</em> seems to be shifted to the right one position with respect to the <em>reference</em>. The plot also shows the resulting alignment after applying the DTW algorithm (dashed lines between the sequences). The resulting distance between the sequences is <span class="math inline">\(3\)</span>. In the following, we will see how the problem can be formalized and the steps to compute that final distance of <span class="math inline">\(3\)</span>. Don’t worry if you find the math notation a bit difficult to grasp at this pint. A step by step example will follow which should help to explain how the method works.</p>
<div class="figure" style="text-align: center"><span id="fig:queryref"></span>
<img src="images/query_ref.png" alt="DTW alignment between the query and reference sequences (Solid line is the query)." width="100%" />
<p class="caption">
Figure 2.16: DTW alignment between the query and reference sequences (Solid line is the query).
</p>
</div>
<p>The problem of aligning two sequences can be formalized as follows <span class="citation">(Rabiner and Juang <a href="#ref-Rabiner1993" role="doc-biblioref">1993</a>)</span>. Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two sequences:</p>
<p><span class="math display">\[\begin{align*}
X&amp;=(x_1,x_2,\dots,x_{T_x}) \\
Y&amp;=(y_1,y_2,\dots,y_{T_y})
\end{align*}\]</span></p>
<p>where <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> are vectors. In the previous example, the vectors only have one element since the sequences are <span class="math inline">\(1\)</span>-dimensional but DTW also works with multidimensional sequences. <span class="math inline">\(T_x\)</span> and <span class="math inline">\(T_y\)</span> are the sequences’ lengths. Let</p>
<p><span class="math display">\[\begin{align*}
d(i_x,i_y)
\end{align*}\]</span></p>
<p>be the <em>dissimilarity</em> (distance) between vectors <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> (e.g., Euclidean distance). Then, <span class="math inline">\(\phi_x\)</span> and <span class="math inline">\(\phi_y\)</span> are the warping functions that relate <span class="math inline">\(i_x\)</span> and <span class="math inline">\(i_y\)</span> to a common axis <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[\begin{align*}
i_x&amp;=\phi_x (k), k=1,2,\dots,T \\
i_y&amp;=\phi_y (k), k=1,2,\dots,T.
\end{align*}\]</span></p>
<p>The total dissimilarity between the two sequences is:</p>
<p><span class="math display" id="eq:dtwDissimilarity">\[\begin{equation}
  d_\phi (X,Y) = \sum_{k=1}^T{d\left(\phi_x (k), \phi_y (k)\right)}
  \tag{2.14}
\end{equation}\]</span></p>
<p>The aim is to find the warping function <span class="math inline">\(\phi\)</span> that minimizes the total dissimilarity:</p>
<p><span class="math display" id="eq:dtwmin">\[\begin{equation}
  \operatorname*{min}_{\phi} d_\phi (X,Y)
  \tag{2.15}
\end{equation}\]</span></p>
<p>The solution can be efficiently computed using dynamic programming. Usually, when solving this minimization problem, some constraints are applied:</p>
<ul>
<li><strong>Endpoint constraints.</strong> This constraint makes sure that the first and last elements of each sequence are connected.</li>
</ul>
<p><span class="math display">\[\begin{align*}
\phi_x (1)&amp;=1, \phi_y (1)=1 \\
\phi_x (T)&amp;=T_x, \phi_y (T)=T_y
\end{align*}\]</span></p>
<ul>
<li><strong>Monotonicity.</strong> This constraint allows ‘time to flow’ only from left to right. That is, we can not go back in time.</li>
</ul>
<p><span class="math display">\[\begin{align*}
\phi_x (k+1) \geq \phi_x(k) \\
\phi_y (k+1) \geq \phi_y(k)
\end{align*}\]</span></p>
<ul>
<li><strong>Local constraints.</strong> For example, allow jumps of at most <span class="math inline">\(1\)</span> step.</li>
</ul>
<p><span class="math display">\[\begin{align*}
\phi_x (k+1) - \phi_x(k) \leq 1 \\
\phi_y (k+1) - \phi_y(k) \leq 1
\end{align*}\]</span></p>
<p>Also, it is possible to apply global constraints, other local constraints, and apply different weights to slopes but the three described above are the most common ones. For a comprehensive list of constraints, please see <span class="citation">(Rabiner and Juang <a href="#ref-Rabiner1993" role="doc-biblioref">1993</a>)</span>. Now let’s get back to our example and go through the steps to compute the dissimilarity and warping functions between our query (<span class="math inline">\(Q\)</span>) and reference (<span class="math inline">\(R\)</span>) sequences:</p>
<p><span class="math display">\[\begin{align*}
Q&amp;=(2,2,2,4,4,3) \\
R&amp;=(2,2,3,3,2)
\end{align*}\]</span></p>
<p>The first step is to compute a <em>local cost matrix</em>. This is just a matrix that contains the distance between every pair of points between the two sequences. For this example, we will use the <em>Manhattan distance</em>. Since our sequences are <span class="math inline">\(1\)</span>-dimensional this distance can be computed as the absolute difference <span class="math inline">\(|x_i - y_i|\)</span>. Figure <a href="classification.html#fig:localCost">2.17</a> shows the resulting local cost matrix.</p>
<div class="figure" style="text-align: center"><span id="fig:localCost"></span>
<img src="images/localCostMatrix.png" alt="Local cost matrix between Q and R." width="50%" />
<p class="caption">
Figure 2.17: Local cost matrix between Q and R.
</p>
</div>
<p>For example, position <span class="math inline">\((1,1)=0\)</span> (<em>row</em>,<em>column</em>) because the first element of <span class="math inline">\(Q\)</span> is <span class="math inline">\(2\)</span> and the first element of <span class="math inline">\(R\)</span> is also <span class="math inline">\(2\)</span>, thus, <span class="math inline">\(|2-2|=0\)</span>. The rest of the matrix is filled in the same way. In dynamic programming, partial results are computed and stored in a table. Figure <a href="classification.html#fig:dynamicTable">2.18</a> shows the final dynamic programming table computed from the local cost matrix. Initially, this table is empty. We start to fill it from bottom left at position <span class="math inline">\((1,1)\)</span>. From the local cost matrix, the cost at position <span class="math inline">\((1,1)\)</span> is <span class="math inline">\(0\)</span> so the cost at that position in the dynamic programming table is <span class="math inline">\(0\)</span>. Then we can start filling in the contiguous cells. The only direction from which we can arrive at position <span class="math inline">\((1,2)\)</span> is from the west. The cost at position <span class="math inline">\((1,2)\)</span> from the local cost matrix is <span class="math inline">\(0\)</span> and the cost of the <em>minimum</em> of the cell from the west <span class="math inline">\((1,1)\)</span> is also <span class="math inline">\(0\)</span>. So <span class="math inline">\(W:0+0=0\)</span>. For each cell we add the current cost plus the minimum cost when coming from the contiguous cell. The minimum costs are marked with red. For some cells it is possible to arrive from three different directions: S, W, and SW, thus we need to compute the cost when coming from each of those. The final minimum cost at position <span class="math inline">\((5,6)\)</span> is <span class="math inline">\(3\)</span>. Thus, that is the global DTW distance. In the example, it is possible to get the minimum at <span class="math inline">\((5,6)\)</span> when arriving from the south or southwest.</p>
<div class="figure" style="text-align: center"><span id="fig:dynamicTable"></span>
<img src="images/dynamicprogramming.png" alt="Dynamic programming table." width="90%" />
<p class="caption">
Figure 2.18: Dynamic programming table.
</p>
</div>
<p>Once the table is filled in, we can backtrack starting at <span class="math inline">\((5,6)\)</span> to find the warping functions. Figure <a href="classification.html#fig:warpingResult">2.19</a> shows the final warping functions. Because of the endpoint constraints we know that <span class="math inline">\(\phi_Q(1)=1, \phi_R(1)=1\)</span> and <span class="math inline">\(\phi_Q(6)=6, \phi_R(6)=5\)</span>. Then, from <span class="math inline">\((5,6)\)</span> the minimum contiguous value is <span class="math inline">\(2\)</span> coming from SW, thus <span class="math inline">\(\phi_Q(5)=5, \phi_R(5)=4\)</span> and so on. Note that we could also had chosen to arrive from the south with the same minimum value of <span class="math inline">\(2\)</span> but still this would have resulted in the same overall distance. The dashed line in figure <a href="classification.html#fig:dynamicTable">2.18</a> shows the full backtracking.</p>
<div class="figure" style="text-align: center"><span id="fig:warpingResult"></span>
<img src="images/warpingResult.png" alt="Resulting warping functions." width="70%" />
<p class="caption">
Figure 2.19: Resulting warping functions.
</p>
</div>
<p>The runtime complexity of DTW is <span class="math inline">\(O(T_x T_y)\)</span> which is the required time to compute the local cost matrix and the dynamic programming table.</p>
<p>In R, the <code>dtw</code> package <span class="citation">(Giorgino <a href="#ref-giorgino2009" role="doc-biblioref">2009</a>)</span> has the function <code>dtw()</code> to compute the DTW distance between two sequences. Let’s use this package to solve the previous example.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="classification.html#cb52-1"></a><span class="kw">library</span>(<span class="st">&quot;dtw&quot;</span>)</span>
<span id="cb52-2"><a href="classification.html#cb52-2"></a></span>
<span id="cb52-3"><a href="classification.html#cb52-3"></a><span class="co"># Sequences from the example</span></span>
<span id="cb52-4"><a href="classification.html#cb52-4"></a>query &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">3</span>)</span>
<span id="cb52-5"><a href="classification.html#cb52-5"></a>ref &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>)</span>
<span id="cb52-6"><a href="classification.html#cb52-6"></a></span>
<span id="cb52-7"><a href="classification.html#cb52-7"></a><span class="co"># Find dtw distance.</span></span>
<span id="cb52-8"><a href="classification.html#cb52-8"></a>alignment &lt;-<span class="st"> </span><span class="kw">dtw</span>(query, ref,</span>
<span id="cb52-9"><a href="classification.html#cb52-9"></a>                 <span class="dt">step =</span> symmetric1, <span class="dt">keep.internals =</span> T)</span></code></pre></div>
<p>The <code>keep.internals = T</code> keeps the input data so it can be accessed later, e.g., for plotting. The cost matrix and final distance can be accessed from the resulting object. The <code>step</code> argument specifies a step pattern. A step pattern describes some of the algorithm constraints such as endpoint and local constraints. In this case, we use <code>symmetric1</code> which applies the constraints explained before. We can access the cost matrix and the final distance <span class="math inline">\(\phi_x\)</span> and <span class="math inline">\(\phi_y\)</span> as follows:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="classification.html#cb53-1"></a>alignment<span class="op">$</span>localCostMatrix</span>
<span id="cb53-2"><a href="classification.html#cb53-2"></a><span class="co">#&gt;      [,1] [,2] [,3] [,4] [,5]</span></span>
<span id="cb53-3"><a href="classification.html#cb53-3"></a><span class="co">#&gt; [1,]    0    0    1    1    0</span></span>
<span id="cb53-4"><a href="classification.html#cb53-4"></a><span class="co">#&gt; [2,]    0    0    1    1    0</span></span>
<span id="cb53-5"><a href="classification.html#cb53-5"></a><span class="co">#&gt; [3,]    0    0    1    1    0</span></span>
<span id="cb53-6"><a href="classification.html#cb53-6"></a><span class="co">#&gt; [4,]    2    2    1    1    2</span></span>
<span id="cb53-7"><a href="classification.html#cb53-7"></a><span class="co">#&gt; [5,]    2    2    1    1    2</span></span>
<span id="cb53-8"><a href="classification.html#cb53-8"></a><span class="co">#&gt; [6,]    1    1    0    0    1</span></span>
<span id="cb53-9"><a href="classification.html#cb53-9"></a></span>
<span id="cb53-10"><a href="classification.html#cb53-10"></a>alignment<span class="op">$</span>distance</span>
<span id="cb53-11"><a href="classification.html#cb53-11"></a><span class="co">#&gt; [1] 3</span></span>
<span id="cb53-12"><a href="classification.html#cb53-12"></a></span>
<span id="cb53-13"><a href="classification.html#cb53-13"></a>alignment<span class="op">$</span>index1</span>
<span id="cb53-14"><a href="classification.html#cb53-14"></a><span class="co">#&gt; [1] 1 2 3 4 5 6</span></span>
<span id="cb53-15"><a href="classification.html#cb53-15"></a></span>
<span id="cb53-16"><a href="classification.html#cb53-16"></a>alignment<span class="op">$</span>index2</span>
<span id="cb53-17"><a href="classification.html#cb53-17"></a><span class="co">#&gt; [1] 1 1 2 3 4 5</span></span></code></pre></div>
<p>The local cost matrix is the same one as in Figure <a href="classification.html#fig:localCost">2.17</a> but in rotated form. The resulting object also has the dynamic programming table which can be plotted along with the resulting backtracking.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="classification.html#cb54-1"></a>ccm &lt;-<span class="st"> </span>alignment<span class="op">$</span>costMatrix</span>
<span id="cb54-2"><a href="classification.html#cb54-2"></a><span class="kw">image</span>(<span class="dt">x =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(ccm), <span class="dt">y =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(ccm),</span>
<span id="cb54-3"><a href="classification.html#cb54-3"></a>      ccm, <span class="dt">xlab =</span> <span class="st">&quot;Q&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;R&quot;</span>)</span>
<span id="cb54-4"><a href="classification.html#cb54-4"></a><span class="kw">text</span>(<span class="kw">row</span>(ccm), <span class="kw">col</span>(ccm), <span class="dt">label =</span> ccm)</span>
<span id="cb54-5"><a href="classification.html#cb54-5"></a><span class="kw">lines</span>(alignment<span class="op">$</span>index1, alignment<span class="op">$</span>index2)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:backtracking"></span>
<img src="images/backtracking.png" alt="Dynamic programming table and backtracking." width="90%" />
<p class="caption">
Figure 2.20: Dynamic programming table and backtracking.
</p>
</div>
<p>And finally, the aligned sequences can be plotted. The previous Figure <a href="classification.html#fig:queryref">2.16</a> shows the result of the following command.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="classification.html#cb55-1"></a><span class="kw">plot</span>(alignment, <span class="dt">type=</span><span class="st">&quot;two&quot;</span>, <span class="dt">off=</span><span class="fl">1.5</span>,</span>
<span id="cb55-2"><a href="classification.html#cb55-2"></a>     <span class="dt">match.lty=</span><span class="dv">2</span>,</span>
<span id="cb55-3"><a href="classification.html#cb55-3"></a>     <span class="dt">match.indices=</span><span class="dv">10</span>,</span>
<span id="cb55-4"><a href="classification.html#cb55-4"></a>     <span class="dt">main=</span><span class="st">&quot;DTW resulting alignment&quot;</span>,</span>
<span id="cb55-5"><a href="classification.html#cb55-5"></a>     <span class="dt">xlab=</span><span class="st">&quot;time&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;magnitude&quot;</span>)</span></code></pre></div>
<div id="sechandgestures" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Hand Gesture Recognition</h3>

<div class="rmdfolder">
<code>hand_gestures.R</code>, <code>hand_gestures_auxiliary.R</code>
</div>

<p>Gestures are a form of communication. They are often accompanied with speech but can also be used to communicate something independently of speech (like in sign language). Gestures allow us to externalize and emphasize emotions and thoughts. They are based on body movements from arms, hands, fingers, face, head, etc. Gestures can be used as a non-verbal way to identify and study behaviors for different purposes such as for emotion <span class="citation">(De Gelder <a href="#ref-de2006towards" role="doc-biblioref">2006</a>)</span> or for the identification of developmental disorders like autism <span class="citation">(Anzulewicz, Sobota, and Delafield-Butt <a href="#ref-anzulewicz2016toward" role="doc-biblioref">2016</a>)</span>.</p>
<p>Gestures can also be used to develop user-computer interaction applications. The following video shows an example application of gesture recognition for domotics. The application determines the indoor location using <span class="math inline">\(k\)</span>-nn as it was shown in this chapter. The gestures are classified using DTW (I’ll show how to do it in a moment). Based on the location and type of gesture, an specific home appliance is activated. I programmed that app some time ago using the same algorithms presented here.</p>
<iframe src="https://www.youtube.com/embed/47-35YmimN4" width="672" height="400px">
</iframe>
<p>To demonstrate how DTW can be used for hand gesture recognition, we will examine the <em>HAND GESTURES</em> dataset that was collected with a Smartphone using its accelerometer sensor. The data was collected by <span class="math inline">\(10\)</span> individuals who performed <span class="math inline">\(5\)</span> repetitions of <span class="math inline">\(10\)</span> different gestures (<em>‘triangle’</em>, <em>‘square’</em>, <em>‘circle’</em>, <em>‘a’</em>, <em>‘b’</em>, <em>‘c’</em>, <em>‘1’</em>, <em>‘2’</em>, <em>‘3’</em>, <em>‘4’</em>). The sensor is a tri-axial accelerometer that returns values for the <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, and <span class="math inline">\(z\)</span> axes. The participants were not instructed to hold the smartphone in any particular way. The sampling rate was set at <span class="math inline">\(50\)</span> Hz. To record a gesture, the user presses the phone’s screen with her/his thumb, performs the gesture in the air, and stops pressing the screen after the gesture is complete. Figure <a href="classification.html#fig:gesturesFigure">2.21</a> shows the start and end positions of the <span class="math inline">\(10\)</span> gestures.</p>
<div class="figure" style="text-align: center"><span id="fig:gesturesFigure"></span>
<img src="images/gestures.png" alt="Paths for the 10 considered gestures." width="40%" />
<p class="caption">
Figure 2.21: Paths for the 10 considered gestures.
</p>
</div>
<p>In order to make the recognition orientation-independent, we can compute the <em>magnitude</em> of the <span class="math inline">\(3\)</span> accelerometer axes. This will provide us with the overall movement patterns regardless of orientation.</p>
<p><span class="math display" id="eq:magnitude">\[\begin{equation}
Magnitude(t) = \sqrt {{a_x}{{(t)}^2} + {a_y}{{(t)}^2} + {a_z}{{(t)}^2}}
\tag{2.16}
\end{equation}\]</span></p>
<p>where <span class="math inline">\({a_x}{{(t)}}\)</span>, <span class="math inline">\({a_y}{{(t)}}\)</span> and <span class="math inline">\({a_z}{{(t)}}\)</span> are the accelerations at time <span class="math inline">\(t\)</span>.</p>
<p>Figure <a href="classification.html#fig:handGestureMagnitude">2.22</a> shows the raw accelerometer values (dashed lines) for a <em>triangle</em> gesture. The solid line shows the resulting magnitude. This will also simplify things since we will now work with <span class="math inline">\(1\)</span>-dimensional sequences by just using the magnitude instead of the other <span class="math inline">\(3\)</span> axes.</p>
<div class="figure" style="text-align: center"><span id="fig:handGestureMagnitude"></span>
<img src="images/triangle_xyzmagnitude.png" alt="Triangle gesture." width="90%" />
<p class="caption">
Figure 2.22: Triangle gesture.
</p>
</div>
<p>The gestures are stored in text files that contain the <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, and <span class="math inline">\(z\)</span> recordings. The script <code>hand_gestures_auxiliary.R</code> has some auxiliary functions to preprocess the data. Since the sequences of each gesture are of varying length, storing them as a data frame could be problematic since data frames have fixed sizes. Instead, the <code>gen.instances()</code> function processes the files and returns all hand gestures as a list. This function also computes the magnitude (equation <a href="classification.html#eq:magnitude">(2.16)</a>). The following code (from <code>hand_gestures.R</code>) calls the <code>gen.instances()</code> function and stores the results in the <code>instances</code> variable which is a list. Then, we select the first and second instances to be the query and the reference.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="classification.html#cb56-1"></a><span class="co"># Format instances from files.</span></span>
<span id="cb56-2"><a href="classification.html#cb56-2"></a>instances &lt;-<span class="st"> </span><span class="kw">gen.instances</span>(<span class="st">&quot;../data/hand_gestures/&quot;</span>)</span>
<span id="cb56-3"><a href="classification.html#cb56-3"></a></span>
<span id="cb56-4"><a href="classification.html#cb56-4"></a><span class="co"># Use first instance as the query.</span></span>
<span id="cb56-5"><a href="classification.html#cb56-5"></a>query &lt;-<span class="st"> </span>instances[[<span class="dv">1</span>]]</span>
<span id="cb56-6"><a href="classification.html#cb56-6"></a></span>
<span id="cb56-7"><a href="classification.html#cb56-7"></a><span class="co"># Use second instance as the reference.</span></span>
<span id="cb56-8"><a href="classification.html#cb56-8"></a>ref &lt;-<span class="st"> </span>instances[[<span class="dv">2</span>]]</span></code></pre></div>
<p>Each element in <code>instances</code> is also a list that stores the <em>type</em> and <em>values</em> (magnitude) of each gesture.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="classification.html#cb57-1"></a><span class="co"># Print their respective classes</span></span>
<span id="cb57-2"><a href="classification.html#cb57-2"></a><span class="kw">print</span>(query<span class="op">$</span>type)</span>
<span id="cb57-3"><a href="classification.html#cb57-3"></a><span class="co">#&gt; [1] &quot;1&quot;</span></span>
<span id="cb57-4"><a href="classification.html#cb57-4"></a></span>
<span id="cb57-5"><a href="classification.html#cb57-5"></a><span class="kw">print</span>(ref<span class="op">$</span>type)</span>
<span id="cb57-6"><a href="classification.html#cb57-6"></a><span class="co">#&gt; [1] &quot;1&quot;</span></span></code></pre></div>
<p>Here, the first two instances are of type <em>‘1’</em>. We can also print the magnitude values.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="classification.html#cb58-1"></a><span class="co"># Print values.</span></span>
<span id="cb58-2"><a href="classification.html#cb58-2"></a><span class="kw">print</span>(query<span class="op">$</span>values)</span>
<span id="cb58-3"><a href="classification.html#cb58-3"></a><span class="co">#&gt; [1]  9.167477  9.291464  9.729926  9.901090  ....</span></span></code></pre></div>
<p>In this case, both classes are “1”. We can use the <code>dtw()</code> function to compute the similarity between the <em>query</em> and the <em>reference</em> instance and plot the resulting alignment.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="classification.html#cb59-1"></a>alignment &lt;-<span class="st"> </span><span class="kw">dtw</span>(query<span class="op">$</span>values, ref<span class="op">$</span>values, <span class="dt">keep =</span> <span class="ot">TRUE</span>)</span>
<span id="cb59-2"><a href="classification.html#cb59-2"></a></span>
<span id="cb59-3"><a href="classification.html#cb59-3"></a><span class="co"># Print similarity (distance)</span></span>
<span id="cb59-4"><a href="classification.html#cb59-4"></a>alignment<span class="op">$</span>distance</span>
<span id="cb59-5"><a href="classification.html#cb59-5"></a><span class="co">#&gt; [1] 68.56493</span></span>
<span id="cb59-6"><a href="classification.html#cb59-6"></a></span>
<span id="cb59-7"><a href="classification.html#cb59-7"></a><span class="co"># Plot result.</span></span>
<span id="cb59-8"><a href="classification.html#cb59-8"></a><span class="kw">plot</span>(alignment, <span class="dt">type=</span><span class="st">&quot;two&quot;</span>, <span class="dt">off=</span><span class="dv">1</span>, <span class="dt">match.lty=</span><span class="dv">2</span>, <span class="dt">match.indices=</span><span class="dv">40</span>,</span>
<span id="cb59-9"><a href="classification.html#cb59-9"></a>     <span class="dt">main=</span><span class="st">&quot;DTW resulting alignment&quot;</span>,</span>
<span id="cb59-10"><a href="classification.html#cb59-10"></a>     <span class="dt">xlab=</span><span class="st">&quot;time&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;magnitude&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:alignmentExample"></span>
<img src="images/alignmentExample.png" alt="Resulting alignment." width="90%" />
<p class="caption">
Figure 2.23: Resulting alignment.
</p>
</div>
<p>To perform the actual classification, we will use our well-known <span class="math inline">\(k\)</span>-nn classifier with <span class="math inline">\(k=1\)</span>. To classify a <em>query instance</em>, we need to compute the DTW distance from it to every other instance in the training set and predict the label from the closest one. We will test the performance using <span class="math inline">\(10\)</span>-fold cross-validation. Since computing all DTW distances takes some time, we can precompute all pairs of distances and store them in a matrix. The auxiliary function does just that <code>matrix.distances()</code>. Since this can take some minutes, the results are saved so we don’t need to wait again.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="classification.html#cb60-1"></a>D &lt;-<span class="st"> </span><span class="kw">matrix.distances</span>(instances)</span>
<span id="cb60-2"><a href="classification.html#cb60-2"></a></span>
<span id="cb60-3"><a href="classification.html#cb60-3"></a><span class="co"># Save results.</span></span>
<span id="cb60-4"><a href="classification.html#cb60-4"></a><span class="kw">save</span>(D, <span class="dt">file=</span><span class="st">&quot;D.RData&quot;</span>)</span></code></pre></div>
<p>The <code>matrix.distances()</code> returns a list. The first element is an array with the gestures’ classes and the second element is the actual distance matrix. The elements in the diagonal are set to <code>Inf</code> to signal that we don’t want to take into account the dissimilarity between a gesture with itself.</p>
<p>For convenience, this matrix is already stored in the file <code>D.RData</code> located this chapter’s code directory. The following code performs the <span class="math inline">\(10\)</span>-fold cross-validation and computes the performance results.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="classification.html#cb61-1"></a><span class="co"># Load the DTW distances matrix.</span></span>
<span id="cb61-2"><a href="classification.html#cb61-2"></a><span class="kw">load</span>(<span class="st">&quot;D.RData&quot;</span>)</span>
<span id="cb61-3"><a href="classification.html#cb61-3"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb61-4"><a href="classification.html#cb61-4"></a>k &lt;-<span class="st"> </span><span class="dv">10</span> <span class="co"># Number of folds.</span></span>
<span id="cb61-5"><a href="classification.html#cb61-5"></a>folds &lt;-<span class="st"> </span><span class="kw">sample</span>(k, <span class="dt">size =</span> <span class="kw">length</span>(D[[<span class="dv">1</span>]]), <span class="dt">replace =</span> T)</span>
<span id="cb61-6"><a href="classification.html#cb61-6"></a>predictions &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb61-7"><a href="classification.html#cb61-7"></a>groundTruth &lt;-<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb61-8"><a href="classification.html#cb61-8"></a></span>
<span id="cb61-9"><a href="classification.html#cb61-9"></a><span class="co"># Implement k-nn with k=1.</span></span>
<span id="cb61-10"><a href="classification.html#cb61-10"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k){</span>
<span id="cb61-11"><a href="classification.html#cb61-11"></a>  </span>
<span id="cb61-12"><a href="classification.html#cb61-12"></a>  trainSet &lt;-<span class="st"> </span><span class="kw">which</span>(folds <span class="op">!=</span><span class="st"> </span>i)</span>
<span id="cb61-13"><a href="classification.html#cb61-13"></a>  testSet &lt;-<span class="st"> </span><span class="kw">which</span>(folds <span class="op">==</span><span class="st"> </span>i)</span>
<span id="cb61-14"><a href="classification.html#cb61-14"></a>  train.labels &lt;-<span class="st"> </span>D[[<span class="dv">1</span>]][trainSet]</span>
<span id="cb61-15"><a href="classification.html#cb61-15"></a>  </span>
<span id="cb61-16"><a href="classification.html#cb61-16"></a>  <span class="cf">for</span>(query <span class="cf">in</span> testSet){</span>
<span id="cb61-17"><a href="classification.html#cb61-17"></a>    </span>
<span id="cb61-18"><a href="classification.html#cb61-18"></a>    type &lt;-<span class="st"> </span>D[[<span class="dv">1</span>]][query]</span>
<span id="cb61-19"><a href="classification.html#cb61-19"></a>    distances &lt;-<span class="st"> </span>D[[<span class="dv">2</span>]][query, ][trainSet]</span>
<span id="cb61-20"><a href="classification.html#cb61-20"></a>    </span>
<span id="cb61-21"><a href="classification.html#cb61-21"></a>    <span class="co"># Return the closest one.</span></span>
<span id="cb61-22"><a href="classification.html#cb61-22"></a>    nn &lt;-<span class="st"> </span><span class="kw">sort</span>(distances, <span class="dt">index.return =</span> T)<span class="op">$</span>ix[<span class="dv">1</span>]</span>
<span id="cb61-23"><a href="classification.html#cb61-23"></a>    pred &lt;-<span class="st"> </span>train.labels[nn]</span>
<span id="cb61-24"><a href="classification.html#cb61-24"></a>    predictions &lt;-<span class="st"> </span><span class="kw">c</span>(predictions, pred)</span>
<span id="cb61-25"><a href="classification.html#cb61-25"></a>    groundTruth &lt;-<span class="st"> </span><span class="kw">c</span>(groundTruth, type)</span>
<span id="cb61-26"><a href="classification.html#cb61-26"></a>    </span>
<span id="cb61-27"><a href="classification.html#cb61-27"></a>  }</span>
<span id="cb61-28"><a href="classification.html#cb61-28"></a>} <span class="co"># end of for</span></span></code></pre></div>
<p>The line <code>distances &lt;- D[[2]][query, ][trainSet]</code> retrieves the pre-computed distances between the test <em>query</em> and all gestures in the train set. Then, those distances are sorted in ascending order and the class from the closest one is used as the prediction. Finally, we can compute the performance.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="classification.html#cb62-1"></a>cm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="kw">factor</span>(predictions),</span>
<span id="cb62-2"><a href="classification.html#cb62-2"></a>                      <span class="kw">factor</span>(groundTruth))</span>
<span id="cb62-3"><a href="classification.html#cb62-3"></a></span>
<span id="cb62-4"><a href="classification.html#cb62-4"></a><span class="co"># Compute performance metrics per class.</span></span>
<span id="cb62-5"><a href="classification.html#cb62-5"></a>cm<span class="op">$</span>byClass[,<span class="kw">c</span>(<span class="st">&quot;Recall&quot;</span>, <span class="st">&quot;Specificity&quot;</span>, <span class="st">&quot;Precision&quot;</span>, <span class="st">&quot;F1&quot;</span>)]</span>
<span id="cb62-6"><a href="classification.html#cb62-6"></a><span class="co">#&gt;                   Recall Specificity Precision        F1</span></span>
<span id="cb62-7"><a href="classification.html#cb62-7"></a><span class="co">#&gt; Class: 1            0.84   0.9911111 0.9130435 0.8750000</span></span>
<span id="cb62-8"><a href="classification.html#cb62-8"></a><span class="co">#&gt; Class: 2            0.84   0.9866667 0.8750000 0.8571429</span></span>
<span id="cb62-9"><a href="classification.html#cb62-9"></a><span class="co">#&gt; Class: 3            0.96   0.9911111 0.9230769 0.9411765</span></span>
<span id="cb62-10"><a href="classification.html#cb62-10"></a><span class="co">#&gt; Class: 4            0.98   0.9933333 0.9423077 0.9607843</span></span>
<span id="cb62-11"><a href="classification.html#cb62-11"></a><span class="co">#&gt; Class: a            0.78   0.9733333 0.7647059 0.7722772</span></span>
<span id="cb62-12"><a href="classification.html#cb62-12"></a><span class="co">#&gt; Class: b            0.76   0.9955556 0.9500000 0.8444444</span></span>
<span id="cb62-13"><a href="classification.html#cb62-13"></a><span class="co">#&gt; Class: c            0.90   1.0000000 1.0000000 0.9473684</span></span>
<span id="cb62-14"><a href="classification.html#cb62-14"></a><span class="co">#&gt; Class: circleLeft   0.78   0.9622222 0.6964286 0.7358491</span></span>
<span id="cb62-15"><a href="classification.html#cb62-15"></a><span class="co">#&gt; Class: square       1.00   0.9977778 0.9803922 0.9900990</span></span>
<span id="cb62-16"><a href="classification.html#cb62-16"></a><span class="co">#&gt; Class: triangle     0.92   0.9711111 0.7796610 0.8440367</span></span>
<span id="cb62-17"><a href="classification.html#cb62-17"></a></span>
<span id="cb62-18"><a href="classification.html#cb62-18"></a><span class="co"># Overall performance metrics</span></span>
<span id="cb62-19"><a href="classification.html#cb62-19"></a><span class="kw">colMeans</span>(cm<span class="op">$</span>byClass[,<span class="kw">c</span>(<span class="st">&quot;Recall&quot;</span>, <span class="st">&quot;Specificity&quot;</span>,</span>
<span id="cb62-20"><a href="classification.html#cb62-20"></a>                       <span class="st">&quot;Precision&quot;</span>, <span class="st">&quot;F1&quot;</span>)])</span>
<span id="cb62-21"><a href="classification.html#cb62-21"></a><span class="co">#&gt;     Recall Specificity   Precision          F1 </span></span>
<span id="cb62-22"><a href="classification.html#cb62-22"></a><span class="co">#&gt;  0.8760000   0.9862222   0.8824616   0.8768178 </span></span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:gesturesCM"></span>
<img src="images/gesturesCM.png" alt="Confusion matrix for hand gestures' predictions." width="90%" />
<p class="caption">
Figure 2.24: Confusion matrix for hand gestures’ predictions.
</p>
</div>
<p>The overall recall was <span class="math inline">\(0.87\)</span> which is not bad. From the confusion matrix, we can see that the class <em>‘a’</em> was often confused with <em>‘circleLeft’</em> and vice versa. This makes sense since both have similar motions (see Figure <a href="classification.html#fig:gesturesFigure">2.21</a>). Also, <em>‘b’</em> was often confused with <em>‘circleLeft’</em>. The <em>‘square’</em> class was always correctly classified. This example demonstrated how DTW can be used with <span class="math inline">\(k\)</span>-nn to recognize hand gestures.</p>
<!-- ## Baseline Classifiers -->
<!-- ## Other Classification Models -->
<!-- ### Nearest Centroid -->
<!-- ### Neural Networks -->

</div>
</div>
<div id="summaryClassification" class="section level2">
<h2><span class="header-section-number">2.6</span> Summary</h2>
<p>This chapter focused on <strong>classification</strong> models. Classifiers predict a category based on the input features. Here, we showed how classifiers can be used to detect indoor locations, classify activities and had gestures.</p>
<ul>
<li><strong><span class="math inline">\(k\)</span>-nearest neighbors (<span class="math inline">\(k\)</span>-nn)</strong> predicts the class of a test point as the majority class of the <span class="math inline">\(k\)</span> nearest neighbors.</li>
<li>Some classification performance metrics are <strong>recall</strong>, <strong>specificity</strong>, <strong>precision</strong>, <strong>accuracy</strong>, <strong>F1-score</strong>, etc.</li>
<li><strong>Decision trees</strong> are easy-to-interpret classifiers trained recursively based on feature importance (for example, purity).</li>
<li><strong>Naive Bayes</strong> is a type of classifier where features are assumed to be independent.</li>
<li><strong>Dynamic Time Warping (DTW)</strong> computes the similarity between two timeseries after ‘aligning’ them in time. This can be used for classification for example, in combination with <span class="math inline">\(k\)</span>-nn.</li>
</ul>
<p><img src="images/comic_dtw.png" width="100%" style="display: block; margin: auto;" />
</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-anzulewicz2016toward">
<p>Anzulewicz, Anna, Krzysztof Sobota, and Jonathan T Delafield-Butt. 2016. “Toward the Autism Motor Signature: Gesture Patterns During Smart Tablet Gameplay Identify Children with Autism.” <em>Scientific Reports</em> 6 (1): 1–13.</p>
</div>
<div id="ref-de2006towards">
<p>De Gelder, Beatrice. 2006. “Towards the Neurobiology of Emotional Body Language.” <em>Nature Reviews Neuroscience</em> 7 (3): 242–49.</p>
</div>
<div id="ref-giorgino2009">
<p>Giorgino, Toni. 2009. “Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.” <em>Journal of Statistical Software</em> 31 (7): 1–24.</p>
</div>
<div id="ref-kwapisz2010">
<p>Kwapisz, Jennifer R., Gary M. Weiss, and Samuel A. Moore. 2010. “Activity Recognition Using Cell Phone Accelerometers.” In <em>Proceedings of the Fourth International Workshop on Knowledge Discovery from Sensor Data (at KDD-10), Washington DC.</em></p>
</div>
<div id="ref-e1071">
<p>Meyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and Friedrich Leisch. 2019. <em>E1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), Tu Wien</em>. <a href="https://CRAN.R-project.org/package=e1071">https://CRAN.R-project.org/package=e1071</a>.</p>
</div>
<div id="ref-rpartplot">
<p>Milborrow, Stephen. 2019. <em>Rpart.plot: Plot ’Rpart’ Models: An Enhanced Version of ’plot.rpart’</em>. <a href="https://CRAN.R-project.org/package=rpart.plot">https://CRAN.R-project.org/package=rpart.plot</a>.</p>
</div>
<div id="ref-quinlan2014">
<p>Quinlan, J Ross. 2014. <em>C4.5: Programs for Machine Learning</em>. Elsevier.</p>
</div>
<div id="ref-Rabiner1993">
<p>Rabiner, Lawrence, and Biing-Hwang Juang. 1993. <em>Fundamentals of Speech Recognition</em>. Prentice hall.</p>
</div>
<div id="ref-sakoe1990dynamic">
<p>Sakoe, Hiroaki, Seibi Chiba, A Waibel, and KF Lee. 1990. “Dynamic Programming Algorithm Optimization for Spoken Word Recognition.” <em>Readings in Speech Recognition</em> 159: 224.</p>
</div>
<div id="ref-steinberg2009">
<p>Steinberg, Dan, and Phillip Colla. 2009. “CART: Classification and Regression Trees.” <em>The Top Ten Algorithms in Data Mining</em> 9: 179.</p>
</div>
<div id="ref-rpart">
<p>Therneau, Terry, and Beth Atkinson. 2019. <em>Rpart: Recursive Partitioning and Regression Trees</em>. <a href="https://CRAN.R-project.org/package=rpart">https://CRAN.R-project.org/package=rpart</a>.</p>
</div>
<div id="ref-xi2006">
<p>Xi, Xiaopeng, Eamonn Keogh, Christian Shelton, Li Wei, and Chotirat Ann Ratanamahatana. 2006. “Fast Time Series Classification Using Numerosity Reduction.” In <em>Proceedings of the 23rd International Conference on Machine Learning</em>, 1033–40.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" class="uri">https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm</a><a href="classification.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p><a href="http://www.cis.fordham.edu/wisdm/dataset.php" class="uri">http://www.cis.fordham.edu/wisdm/dataset.php</a><a href="classification.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p><a href="https://commons.wikimedia.org/wiki/File:Anscombe%27s_quartet_3.svg" class="uri">https://commons.wikimedia.org/wiki/File:Anscombe%27s_quartet_3.svg</a><a href="classification.html#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ensemble.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
